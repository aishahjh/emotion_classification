{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a4b07d",
   "metadata": {},
   "source": [
    "# DistilBERT M2 (with class weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00c3d39",
   "metadata": {},
   "source": [
    "Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019c1d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn ,cuda\n",
    "from torch.utils.data import DataLoader,Dataset,RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "import nltk.corpus\n",
    "from sklearn import metrics\n",
    "from scipy.special import softmax\n",
    "\n",
    "import transformers\n",
    "from transformers import  AutoTokenizer, AutoModel\n",
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer,TrainerCallback, EarlyStoppingCallback\n",
    "import glob\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from transformers import EvalPrediction   \n",
    "import copy\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from typing import Optional\n",
    "from torch import FloatTensor\n",
    "from torch.nn import BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2acf6f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb30b595",
   "metadata": {},
   "source": [
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9fc1af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loading each set into seperate dataframes\n",
    "folder_path = 'data/clean/'\n",
    "file_pattern = folder_path + '*.csv'\n",
    "csv_files = glob.glob(file_pattern)\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    if 'train' in csv_file:\n",
    "        df_train = pd.read_csv(csv_file)\n",
    "    elif 'val' in csv_file:\n",
    "        df_val = pd.read_csv(csv_file)\n",
    "    else:\n",
    "        df_test = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6402572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my favourite food is anything i did not have t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>now if he does off himself everyone will think...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why the fuck is bayless isoing</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to make her feel threatened</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dirty southern wankers</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  admiration  amusement  \\\n",
       "0  my favourite food is anything i did not have t...           0          0   \n",
       "1  now if he does off himself everyone will think...           0          0   \n",
       "2                     why the fuck is bayless isoing           0          0   \n",
       "3                        to make her feel threatened           0          0   \n",
       "4                             dirty southern wankers           0          0   \n",
       "\n",
       "   anger  annoyance  approval  caring  confusion  curiosity  desire  ...  \\\n",
       "0      0          0         0       0          0          0       0  ...   \n",
       "1      0          0         0       0          0          0       0  ...   \n",
       "2      1          0         0       0          0          0       0  ...   \n",
       "3      0          0         0       0          0          0       0  ...   \n",
       "4      0          1         0       0          0          0       0  ...   \n",
       "\n",
       "   love  nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0     0            0         0      0            0       0        0        0   \n",
       "1     0            0         0      0            0       0        0        0   \n",
       "2     0            0         0      0            0       0        0        0   \n",
       "3     0            0         0      0            0       0        0        0   \n",
       "4     0            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  neutral  \n",
       "0         0        1  \n",
       "1         0        1  \n",
       "2         0        0  \n",
       "3         0        0  \n",
       "4         0        0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30bbe2a",
   "metadata": {},
   "source": [
    "## 2. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46926bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections of config\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 200\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 0.2\n",
    "MODEL_NAME = 'distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ac2a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaa441b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbab4d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87038cff",
   "metadata": {},
   "source": [
    "#### Storing all 28 labels into variable target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4af580be",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [col for col in df_train.columns if col not in ['clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43261378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b7c54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ffe4fe",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42c58afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb3d783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the training set\n",
    "train_encodings = tokenizer(list(df_train['clean_text']), padding=True, truncation=True, return_tensors='pt')\n",
    "train_dataset = Dataset.from_dict({'input_ids': train_encodings['input_ids'],\n",
    "                                   'attention_mask': train_encodings['attention_mask'],\n",
    "                                   'labels': torch.tensor(df_train[target_cols].values, dtype=torch.float32)})\n",
    "\n",
    "# Tokenize the validation set\n",
    "val_encodings = tokenizer(list(df_val['clean_text']), padding=True, truncation=True, return_tensors='pt')\n",
    "valid_dataset = Dataset.from_dict({'input_ids': val_encodings['input_ids'],\n",
    "                                 'attention_mask': val_encodings['attention_mask'],\n",
    "                                 'labels': torch.tensor(df_val[target_cols].values, dtype=torch.float32)})\n",
    "\n",
    "# Tokenize the test set\n",
    "test_encodings = tokenizer(list(df_test['clean_text']), padding=True, truncation=True, return_tensors='pt')\n",
    "test_dataset = Dataset.from_dict({'input_ids': test_encodings['input_ids'],\n",
    "                                 'attention_mask': test_encodings['attention_mask'],\n",
    "                                 'labels': torch.tensor(df_test[target_cols].values, dtype=torch.float32)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ae4840",
   "metadata": {},
   "source": [
    "### Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02efe9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Model\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(target_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e91901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom callback to get train and validation info during training\n",
    "class CustomCallback(TrainerCallback):\n",
    "    \n",
    "    def __init__(self, trainer) -> None:\n",
    "        super().__init__()\n",
    "        self._trainer = trainer\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if control.should_evaluate:\n",
    "            control_copy = copy.deepcopy(control)\n",
    "            self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\")\n",
    "            return control_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1980801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='data/output/dis/copy/',\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=VALID_BATCH_SIZE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    logging_dir=\"data/output/dis/copy/logs\",\n",
    "    learning_rate=float(LEARNING_RATE),\n",
    "    weight_decay=0.2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0eae176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom function to calculate the metrics for multi label classification\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'recall' : recall_micro,\n",
    "               'precision': precision_micro,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb5596",
   "metadata": {},
   "source": [
    "#### Compute class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cdb39da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "num_classes = len(target_cols)\n",
    "class_counts = np.sum(train_dataset['labels'], axis=0)\n",
    "class_weights = 1.0 / class_counts\n",
    "\n",
    "# Normalize weights\n",
    "class_weights /= class_weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82eeccf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00434095, 0.00770108, 0.01144105, 0.00725835, 0.00610007,\n",
       "       0.01649321, 0.01310535, 0.00818262, 0.02796899, 0.0141389 ,\n",
       "       0.00886653, 0.02260797, 0.05936463, 0.02101773, 0.03013129,\n",
       "       0.00673483, 0.23283272, 0.01234719, 0.0085945 , 0.10998846,\n",
       "       0.01133973, 0.16151459, 0.01615146, 0.11717725, 0.03289563,\n",
       "       0.01353066, 0.01691332, 0.00126095])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "611e9804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom trainer to incorporate class weights\n",
    "class MultiLabelTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights: Optional[FloatTensor] = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            class_weights = class_weights.to(self.args.device)\n",
    "            logging.info(f\"Using multi-label classification with class weights\", class_weights)\n",
    "        self.loss_fct = BCEWithLogitsLoss(weight=class_weights)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        try:\n",
    "            loss = self.loss_fct(outputs.logits.view(-1, model.num_labels), labels.view(-1,model.num_labels))\n",
    "        except AttributeError:  # DataParallel\n",
    "            loss = self.loss_fct(outputs.logits.view(-1, model.module.num_labels), labels.view(-1, model.num_labels))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3287773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# create Trainer instance\n",
    "trainer = MultiLabelTrainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.add_callback(CustomCallback(trainer)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e1c6498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13565' max='13565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13565/13565 45:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>0.079645</td>\n",
       "      <td>0.566597</td>\n",
       "      <td>0.447521</td>\n",
       "      <td>0.772012</td>\n",
       "      <td>0.720861</td>\n",
       "      <td>0.428446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>0.087563</td>\n",
       "      <td>0.531300</td>\n",
       "      <td>0.417712</td>\n",
       "      <td>0.729737</td>\n",
       "      <td>0.705465</td>\n",
       "      <td>0.401032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>0.063105</td>\n",
       "      <td>0.684295</td>\n",
       "      <td>0.587490</td>\n",
       "      <td>0.819296</td>\n",
       "      <td>0.790902</td>\n",
       "      <td>0.555451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>0.085009</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.483856</td>\n",
       "      <td>0.691223</td>\n",
       "      <td>0.737191</td>\n",
       "      <td>0.450240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>0.045192</td>\n",
       "      <td>0.796230</td>\n",
       "      <td>0.724191</td>\n",
       "      <td>0.884184</td>\n",
       "      <td>0.860014</td>\n",
       "      <td>0.685104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>0.089308</td>\n",
       "      <td>0.576271</td>\n",
       "      <td>0.508934</td>\n",
       "      <td>0.664144</td>\n",
       "      <td>0.748826</td>\n",
       "      <td>0.463140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.032261</td>\n",
       "      <td>0.863794</td>\n",
       "      <td>0.809656</td>\n",
       "      <td>0.925691</td>\n",
       "      <td>0.903402</td>\n",
       "      <td>0.771862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.099119</td>\n",
       "      <td>0.567211</td>\n",
       "      <td>0.511912</td>\n",
       "      <td>0.635903</td>\n",
       "      <td>0.749532</td>\n",
       "      <td>0.455953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.891022</td>\n",
       "      <td>0.847427</td>\n",
       "      <td>0.939345</td>\n",
       "      <td>0.922513</td>\n",
       "      <td>0.811233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>0.106081</td>\n",
       "      <td>0.569197</td>\n",
       "      <td>0.525392</td>\n",
       "      <td>0.620971</td>\n",
       "      <td>0.755667</td>\n",
       "      <td>0.461297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13565, training_loss=0.06756549691784834, metrics={'train_runtime': 2720.4436, 'train_samples_per_second': 79.781, 'train_steps_per_second': 4.986, 'total_flos': 8202250098919680.0, 'train_loss': 0.06756549691784834, 'epoch': 5.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start the training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9dffa6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1615</td>\n",
       "      <td>4.815702e-05</td>\n",
       "      <td>0.18</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1087</td>\n",
       "      <td>4.631404e-05</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0989</td>\n",
       "      <td>4.447107e-05</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0945</td>\n",
       "      <td>4.262809e-05</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0920</td>\n",
       "      <td>4.078511e-05</td>\n",
       "      <td>0.92</td>\n",
       "      <td>2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2713</td>\n",
       "      <td>0.079645</td>\n",
       "      <td>0.566597</td>\n",
       "      <td>0.447521</td>\n",
       "      <td>0.772012</td>\n",
       "      <td>0.720861</td>\n",
       "      <td>0.428446</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087563</td>\n",
       "      <td>0.531300</td>\n",
       "      <td>0.417712</td>\n",
       "      <td>0.729737</td>\n",
       "      <td>0.705465</td>\n",
       "      <td>0.401032</td>\n",
       "      <td>11.2008</td>\n",
       "      <td>484.431</td>\n",
       "      <td>30.355</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0861</td>\n",
       "      <td>3.894213e-05</td>\n",
       "      <td>1.11</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0801</td>\n",
       "      <td>3.709915e-05</td>\n",
       "      <td>1.29</td>\n",
       "      <td>3500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0808</td>\n",
       "      <td>3.525617e-05</td>\n",
       "      <td>1.47</td>\n",
       "      <td>4000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0796</td>\n",
       "      <td>3.341320e-05</td>\n",
       "      <td>1.66</td>\n",
       "      <td>4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0785</td>\n",
       "      <td>3.157022e-05</td>\n",
       "      <td>1.84</td>\n",
       "      <td>5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5426</td>\n",
       "      <td>0.063105</td>\n",
       "      <td>0.684295</td>\n",
       "      <td>0.587490</td>\n",
       "      <td>0.819296</td>\n",
       "      <td>0.790902</td>\n",
       "      <td>0.555451</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085009</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.483856</td>\n",
       "      <td>0.691223</td>\n",
       "      <td>0.737191</td>\n",
       "      <td>0.450240</td>\n",
       "      <td>10.9043</td>\n",
       "      <td>497.603</td>\n",
       "      <td>31.180</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0769</td>\n",
       "      <td>2.972724e-05</td>\n",
       "      <td>2.03</td>\n",
       "      <td>5500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0653</td>\n",
       "      <td>2.788426e-05</td>\n",
       "      <td>2.21</td>\n",
       "      <td>6000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0646</td>\n",
       "      <td>2.604128e-05</td>\n",
       "      <td>2.40</td>\n",
       "      <td>6500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0631</td>\n",
       "      <td>2.419830e-05</td>\n",
       "      <td>2.58</td>\n",
       "      <td>7000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0642</td>\n",
       "      <td>2.235533e-05</td>\n",
       "      <td>2.76</td>\n",
       "      <td>7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0635</td>\n",
       "      <td>2.051235e-05</td>\n",
       "      <td>2.95</td>\n",
       "      <td>8000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.045192</td>\n",
       "      <td>0.796230</td>\n",
       "      <td>0.724191</td>\n",
       "      <td>0.884184</td>\n",
       "      <td>0.860014</td>\n",
       "      <td>0.685104</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>8139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089308</td>\n",
       "      <td>0.576271</td>\n",
       "      <td>0.508934</td>\n",
       "      <td>0.664144</td>\n",
       "      <td>0.748826</td>\n",
       "      <td>0.463140</td>\n",
       "      <td>11.7338</td>\n",
       "      <td>462.424</td>\n",
       "      <td>28.976</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0517</td>\n",
       "      <td>1.866937e-05</td>\n",
       "      <td>3.13</td>\n",
       "      <td>8500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0475</td>\n",
       "      <td>1.682639e-05</td>\n",
       "      <td>3.32</td>\n",
       "      <td>9000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0479</td>\n",
       "      <td>1.498341e-05</td>\n",
       "      <td>3.50</td>\n",
       "      <td>9500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0472</td>\n",
       "      <td>1.314043e-05</td>\n",
       "      <td>3.69</td>\n",
       "      <td>10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0480</td>\n",
       "      <td>1.129746e-05</td>\n",
       "      <td>3.87</td>\n",
       "      <td>10500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.00</td>\n",
       "      <td>10852</td>\n",
       "      <td>0.032261</td>\n",
       "      <td>0.863794</td>\n",
       "      <td>0.809656</td>\n",
       "      <td>0.925691</td>\n",
       "      <td>0.903402</td>\n",
       "      <td>0.771862</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.00</td>\n",
       "      <td>10852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099119</td>\n",
       "      <td>0.567211</td>\n",
       "      <td>0.511912</td>\n",
       "      <td>0.635903</td>\n",
       "      <td>0.749532</td>\n",
       "      <td>0.455953</td>\n",
       "      <td>10.7364</td>\n",
       "      <td>505.384</td>\n",
       "      <td>31.668</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0461</td>\n",
       "      <td>9.454478e-06</td>\n",
       "      <td>4.05</td>\n",
       "      <td>11000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0362</td>\n",
       "      <td>7.611500e-06</td>\n",
       "      <td>4.24</td>\n",
       "      <td>11500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0370</td>\n",
       "      <td>5.768522e-06</td>\n",
       "      <td>4.42</td>\n",
       "      <td>12000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0371</td>\n",
       "      <td>3.925544e-06</td>\n",
       "      <td>4.61</td>\n",
       "      <td>12500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0357</td>\n",
       "      <td>2.082565e-06</td>\n",
       "      <td>4.79</td>\n",
       "      <td>13000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0361</td>\n",
       "      <td>2.395872e-07</td>\n",
       "      <td>4.98</td>\n",
       "      <td>13500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.891022</td>\n",
       "      <td>0.847427</td>\n",
       "      <td>0.939345</td>\n",
       "      <td>0.922513</td>\n",
       "      <td>0.811233</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106081</td>\n",
       "      <td>0.569197</td>\n",
       "      <td>0.525392</td>\n",
       "      <td>0.620971</td>\n",
       "      <td>0.755667</td>\n",
       "      <td>0.461297</td>\n",
       "      <td>10.8172</td>\n",
       "      <td>501.609</td>\n",
       "      <td>31.431</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.067565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.202250e+15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      loss  learning_rate  epoch   step  train_loss  train_f1  train_recall  \\\n",
       "0   0.1615   4.815702e-05   0.18    500         NaN       NaN           NaN   \n",
       "1   0.1087   4.631404e-05   0.37   1000         NaN       NaN           NaN   \n",
       "2   0.0989   4.447107e-05   0.55   1500         NaN       NaN           NaN   \n",
       "3   0.0945   4.262809e-05   0.74   2000         NaN       NaN           NaN   \n",
       "4   0.0920   4.078511e-05   0.92   2500         NaN       NaN           NaN   \n",
       "5      NaN            NaN   1.00   2713    0.079645  0.566597      0.447521   \n",
       "6      NaN            NaN   1.00   2713         NaN       NaN           NaN   \n",
       "7   0.0861   3.894213e-05   1.11   3000         NaN       NaN           NaN   \n",
       "8   0.0801   3.709915e-05   1.29   3500         NaN       NaN           NaN   \n",
       "9   0.0808   3.525617e-05   1.47   4000         NaN       NaN           NaN   \n",
       "10  0.0796   3.341320e-05   1.66   4500         NaN       NaN           NaN   \n",
       "11  0.0785   3.157022e-05   1.84   5000         NaN       NaN           NaN   \n",
       "12     NaN            NaN   2.00   5426    0.063105  0.684295      0.587490   \n",
       "13     NaN            NaN   2.00   5426         NaN       NaN           NaN   \n",
       "14  0.0769   2.972724e-05   2.03   5500         NaN       NaN           NaN   \n",
       "15  0.0653   2.788426e-05   2.21   6000         NaN       NaN           NaN   \n",
       "16  0.0646   2.604128e-05   2.40   6500         NaN       NaN           NaN   \n",
       "17  0.0631   2.419830e-05   2.58   7000         NaN       NaN           NaN   \n",
       "18  0.0642   2.235533e-05   2.76   7500         NaN       NaN           NaN   \n",
       "19  0.0635   2.051235e-05   2.95   8000         NaN       NaN           NaN   \n",
       "20     NaN            NaN   3.00   8139    0.045192  0.796230      0.724191   \n",
       "21     NaN            NaN   3.00   8139         NaN       NaN           NaN   \n",
       "22  0.0517   1.866937e-05   3.13   8500         NaN       NaN           NaN   \n",
       "23  0.0475   1.682639e-05   3.32   9000         NaN       NaN           NaN   \n",
       "24  0.0479   1.498341e-05   3.50   9500         NaN       NaN           NaN   \n",
       "25  0.0472   1.314043e-05   3.69  10000         NaN       NaN           NaN   \n",
       "26  0.0480   1.129746e-05   3.87  10500         NaN       NaN           NaN   \n",
       "27     NaN            NaN   4.00  10852    0.032261  0.863794      0.809656   \n",
       "28     NaN            NaN   4.00  10852         NaN       NaN           NaN   \n",
       "29  0.0461   9.454478e-06   4.05  11000         NaN       NaN           NaN   \n",
       "30  0.0362   7.611500e-06   4.24  11500         NaN       NaN           NaN   \n",
       "31  0.0370   5.768522e-06   4.42  12000         NaN       NaN           NaN   \n",
       "32  0.0371   3.925544e-06   4.61  12500         NaN       NaN           NaN   \n",
       "33  0.0357   2.082565e-06   4.79  13000         NaN       NaN           NaN   \n",
       "34  0.0361   2.395872e-07   4.98  13500         NaN       NaN           NaN   \n",
       "35     NaN            NaN   5.00  13565    0.026786  0.891022      0.847427   \n",
       "36     NaN            NaN   5.00  13565         NaN       NaN           NaN   \n",
       "37     NaN            NaN   5.00  13565    0.067565       NaN           NaN   \n",
       "\n",
       "    train_precision  train_roc_auc  train_accuracy  ...  eval_loss   eval_f1  \\\n",
       "0               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "1               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "2               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "3               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "4               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "5          0.772012       0.720861        0.428446  ...        NaN       NaN   \n",
       "6               NaN            NaN             NaN  ...   0.087563  0.531300   \n",
       "7               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "8               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "9               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "10              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "11              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "12         0.819296       0.790902        0.555451  ...        NaN       NaN   \n",
       "13              NaN            NaN             NaN  ...   0.085009  0.569242   \n",
       "14              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "15              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "16              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "17              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "18              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "19              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "20         0.884184       0.860014        0.685104  ...        NaN       NaN   \n",
       "21              NaN            NaN             NaN  ...   0.089308  0.576271   \n",
       "22              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "23              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "24              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "25              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "26              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "27         0.925691       0.903402        0.771862  ...        NaN       NaN   \n",
       "28              NaN            NaN             NaN  ...   0.099119  0.567211   \n",
       "29              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "30              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "31              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "32              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "33              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "34              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "35         0.939345       0.922513        0.811233  ...        NaN       NaN   \n",
       "36              NaN            NaN             NaN  ...   0.106081  0.569197   \n",
       "37              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "\n",
       "    eval_recall  eval_precision  eval_roc_auc  eval_accuracy  eval_runtime  \\\n",
       "0           NaN             NaN           NaN            NaN           NaN   \n",
       "1           NaN             NaN           NaN            NaN           NaN   \n",
       "2           NaN             NaN           NaN            NaN           NaN   \n",
       "3           NaN             NaN           NaN            NaN           NaN   \n",
       "4           NaN             NaN           NaN            NaN           NaN   \n",
       "5           NaN             NaN           NaN            NaN           NaN   \n",
       "6      0.417712        0.729737      0.705465       0.401032       11.2008   \n",
       "7           NaN             NaN           NaN            NaN           NaN   \n",
       "8           NaN             NaN           NaN            NaN           NaN   \n",
       "9           NaN             NaN           NaN            NaN           NaN   \n",
       "10          NaN             NaN           NaN            NaN           NaN   \n",
       "11          NaN             NaN           NaN            NaN           NaN   \n",
       "12          NaN             NaN           NaN            NaN           NaN   \n",
       "13     0.483856        0.691223      0.737191       0.450240       10.9043   \n",
       "14          NaN             NaN           NaN            NaN           NaN   \n",
       "15          NaN             NaN           NaN            NaN           NaN   \n",
       "16          NaN             NaN           NaN            NaN           NaN   \n",
       "17          NaN             NaN           NaN            NaN           NaN   \n",
       "18          NaN             NaN           NaN            NaN           NaN   \n",
       "19          NaN             NaN           NaN            NaN           NaN   \n",
       "20          NaN             NaN           NaN            NaN           NaN   \n",
       "21     0.508934        0.664144      0.748826       0.463140       11.7338   \n",
       "22          NaN             NaN           NaN            NaN           NaN   \n",
       "23          NaN             NaN           NaN            NaN           NaN   \n",
       "24          NaN             NaN           NaN            NaN           NaN   \n",
       "25          NaN             NaN           NaN            NaN           NaN   \n",
       "26          NaN             NaN           NaN            NaN           NaN   \n",
       "27          NaN             NaN           NaN            NaN           NaN   \n",
       "28     0.511912        0.635903      0.749532       0.455953       10.7364   \n",
       "29          NaN             NaN           NaN            NaN           NaN   \n",
       "30          NaN             NaN           NaN            NaN           NaN   \n",
       "31          NaN             NaN           NaN            NaN           NaN   \n",
       "32          NaN             NaN           NaN            NaN           NaN   \n",
       "33          NaN             NaN           NaN            NaN           NaN   \n",
       "34          NaN             NaN           NaN            NaN           NaN   \n",
       "35          NaN             NaN           NaN            NaN           NaN   \n",
       "36     0.525392        0.620971      0.755667       0.461297       10.8172   \n",
       "37          NaN             NaN           NaN            NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second    total_flos  \n",
       "0                       NaN                    NaN           NaN  \n",
       "1                       NaN                    NaN           NaN  \n",
       "2                       NaN                    NaN           NaN  \n",
       "3                       NaN                    NaN           NaN  \n",
       "4                       NaN                    NaN           NaN  \n",
       "5                       NaN                    NaN           NaN  \n",
       "6                   484.431                 30.355           NaN  \n",
       "7                       NaN                    NaN           NaN  \n",
       "8                       NaN                    NaN           NaN  \n",
       "9                       NaN                    NaN           NaN  \n",
       "10                      NaN                    NaN           NaN  \n",
       "11                      NaN                    NaN           NaN  \n",
       "12                      NaN                    NaN           NaN  \n",
       "13                  497.603                 31.180           NaN  \n",
       "14                      NaN                    NaN           NaN  \n",
       "15                      NaN                    NaN           NaN  \n",
       "16                      NaN                    NaN           NaN  \n",
       "17                      NaN                    NaN           NaN  \n",
       "18                      NaN                    NaN           NaN  \n",
       "19                      NaN                    NaN           NaN  \n",
       "20                      NaN                    NaN           NaN  \n",
       "21                  462.424                 28.976           NaN  \n",
       "22                      NaN                    NaN           NaN  \n",
       "23                      NaN                    NaN           NaN  \n",
       "24                      NaN                    NaN           NaN  \n",
       "25                      NaN                    NaN           NaN  \n",
       "26                      NaN                    NaN           NaN  \n",
       "27                      NaN                    NaN           NaN  \n",
       "28                  505.384                 31.668           NaN  \n",
       "29                      NaN                    NaN           NaN  \n",
       "30                      NaN                    NaN           NaN  \n",
       "31                      NaN                    NaN           NaN  \n",
       "32                      NaN                    NaN           NaN  \n",
       "33                      NaN                    NaN           NaN  \n",
       "34                      NaN                    NaN           NaN  \n",
       "35                      NaN                    NaN           NaN  \n",
       "36                  501.609                 31.431           NaN  \n",
       "37                      NaN                    NaN  8.202250e+15  \n",
       "\n",
       "[38 rows x 23 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view train and validation metrics from training\n",
    "log_history = pd.DataFrame(trainer.state.log_history)\n",
    "log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9edd140f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2713</td>\n",
       "      <td>0.079645</td>\n",
       "      <td>0.566597</td>\n",
       "      <td>0.447521</td>\n",
       "      <td>0.772012</td>\n",
       "      <td>0.720861</td>\n",
       "      <td>0.428446</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5426</td>\n",
       "      <td>0.063105</td>\n",
       "      <td>0.684295</td>\n",
       "      <td>0.587490</td>\n",
       "      <td>0.819296</td>\n",
       "      <td>0.790902</td>\n",
       "      <td>0.555451</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.045192</td>\n",
       "      <td>0.796230</td>\n",
       "      <td>0.724191</td>\n",
       "      <td>0.884184</td>\n",
       "      <td>0.860014</td>\n",
       "      <td>0.685104</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10852</td>\n",
       "      <td>0.032261</td>\n",
       "      <td>0.863794</td>\n",
       "      <td>0.809656</td>\n",
       "      <td>0.925691</td>\n",
       "      <td>0.903402</td>\n",
       "      <td>0.771862</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.891022</td>\n",
       "      <td>0.847427</td>\n",
       "      <td>0.939345</td>\n",
       "      <td>0.922513</td>\n",
       "      <td>0.811233</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    loss  learning_rate  epoch   step  train_loss  train_f1  train_recall  \\\n",
       "5    NaN            NaN    1.0   2713    0.079645  0.566597      0.447521   \n",
       "12   NaN            NaN    2.0   5426    0.063105  0.684295      0.587490   \n",
       "20   NaN            NaN    3.0   8139    0.045192  0.796230      0.724191   \n",
       "27   NaN            NaN    4.0  10852    0.032261  0.863794      0.809656   \n",
       "35   NaN            NaN    5.0  13565    0.026786  0.891022      0.847427   \n",
       "\n",
       "    train_precision  train_roc_auc  train_accuracy  ...  eval_loss  eval_f1  \\\n",
       "5          0.772012       0.720861        0.428446  ...        NaN      NaN   \n",
       "12         0.819296       0.790902        0.555451  ...        NaN      NaN   \n",
       "20         0.884184       0.860014        0.685104  ...        NaN      NaN   \n",
       "27         0.925691       0.903402        0.771862  ...        NaN      NaN   \n",
       "35         0.939345       0.922513        0.811233  ...        NaN      NaN   \n",
       "\n",
       "    eval_recall  eval_precision  eval_roc_auc  eval_accuracy  eval_runtime  \\\n",
       "5           NaN             NaN           NaN            NaN           NaN   \n",
       "12          NaN             NaN           NaN            NaN           NaN   \n",
       "20          NaN             NaN           NaN            NaN           NaN   \n",
       "27          NaN             NaN           NaN            NaN           NaN   \n",
       "35          NaN             NaN           NaN            NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  total_flos  \n",
       "5                       NaN                    NaN         NaN  \n",
       "12                      NaN                    NaN         NaN  \n",
       "20                      NaN                    NaN         NaN  \n",
       "27                      NaN                    NaN         NaN  \n",
       "35                      NaN                    NaN         NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store train metrics in dataframe\n",
    "train_history = log_history[log_history['train_f1'].notna()]\n",
    "train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d7c76fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087563</td>\n",
       "      <td>0.531300</td>\n",
       "      <td>0.417712</td>\n",
       "      <td>0.729737</td>\n",
       "      <td>0.705465</td>\n",
       "      <td>0.401032</td>\n",
       "      <td>11.2008</td>\n",
       "      <td>484.431</td>\n",
       "      <td>30.355</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085009</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.483856</td>\n",
       "      <td>0.691223</td>\n",
       "      <td>0.737191</td>\n",
       "      <td>0.450240</td>\n",
       "      <td>10.9043</td>\n",
       "      <td>497.603</td>\n",
       "      <td>31.180</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089308</td>\n",
       "      <td>0.576271</td>\n",
       "      <td>0.508934</td>\n",
       "      <td>0.664144</td>\n",
       "      <td>0.748826</td>\n",
       "      <td>0.463140</td>\n",
       "      <td>11.7338</td>\n",
       "      <td>462.424</td>\n",
       "      <td>28.976</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099119</td>\n",
       "      <td>0.567211</td>\n",
       "      <td>0.511912</td>\n",
       "      <td>0.635903</td>\n",
       "      <td>0.749532</td>\n",
       "      <td>0.455953</td>\n",
       "      <td>10.7364</td>\n",
       "      <td>505.384</td>\n",
       "      <td>31.668</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106081</td>\n",
       "      <td>0.569197</td>\n",
       "      <td>0.525392</td>\n",
       "      <td>0.620971</td>\n",
       "      <td>0.755667</td>\n",
       "      <td>0.461297</td>\n",
       "      <td>10.8172</td>\n",
       "      <td>501.609</td>\n",
       "      <td>31.431</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    loss  learning_rate  epoch   step  train_loss  train_f1  train_recall  \\\n",
       "6    NaN            NaN    1.0   2713         NaN       NaN           NaN   \n",
       "13   NaN            NaN    2.0   5426         NaN       NaN           NaN   \n",
       "21   NaN            NaN    3.0   8139         NaN       NaN           NaN   \n",
       "28   NaN            NaN    4.0  10852         NaN       NaN           NaN   \n",
       "36   NaN            NaN    5.0  13565         NaN       NaN           NaN   \n",
       "\n",
       "    train_precision  train_roc_auc  train_accuracy  ...  eval_loss   eval_f1  \\\n",
       "6               NaN            NaN             NaN  ...   0.087563  0.531300   \n",
       "13              NaN            NaN             NaN  ...   0.085009  0.569242   \n",
       "21              NaN            NaN             NaN  ...   0.089308  0.576271   \n",
       "28              NaN            NaN             NaN  ...   0.099119  0.567211   \n",
       "36              NaN            NaN             NaN  ...   0.106081  0.569197   \n",
       "\n",
       "    eval_recall  eval_precision  eval_roc_auc  eval_accuracy  eval_runtime  \\\n",
       "6      0.417712        0.729737      0.705465       0.401032       11.2008   \n",
       "13     0.483856        0.691223      0.737191       0.450240       10.9043   \n",
       "21     0.508934        0.664144      0.748826       0.463140       11.7338   \n",
       "28     0.511912        0.635903      0.749532       0.455953       10.7364   \n",
       "36     0.525392        0.620971      0.755667       0.461297       10.8172   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  total_flos  \n",
       "6                   484.431                 30.355         NaN  \n",
       "13                  497.603                 31.180         NaN  \n",
       "21                  462.424                 28.976         NaN  \n",
       "28                  505.384                 31.668         NaN  \n",
       "36                  501.609                 31.431         NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store validation metrics in dataframe\n",
    "val_history = log_history[log_history['eval_f1'].notna()]\n",
    "val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1febf2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_x</th>\n",
       "      <th>learning_rate_x</th>\n",
       "      <th>epoch_x</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss_x</th>\n",
       "      <th>train_f1_x</th>\n",
       "      <th>train_recall_x</th>\n",
       "      <th>train_precision_x</th>\n",
       "      <th>train_roc_auc_x</th>\n",
       "      <th>train_accuracy_x</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss_y</th>\n",
       "      <th>eval_f1_y</th>\n",
       "      <th>eval_recall_y</th>\n",
       "      <th>eval_precision_y</th>\n",
       "      <th>eval_roc_auc_y</th>\n",
       "      <th>eval_accuracy_y</th>\n",
       "      <th>eval_runtime_y</th>\n",
       "      <th>eval_samples_per_second_y</th>\n",
       "      <th>eval_steps_per_second_y</th>\n",
       "      <th>total_flos_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2713</td>\n",
       "      <td>0.079645</td>\n",
       "      <td>0.566597</td>\n",
       "      <td>0.447521</td>\n",
       "      <td>0.772012</td>\n",
       "      <td>0.720861</td>\n",
       "      <td>0.428446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087563</td>\n",
       "      <td>0.531300</td>\n",
       "      <td>0.417712</td>\n",
       "      <td>0.729737</td>\n",
       "      <td>0.705465</td>\n",
       "      <td>0.401032</td>\n",
       "      <td>11.2008</td>\n",
       "      <td>484.431</td>\n",
       "      <td>30.355</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5426</td>\n",
       "      <td>0.063105</td>\n",
       "      <td>0.684295</td>\n",
       "      <td>0.587490</td>\n",
       "      <td>0.819296</td>\n",
       "      <td>0.790902</td>\n",
       "      <td>0.555451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085009</td>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.483856</td>\n",
       "      <td>0.691223</td>\n",
       "      <td>0.737191</td>\n",
       "      <td>0.450240</td>\n",
       "      <td>10.9043</td>\n",
       "      <td>497.603</td>\n",
       "      <td>31.180</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.045192</td>\n",
       "      <td>0.796230</td>\n",
       "      <td>0.724191</td>\n",
       "      <td>0.884184</td>\n",
       "      <td>0.860014</td>\n",
       "      <td>0.685104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089308</td>\n",
       "      <td>0.576271</td>\n",
       "      <td>0.508934</td>\n",
       "      <td>0.664144</td>\n",
       "      <td>0.748826</td>\n",
       "      <td>0.463140</td>\n",
       "      <td>11.7338</td>\n",
       "      <td>462.424</td>\n",
       "      <td>28.976</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10852</td>\n",
       "      <td>0.032261</td>\n",
       "      <td>0.863794</td>\n",
       "      <td>0.809656</td>\n",
       "      <td>0.925691</td>\n",
       "      <td>0.903402</td>\n",
       "      <td>0.771862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099119</td>\n",
       "      <td>0.567211</td>\n",
       "      <td>0.511912</td>\n",
       "      <td>0.635903</td>\n",
       "      <td>0.749532</td>\n",
       "      <td>0.455953</td>\n",
       "      <td>10.7364</td>\n",
       "      <td>505.384</td>\n",
       "      <td>31.668</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.891022</td>\n",
       "      <td>0.847427</td>\n",
       "      <td>0.939345</td>\n",
       "      <td>0.922513</td>\n",
       "      <td>0.811233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106081</td>\n",
       "      <td>0.569197</td>\n",
       "      <td>0.525392</td>\n",
       "      <td>0.620971</td>\n",
       "      <td>0.755667</td>\n",
       "      <td>0.461297</td>\n",
       "      <td>10.8172</td>\n",
       "      <td>501.609</td>\n",
       "      <td>31.431</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loss_x  learning_rate_x  epoch_x   step  train_loss_x  train_f1_x  \\\n",
       "0     NaN              NaN      1.0   2713      0.079645    0.566597   \n",
       "1     NaN              NaN      2.0   5426      0.063105    0.684295   \n",
       "2     NaN              NaN      3.0   8139      0.045192    0.796230   \n",
       "3     NaN              NaN      4.0  10852      0.032261    0.863794   \n",
       "4     NaN              NaN      5.0  13565      0.026786    0.891022   \n",
       "\n",
       "   train_recall_x  train_precision_x  train_roc_auc_x  train_accuracy_x  ...  \\\n",
       "0        0.447521           0.772012         0.720861          0.428446  ...   \n",
       "1        0.587490           0.819296         0.790902          0.555451  ...   \n",
       "2        0.724191           0.884184         0.860014          0.685104  ...   \n",
       "3        0.809656           0.925691         0.903402          0.771862  ...   \n",
       "4        0.847427           0.939345         0.922513          0.811233  ...   \n",
       "\n",
       "   eval_loss_y  eval_f1_y  eval_recall_y  eval_precision_y  eval_roc_auc_y  \\\n",
       "0     0.087563   0.531300       0.417712          0.729737        0.705465   \n",
       "1     0.085009   0.569242       0.483856          0.691223        0.737191   \n",
       "2     0.089308   0.576271       0.508934          0.664144        0.748826   \n",
       "3     0.099119   0.567211       0.511912          0.635903        0.749532   \n",
       "4     0.106081   0.569197       0.525392          0.620971        0.755667   \n",
       "\n",
       "   eval_accuracy_y  eval_runtime_y  eval_samples_per_second_y  \\\n",
       "0         0.401032         11.2008                    484.431   \n",
       "1         0.450240         10.9043                    497.603   \n",
       "2         0.463140         11.7338                    462.424   \n",
       "3         0.455953         10.7364                    505.384   \n",
       "4         0.461297         10.8172                    501.609   \n",
       "\n",
       "   eval_steps_per_second_y  total_flos_y  \n",
       "0                   30.355           NaN  \n",
       "1                   31.180           NaN  \n",
       "2                   28.976           NaN  \n",
       "3                   31.668           NaN  \n",
       "4                   31.431           NaN  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = pd.merge(train_history, val_history, on='step', how='outer')\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9ebe0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting function to check for accuracy with graphs \n",
    "def plot_model_performance(history):\n",
    "\n",
    "    #getting train and validation accuracy\n",
    "    acc = history['train_accuracy_x']\n",
    "    val_acc = history['eval_accuracy_y']\n",
    "\n",
    "    #getting train and validation loss\n",
    "    loss = history['train_loss_x']\n",
    "    val_loss = history['eval_loss_y']\n",
    "\n",
    "    epochs_range = range(5)\n",
    "\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c97c350d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_model_performance(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc7fb89",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5456e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to evaluate the model on the test set\n",
    "def evaluate_model(test_dataset):\n",
    "    test_res = trainer.evaluate(eval_dataset=test_dataset)\n",
    "    print(test_res)\n",
    "         \n",
    "    return trainer.predict(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f7bb617c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08910774439573288, 'eval_f1': 0.5800035835871709, 'eval_recall': 0.5114552061937114, 'eval_precision': 0.6697703289882061, 'eval_roc_auc': 0.7502478500290867, 'eval_accuracy': 0.46360788649345863, 'eval_runtime': 7.2836, 'eval_samples_per_second': 745.099, 'eval_steps_per_second': 46.68, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "y_test = evaluate_model(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c172305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate each emotion label metrics on test set\n",
    "def calc_label_metrics(label, y_targets, y_preds, threshold):\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"accuracy\": metrics.accuracy_score(y_targets, y_preds),\n",
    "        \"precision\": metrics.precision_score(y_targets, y_preds, zero_division=0),\n",
    "        \"recall\": metrics.recall_score(y_targets, y_preds, zero_division=0),\n",
    "        \"f1\": metrics.f1_score(y_targets, y_preds, zero_division=0),\n",
    "        \"mcc\": metrics.matthews_corrcoef(y_targets, y_preds),\n",
    "        \"support\": y_targets.sum(),\n",
    "        \"threshold\": threshold,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "78c37071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate overall metric on test set\n",
    "def calc_test_metrics(y_test, test_dataset, target_cols):\n",
    "    threshold = 0.5\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(y_test.predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= 0.5)] = 1\n",
    "    \n",
    "    # finally, compute metrics\n",
    "    y_true = df_test[target_cols].values\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'recall' : recall_micro,\n",
    "               'precision': precision_micro,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    metrics_df = pd.DataFrame.from_dict(metrics, orient='index', columns=['Value'])\n",
    "\n",
    "    display(metrics_df)\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for label_index, label in enumerate(target_cols):\n",
    "        y_targets, y_preds = y_true[:, label_index], y_pred[:, label_index]\n",
    "        results.append(calc_label_metrics(label, y_targets, y_preds, threshold))\n",
    "\n",
    "    per_label_results = pd.DataFrame(results, index=target_cols)\n",
    "    display(per_label_results.drop(columns=[\"label\"]).round(3))\n",
    "    \n",
    "    return y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "794fec4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.580004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.511455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.669770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc</th>\n",
       "      <td>0.750248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.463608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "f1         0.580004\n",
       "recall     0.511455\n",
       "precision  0.669770\n",
       "roc_auc    0.750248\n",
       "accuracy   0.463608"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>mcc</th>\n",
       "      <th>support</th>\n",
       "      <th>threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.938</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.646</td>\n",
       "      <td>504</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.981</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.804</td>\n",
       "      <td>264</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.966</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.476</td>\n",
       "      <td>198</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.940</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.297</td>\n",
       "      <td>320</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.937</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.346</td>\n",
       "      <td>351</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.976</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.376</td>\n",
       "      <td>135</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.972</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.429</td>\n",
       "      <td>153</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.950</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.439</td>\n",
       "      <td>284</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.987</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.422</td>\n",
       "      <td>83</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.974</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.322</td>\n",
       "      <td>151</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.946</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.346</td>\n",
       "      <td>267</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.982</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.495</td>\n",
       "      <td>123</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.995</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.556</td>\n",
       "      <td>37</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.983</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.395</td>\n",
       "      <td>103</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.992</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.701</td>\n",
       "      <td>78</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.989</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.908</td>\n",
       "      <td>352</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.999</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.977</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.537</td>\n",
       "      <td>161</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.981</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.781</td>\n",
       "      <td>238</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.996</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.375</td>\n",
       "      <td>23</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.975</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.573</td>\n",
       "      <td>186</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.997</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.374</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.975</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.295</td>\n",
       "      <td>145</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.998</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.993</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.687</td>\n",
       "      <td>56</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.980</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.576</td>\n",
       "      <td>156</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.978</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.509</td>\n",
       "      <td>141</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.778</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.480</td>\n",
       "      <td>1787</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                accuracy  precision  recall     f1    mcc  support  threshold\n",
       "admiration         0.938      0.654   0.708  0.680  0.646      504        0.5\n",
       "amusement          0.981      0.768   0.864  0.813  0.804      264        0.5\n",
       "anger              0.966      0.547   0.444  0.490  0.476      198        0.5\n",
       "annoyance          0.940      0.479   0.219  0.300  0.297      320        0.5\n",
       "approval           0.937      0.531   0.265  0.354  0.346      351        0.5\n",
       "caring             0.976      0.544   0.274  0.365  0.376      135        0.5\n",
       "confusion          0.972      0.509   0.386  0.439  0.429      153        0.5\n",
       "curiosity          0.950      0.525   0.412  0.462  0.439      284        0.5\n",
       "desire             0.987      0.688   0.265  0.383  0.422       83        0.5\n",
       "disappointment     0.974      0.634   0.172  0.271  0.322      151        0.5\n",
       "disapproval        0.946      0.438   0.318  0.369  0.346      267        0.5\n",
       "disgust            0.982      0.662   0.382  0.485  0.495      123        0.5\n",
       "embarrassment      0.995      0.824   0.378  0.519  0.556       37        0.5\n",
       "excitement         0.983      0.596   0.272  0.373  0.395      103        0.5\n",
       "fear               0.992      0.705   0.705  0.705  0.701       78        0.5\n",
       "gratitude          0.989      0.927   0.901  0.914  0.908      352        0.5\n",
       "grief              0.999      0.000   0.000  0.000  0.000        6        0.5\n",
       "joy                0.977      0.664   0.453  0.539  0.537      161        0.5\n",
       "love               0.981      0.767   0.815  0.790  0.781      238        0.5\n",
       "nervousness        0.996      0.545   0.261  0.353  0.375       23        0.5\n",
       "optimism           0.975      0.700   0.489  0.576  0.573      186        0.5\n",
       "pride              0.997      0.750   0.188  0.300  0.374       16        0.5\n",
       "realization        0.975      0.773   0.117  0.204  0.295      145        0.5\n",
       "relief             0.998      0.000   0.000  0.000  0.000       11        0.5\n",
       "remorse            0.993      0.636   0.750  0.689  0.687       56        0.5\n",
       "sadness            0.980      0.742   0.462  0.569  0.576      156        0.5\n",
       "surprise           0.978      0.606   0.447  0.514  0.509      141        0.5\n",
       "neutral            0.778      0.697   0.578  0.632  0.480     1787        0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "targets, outputs = calc_test_metrics(y_test, test_dataset, target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f1631d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results DataFrame:\n",
      "         Actual                Predicted\n",
      "0     [sadness]                [remorse]\n",
      "1  [admiration]             [admiration]\n",
      "2  [excitement]               [optimism]\n",
      "3   [gratitude]              [gratitude]\n",
      "4     [neutral]                [neutral]\n",
      "5   [gratitude]              [gratitude]\n",
      "6   [gratitude]              [gratitude]\n",
      "7   [gratitude]  [admiration, gratitude]\n",
      "8     [remorse]       [remorse, sadness]\n",
      "9     [sadness]                [sadness]\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to store actual labels and predicted labels\n",
    "final_df = pd.DataFrame({\n",
    "    'Actual': [list(np.where(targets[i])[0]) for i in range(len(targets))],\n",
    "    'Predicted': [list(np.where(outputs[i])[0]) for i in range(len(outputs))]\n",
    "})\n",
    "\n",
    "# Map label indices to label names in the 'Actual' column\n",
    "final_df['Actual'] = final_df['Actual'].apply(lambda indices: [target_cols[idx] for idx in indices])\n",
    "\n",
    "# Map label indices to label names in the 'Predicted' column\n",
    "final_df['Predicted'] = final_df['Predicted'].apply(lambda indices: [target_cols[idx] for idx in indices])\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(\"Results DataFrame:\")\n",
    "print(final_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "301ef201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the label DataFrame with the original DataFramev\n",
    "val_df_terms = df_test['clean_text']\n",
    "result_df = pd.concat([val_df_terms, final_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a335fd5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am really sorry about your situation frown s...</td>\n",
       "      <td>[sadness]</td>\n",
       "      <td>[remorse]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it is wonderful because it is awful at not with</td>\n",
       "      <td>[admiration]</td>\n",
       "      <td>[admiration]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kings fan here good luck to you guys will be a...</td>\n",
       "      <td>[excitement]</td>\n",
       "      <td>[optimism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i did not know that thank you for teaching me ...</td>\n",
       "      <td>[gratitude]</td>\n",
       "      <td>[gratitude]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>they got bored from haunting earth for thousan...</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5422</th>\n",
       "      <td>thanks i was diagnosed with bp 1 after the hos...</td>\n",
       "      <td>[gratitude]</td>\n",
       "      <td>[gratitude]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5423</th>\n",
       "      <td>well that makes sense</td>\n",
       "      <td>[approval]</td>\n",
       "      <td>[approval]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5424</th>\n",
       "      <td>daddy issues name</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5425</th>\n",
       "      <td>so glad i discovered that subreddit a couple m...</td>\n",
       "      <td>[admiration]</td>\n",
       "      <td>[admiration]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5426</th>\n",
       "      <td>had to watch elmo in grouchland one time too m...</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5427 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_text        Actual  \\\n",
       "0     i am really sorry about your situation frown s...     [sadness]   \n",
       "1       it is wonderful because it is awful at not with  [admiration]   \n",
       "2     kings fan here good luck to you guys will be a...  [excitement]   \n",
       "3     i did not know that thank you for teaching me ...   [gratitude]   \n",
       "4     they got bored from haunting earth for thousan...     [neutral]   \n",
       "...                                                 ...           ...   \n",
       "5422  thanks i was diagnosed with bp 1 after the hos...   [gratitude]   \n",
       "5423                              well that makes sense    [approval]   \n",
       "5424                                  daddy issues name     [neutral]   \n",
       "5425  so glad i discovered that subreddit a couple m...  [admiration]   \n",
       "5426  had to watch elmo in grouchland one time too m...     [neutral]   \n",
       "\n",
       "         Predicted  \n",
       "0        [remorse]  \n",
       "1     [admiration]  \n",
       "2       [optimism]  \n",
       "3      [gratitude]  \n",
       "4        [neutral]  \n",
       "...            ...  \n",
       "5422   [gratitude]  \n",
       "5423    [approval]  \n",
       "5424     [neutral]  \n",
       "5425  [admiration]  \n",
       "5426     [neutral]  \n",
       "\n",
       "[5427 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53492f7a",
   "metadata": {},
   "source": [
    "## 4. Save the output, tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5113b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('output__distilbert_M2.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b8b778c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer\n",
    "tokenizer.save_pretrained('./distilbert_M2_transformer/')\n",
    "\n",
    "# Save model\n",
    "trainer.save_model('./distilbert_M2_transformer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ba9b95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86efa8a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
