{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2678bdf8",
   "metadata": {},
   "source": [
    "# DistilBERT M3 (with word2vec Augmented dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2706837a",
   "metadata": {},
   "source": [
    "Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019c1d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy asImporting the necessary libraries np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn ,cuda\n",
    "from torch.utils.data import DataLoader,Dataset,RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "import nltk.corpus\n",
    "from sklearn import metrics\n",
    "from scipy.special import softmax\n",
    "\n",
    "import transformers\n",
    "from transformers import  AutoTokenizer, AutoModel\n",
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer,TrainerCallback, EarlyStoppingCallback\n",
    "import glob\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from transformers import EvalPrediction   \n",
    "import copy\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from typing import Optional\n",
    "from torch import FloatTensor\n",
    "from torch.nn import BCEWithLogitsLoss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc73b75",
   "metadata": {},
   "source": [
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9fc1af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_path = 'data/clean/'\n",
    "file_pattern = folder_path + '*.csv'\n",
    "csv_files = glob.glob(file_pattern)\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    if 'train' in csv_file:\n",
    "        df_train = pd.read_csv(csv_file)\n",
    "    elif 'val' in csv_file:\n",
    "        df_val = pd.read_csv(csv_file)\n",
    "    else:\n",
    "        df_test = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74ee6884",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the augmented dataset in the train dataframe\n",
    "df_train = pd.read_csv('data/clean/augmented/augmented_w2v_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0b05fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>disappointment</th>\n",
       "      <th>...</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>vaginally my victim favourite food wbz is aust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>now if he does zhang off fosamax himself ridle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>why the irritated fuck yoshinobu is bayless is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>to nat make her risk feel threatened</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dirty southern wkis wankers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   admiration  amusement  anger  annoyance  approval  caring  confusion  \\\n",
       "0           0          0      0          0         0       0          0   \n",
       "1           0          0      0          0         0       0          0   \n",
       "2           0          0      1          0         0       0          0   \n",
       "3           0          0      0          0         0       0          0   \n",
       "4           0          0      0          1         0       0          0   \n",
       "\n",
       "   curiosity  desire  disappointment  ...  nervousness  optimism  pride  \\\n",
       "0          0       0               0  ...            0         0      0   \n",
       "1          0       0               0  ...            0         0      0   \n",
       "2          0       0               0  ...            0         0      0   \n",
       "3          0       0               0  ...            0         0      0   \n",
       "4          0       0               0  ...            0         0      0   \n",
       "\n",
       "   realization  relief  remorse  sadness  surprise  neutral  \\\n",
       "0            0       0        0        0         0        1   \n",
       "1            0       0        0        0         0        1   \n",
       "2            0       0        0        0         0        0   \n",
       "3            0       0        0        0         0        0   \n",
       "4            0       0        0        0         0        0   \n",
       "\n",
       "                                          clean_text  \n",
       "0  vaginally my victim favourite food wbz is aust...  \n",
       "1  now if he does zhang off fosamax himself ridle...  \n",
       "2  why the irritated fuck yoshinobu is bayless is...  \n",
       "3               to nat make her risk feel threatened  \n",
       "4                        dirty southern wkis wankers  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d05b88e",
   "metadata": {},
   "source": [
    "## 2. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46926bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections of config\n",
    "\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 200\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 0.2\n",
    "MODEL_NAME = 'distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ac2a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dd27b0",
   "metadata": {},
   "source": [
    "#### Checking the max token length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a834a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  166.0\n"
     ]
    }
   ],
   "source": [
    "text_data = df_train['clean_text'].values\n",
    "\n",
    "max_len = np.zeros(len(text_data))\n",
    "for i in range(len(text_data)):\n",
    "    input_ids = tokenizer.encode(text_data[i], add_special_tokens=True)\n",
    "    max_len[i] = len(input_ids)\n",
    "print('Max length: ', max_len.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaa441b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbab4d02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765356b5",
   "metadata": {},
   "source": [
    "#### Storing all 28 labels into variable target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4af580be",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [col for col in df_train.columns if col not in ['clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43261378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b7c54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63dd2ea",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42c58afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb3d783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the training set\n",
    "train_encodings = tokenizer(list(df_train['clean_text']), padding=True, truncation=True, return_tensors='pt')\n",
    "train_dataset = Dataset.from_dict({'input_ids': train_encodings['input_ids'],\n",
    "                                   'attention_mask': train_encodings['attention_mask'],\n",
    "                                   'labels': torch.tensor(df_train[target_cols].values, dtype=torch.float32)})\n",
    "\n",
    "# Tokenize the validation set\n",
    "val_encodings = tokenizer(list(df_val['clean_text']), padding=True, truncation=True, return_tensors='pt')\n",
    "valid_dataset = Dataset.from_dict({'input_ids': val_encodings['input_ids'],\n",
    "                                 'attention_mask': val_encodings['attention_mask'],\n",
    "                                 'labels': torch.tensor(df_val[target_cols].values, dtype=torch.float32)})\n",
    "\n",
    "\n",
    "test_encodings = tokenizer(list(df_test['clean_text']), padding=True, truncation=True, return_tensors='pt')\n",
    "test_dataset = Dataset.from_dict({'input_ids': test_encodings['input_ids'],\n",
    "                                 'attention_mask': test_encodings['attention_mask'],\n",
    "                                 'labels': torch.tensor(df_test[target_cols].values, dtype=torch.float32)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac35ee5",
   "metadata": {},
   "source": [
    "### Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02efe9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Model\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(target_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e91901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom callback to get train and validation info during training\n",
    "class CustomCallback(TrainerCallback):\n",
    "    \n",
    "    def __init__(self, trainer) -> None:\n",
    "        super().__init__()\n",
    "        self._trainer = trainer\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if control.should_evaluate:\n",
    "            control_copy = copy.deepcopy(control)\n",
    "            self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\")\n",
    "            return control_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1980801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='data/output/dis/copy/',\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=VALID_BATCH_SIZE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    logging_dir=\"data/output/dis/copy/logs\",\n",
    "    learning_rate=float(LEARNING_RATE),\n",
    "    weight_decay=0.2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0eae176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom function to calculate the metrics for multi label classification\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'recall' : recall_micro,\n",
    "               'precision': precision_micro,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8ee819",
   "metadata": {},
   "source": [
    "#### Compute class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cdb39da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "num_classes = len(target_cols)\n",
    "class_counts = np.sum(train_dataset['labels'], axis=0)\n",
    "class_weights = 1.0 / class_counts\n",
    "\n",
    "# Normalize weights\n",
    "class_weights /= class_weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82eeccf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00434095, 0.00770108, 0.01144105, 0.00725835, 0.00610007,\n",
       "       0.01649321, 0.01310535, 0.00818262, 0.02796899, 0.0141389 ,\n",
       "       0.00886653, 0.02260797, 0.05936463, 0.02101773, 0.03013129,\n",
       "       0.00673483, 0.23283272, 0.01234719, 0.0085945 , 0.10998846,\n",
       "       0.01133973, 0.16151459, 0.01615146, 0.11717725, 0.03289563,\n",
       "       0.01353066, 0.01691332, 0.00126095])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6f99c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom trainer to incorporate class weights\n",
    "class MultiLabelTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights: Optional[FloatTensor] = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            class_weights = class_weights.to(self.args.device)\n",
    "            logging.info(f\"Using multi-label classification with class weights\", class_weights)\n",
    "        self.loss_fct = BCEWithLogitsLoss(weight=class_weights)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        try:\n",
    "            loss = self.loss_fct(outputs.logits.view(-1, model.num_labels), labels.view(-1,model.num_labels))\n",
    "        except AttributeError:  # DataParallel\n",
    "            loss = self.loss_fct(outputs.logits.view(-1, model.module.num_labels), labels.view(-1, model.num_labels))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3287773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# create Trainer instance\n",
    "trainer = MultiLabelTrainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.add_callback(CustomCallback(trainer)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e1c6498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/aishah/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13565' max='13565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13565/13565 52:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>0.082922</td>\n",
       "      <td>0.551682</td>\n",
       "      <td>0.431865</td>\n",
       "      <td>0.763511</td>\n",
       "      <td>0.712997</td>\n",
       "      <td>0.415592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>0.089960</td>\n",
       "      <td>0.520852</td>\n",
       "      <td>0.408150</td>\n",
       "      <td>0.719536</td>\n",
       "      <td>0.700588</td>\n",
       "      <td>0.394213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.081500</td>\n",
       "      <td>0.064464</td>\n",
       "      <td>0.681497</td>\n",
       "      <td>0.581267</td>\n",
       "      <td>0.823495</td>\n",
       "      <td>0.787900</td>\n",
       "      <td>0.553032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.081500</td>\n",
       "      <td>0.086668</td>\n",
       "      <td>0.558734</td>\n",
       "      <td>0.473041</td>\n",
       "      <td>0.682342</td>\n",
       "      <td>0.731694</td>\n",
       "      <td>0.442868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.044740</td>\n",
       "      <td>0.801285</td>\n",
       "      <td>0.724837</td>\n",
       "      <td>0.895760</td>\n",
       "      <td>0.860568</td>\n",
       "      <td>0.690979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.090852</td>\n",
       "      <td>0.562280</td>\n",
       "      <td>0.488558</td>\n",
       "      <td>0.662205</td>\n",
       "      <td>0.738817</td>\n",
       "      <td>0.448581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.047200</td>\n",
       "      <td>0.030515</td>\n",
       "      <td>0.871959</td>\n",
       "      <td>0.813786</td>\n",
       "      <td>0.939091</td>\n",
       "      <td>0.905735</td>\n",
       "      <td>0.781999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.047200</td>\n",
       "      <td>0.101404</td>\n",
       "      <td>0.555124</td>\n",
       "      <td>0.493260</td>\n",
       "      <td>0.634732</td>\n",
       "      <td>0.740409</td>\n",
       "      <td>0.441209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.025094</td>\n",
       "      <td>0.899882</td>\n",
       "      <td>0.856567</td>\n",
       "      <td>0.947811</td>\n",
       "      <td>0.927249</td>\n",
       "      <td>0.822890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.108756</td>\n",
       "      <td>0.558844</td>\n",
       "      <td>0.512069</td>\n",
       "      <td>0.615023</td>\n",
       "      <td>0.749009</td>\n",
       "      <td>0.445816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13565, training_loss=0.06874172874715027, metrics={'train_runtime': 3142.0688, 'train_samples_per_second': 69.076, 'train_steps_per_second': 4.317, 'total_flos': 9325846002881280.0, 'train_loss': 0.06874172874715027, 'epoch': 5.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9dffa6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1644</td>\n",
       "      <td>4.815702e-05</td>\n",
       "      <td>0.18</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1146</td>\n",
       "      <td>4.631404e-05</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1031</td>\n",
       "      <td>4.447107e-05</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0979</td>\n",
       "      <td>4.262809e-05</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0950</td>\n",
       "      <td>4.078511e-05</td>\n",
       "      <td>0.92</td>\n",
       "      <td>2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2713</td>\n",
       "      <td>0.082922</td>\n",
       "      <td>0.551682</td>\n",
       "      <td>0.431865</td>\n",
       "      <td>0.763511</td>\n",
       "      <td>0.712997</td>\n",
       "      <td>0.415592</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089960</td>\n",
       "      <td>0.520852</td>\n",
       "      <td>0.408150</td>\n",
       "      <td>0.719536</td>\n",
       "      <td>0.700588</td>\n",
       "      <td>0.394213</td>\n",
       "      <td>11.2105</td>\n",
       "      <td>484.012</td>\n",
       "      <td>30.329</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0892</td>\n",
       "      <td>3.894213e-05</td>\n",
       "      <td>1.11</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0834</td>\n",
       "      <td>3.709915e-05</td>\n",
       "      <td>1.29</td>\n",
       "      <td>3500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0830</td>\n",
       "      <td>3.525617e-05</td>\n",
       "      <td>1.47</td>\n",
       "      <td>4000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0822</td>\n",
       "      <td>3.341320e-05</td>\n",
       "      <td>1.66</td>\n",
       "      <td>4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0815</td>\n",
       "      <td>3.157022e-05</td>\n",
       "      <td>1.84</td>\n",
       "      <td>5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5426</td>\n",
       "      <td>0.064464</td>\n",
       "      <td>0.681497</td>\n",
       "      <td>0.581267</td>\n",
       "      <td>0.823495</td>\n",
       "      <td>0.787900</td>\n",
       "      <td>0.553032</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086668</td>\n",
       "      <td>0.558734</td>\n",
       "      <td>0.473041</td>\n",
       "      <td>0.682342</td>\n",
       "      <td>0.731694</td>\n",
       "      <td>0.442868</td>\n",
       "      <td>10.9833</td>\n",
       "      <td>494.025</td>\n",
       "      <td>30.956</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0798</td>\n",
       "      <td>2.972724e-05</td>\n",
       "      <td>2.03</td>\n",
       "      <td>5500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0664</td>\n",
       "      <td>2.788426e-05</td>\n",
       "      <td>2.21</td>\n",
       "      <td>6000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0658</td>\n",
       "      <td>2.604128e-05</td>\n",
       "      <td>2.40</td>\n",
       "      <td>6500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0642</td>\n",
       "      <td>2.419830e-05</td>\n",
       "      <td>2.58</td>\n",
       "      <td>7000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0657</td>\n",
       "      <td>2.235533e-05</td>\n",
       "      <td>2.76</td>\n",
       "      <td>7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0645</td>\n",
       "      <td>2.051235e-05</td>\n",
       "      <td>2.95</td>\n",
       "      <td>8000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.044740</td>\n",
       "      <td>0.801285</td>\n",
       "      <td>0.724837</td>\n",
       "      <td>0.895760</td>\n",
       "      <td>0.860568</td>\n",
       "      <td>0.690979</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>8139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090852</td>\n",
       "      <td>0.562280</td>\n",
       "      <td>0.488558</td>\n",
       "      <td>0.662205</td>\n",
       "      <td>0.738817</td>\n",
       "      <td>0.448581</td>\n",
       "      <td>11.1008</td>\n",
       "      <td>488.792</td>\n",
       "      <td>30.628</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0520</td>\n",
       "      <td>1.866937e-05</td>\n",
       "      <td>3.13</td>\n",
       "      <td>8500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0470</td>\n",
       "      <td>1.682639e-05</td>\n",
       "      <td>3.32</td>\n",
       "      <td>9000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0478</td>\n",
       "      <td>1.498341e-05</td>\n",
       "      <td>3.50</td>\n",
       "      <td>9500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0470</td>\n",
       "      <td>1.314043e-05</td>\n",
       "      <td>3.69</td>\n",
       "      <td>10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0472</td>\n",
       "      <td>1.129746e-05</td>\n",
       "      <td>3.87</td>\n",
       "      <td>10500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.00</td>\n",
       "      <td>10852</td>\n",
       "      <td>0.030515</td>\n",
       "      <td>0.871959</td>\n",
       "      <td>0.813786</td>\n",
       "      <td>0.939091</td>\n",
       "      <td>0.905735</td>\n",
       "      <td>0.781999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.00</td>\n",
       "      <td>10852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101404</td>\n",
       "      <td>0.555124</td>\n",
       "      <td>0.493260</td>\n",
       "      <td>0.634732</td>\n",
       "      <td>0.740409</td>\n",
       "      <td>0.441209</td>\n",
       "      <td>11.1271</td>\n",
       "      <td>487.639</td>\n",
       "      <td>30.556</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0447</td>\n",
       "      <td>9.454478e-06</td>\n",
       "      <td>4.05</td>\n",
       "      <td>11000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0346</td>\n",
       "      <td>7.611500e-06</td>\n",
       "      <td>4.24</td>\n",
       "      <td>11500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0351</td>\n",
       "      <td>5.768522e-06</td>\n",
       "      <td>4.42</td>\n",
       "      <td>12000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0356</td>\n",
       "      <td>3.925544e-06</td>\n",
       "      <td>4.61</td>\n",
       "      <td>12500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0342</td>\n",
       "      <td>2.082565e-06</td>\n",
       "      <td>4.79</td>\n",
       "      <td>13000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0349</td>\n",
       "      <td>2.395872e-07</td>\n",
       "      <td>4.98</td>\n",
       "      <td>13500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.025094</td>\n",
       "      <td>0.899882</td>\n",
       "      <td>0.856567</td>\n",
       "      <td>0.947811</td>\n",
       "      <td>0.927249</td>\n",
       "      <td>0.822890</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108756</td>\n",
       "      <td>0.558844</td>\n",
       "      <td>0.512069</td>\n",
       "      <td>0.615023</td>\n",
       "      <td>0.749009</td>\n",
       "      <td>0.445816</td>\n",
       "      <td>11.3898</td>\n",
       "      <td>476.389</td>\n",
       "      <td>29.851</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.068742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.325846e+15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      loss  learning_rate  epoch   step  train_loss  train_f1  train_recall  \\\n",
       "0   0.1644   4.815702e-05   0.18    500         NaN       NaN           NaN   \n",
       "1   0.1146   4.631404e-05   0.37   1000         NaN       NaN           NaN   \n",
       "2   0.1031   4.447107e-05   0.55   1500         NaN       NaN           NaN   \n",
       "3   0.0979   4.262809e-05   0.74   2000         NaN       NaN           NaN   \n",
       "4   0.0950   4.078511e-05   0.92   2500         NaN       NaN           NaN   \n",
       "5      NaN            NaN   1.00   2713    0.082922  0.551682      0.431865   \n",
       "6      NaN            NaN   1.00   2713         NaN       NaN           NaN   \n",
       "7   0.0892   3.894213e-05   1.11   3000         NaN       NaN           NaN   \n",
       "8   0.0834   3.709915e-05   1.29   3500         NaN       NaN           NaN   \n",
       "9   0.0830   3.525617e-05   1.47   4000         NaN       NaN           NaN   \n",
       "10  0.0822   3.341320e-05   1.66   4500         NaN       NaN           NaN   \n",
       "11  0.0815   3.157022e-05   1.84   5000         NaN       NaN           NaN   \n",
       "12     NaN            NaN   2.00   5426    0.064464  0.681497      0.581267   \n",
       "13     NaN            NaN   2.00   5426         NaN       NaN           NaN   \n",
       "14  0.0798   2.972724e-05   2.03   5500         NaN       NaN           NaN   \n",
       "15  0.0664   2.788426e-05   2.21   6000         NaN       NaN           NaN   \n",
       "16  0.0658   2.604128e-05   2.40   6500         NaN       NaN           NaN   \n",
       "17  0.0642   2.419830e-05   2.58   7000         NaN       NaN           NaN   \n",
       "18  0.0657   2.235533e-05   2.76   7500         NaN       NaN           NaN   \n",
       "19  0.0645   2.051235e-05   2.95   8000         NaN       NaN           NaN   \n",
       "20     NaN            NaN   3.00   8139    0.044740  0.801285      0.724837   \n",
       "21     NaN            NaN   3.00   8139         NaN       NaN           NaN   \n",
       "22  0.0520   1.866937e-05   3.13   8500         NaN       NaN           NaN   \n",
       "23  0.0470   1.682639e-05   3.32   9000         NaN       NaN           NaN   \n",
       "24  0.0478   1.498341e-05   3.50   9500         NaN       NaN           NaN   \n",
       "25  0.0470   1.314043e-05   3.69  10000         NaN       NaN           NaN   \n",
       "26  0.0472   1.129746e-05   3.87  10500         NaN       NaN           NaN   \n",
       "27     NaN            NaN   4.00  10852    0.030515  0.871959      0.813786   \n",
       "28     NaN            NaN   4.00  10852         NaN       NaN           NaN   \n",
       "29  0.0447   9.454478e-06   4.05  11000         NaN       NaN           NaN   \n",
       "30  0.0346   7.611500e-06   4.24  11500         NaN       NaN           NaN   \n",
       "31  0.0351   5.768522e-06   4.42  12000         NaN       NaN           NaN   \n",
       "32  0.0356   3.925544e-06   4.61  12500         NaN       NaN           NaN   \n",
       "33  0.0342   2.082565e-06   4.79  13000         NaN       NaN           NaN   \n",
       "34  0.0349   2.395872e-07   4.98  13500         NaN       NaN           NaN   \n",
       "35     NaN            NaN   5.00  13565    0.025094  0.899882      0.856567   \n",
       "36     NaN            NaN   5.00  13565         NaN       NaN           NaN   \n",
       "37     NaN            NaN   5.00  13565    0.068742       NaN           NaN   \n",
       "\n",
       "    train_precision  train_roc_auc  train_accuracy  ...  eval_loss   eval_f1  \\\n",
       "0               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "1               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "2               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "3               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "4               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "5          0.763511       0.712997        0.415592  ...        NaN       NaN   \n",
       "6               NaN            NaN             NaN  ...   0.089960  0.520852   \n",
       "7               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "8               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "9               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "10              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "11              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "12         0.823495       0.787900        0.553032  ...        NaN       NaN   \n",
       "13              NaN            NaN             NaN  ...   0.086668  0.558734   \n",
       "14              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "15              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "16              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "17              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "18              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "19              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "20         0.895760       0.860568        0.690979  ...        NaN       NaN   \n",
       "21              NaN            NaN             NaN  ...   0.090852  0.562280   \n",
       "22              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "23              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "24              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "25              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "26              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "27         0.939091       0.905735        0.781999  ...        NaN       NaN   \n",
       "28              NaN            NaN             NaN  ...   0.101404  0.555124   \n",
       "29              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "30              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "31              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "32              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "33              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "34              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "35         0.947811       0.927249        0.822890  ...        NaN       NaN   \n",
       "36              NaN            NaN             NaN  ...   0.108756  0.558844   \n",
       "37              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "\n",
       "    eval_recall  eval_precision  eval_roc_auc  eval_accuracy  eval_runtime  \\\n",
       "0           NaN             NaN           NaN            NaN           NaN   \n",
       "1           NaN             NaN           NaN            NaN           NaN   \n",
       "2           NaN             NaN           NaN            NaN           NaN   \n",
       "3           NaN             NaN           NaN            NaN           NaN   \n",
       "4           NaN             NaN           NaN            NaN           NaN   \n",
       "5           NaN             NaN           NaN            NaN           NaN   \n",
       "6      0.408150        0.719536      0.700588       0.394213       11.2105   \n",
       "7           NaN             NaN           NaN            NaN           NaN   \n",
       "8           NaN             NaN           NaN            NaN           NaN   \n",
       "9           NaN             NaN           NaN            NaN           NaN   \n",
       "10          NaN             NaN           NaN            NaN           NaN   \n",
       "11          NaN             NaN           NaN            NaN           NaN   \n",
       "12          NaN             NaN           NaN            NaN           NaN   \n",
       "13     0.473041        0.682342      0.731694       0.442868       10.9833   \n",
       "14          NaN             NaN           NaN            NaN           NaN   \n",
       "15          NaN             NaN           NaN            NaN           NaN   \n",
       "16          NaN             NaN           NaN            NaN           NaN   \n",
       "17          NaN             NaN           NaN            NaN           NaN   \n",
       "18          NaN             NaN           NaN            NaN           NaN   \n",
       "19          NaN             NaN           NaN            NaN           NaN   \n",
       "20          NaN             NaN           NaN            NaN           NaN   \n",
       "21     0.488558        0.662205      0.738817       0.448581       11.1008   \n",
       "22          NaN             NaN           NaN            NaN           NaN   \n",
       "23          NaN             NaN           NaN            NaN           NaN   \n",
       "24          NaN             NaN           NaN            NaN           NaN   \n",
       "25          NaN             NaN           NaN            NaN           NaN   \n",
       "26          NaN             NaN           NaN            NaN           NaN   \n",
       "27          NaN             NaN           NaN            NaN           NaN   \n",
       "28     0.493260        0.634732      0.740409       0.441209       11.1271   \n",
       "29          NaN             NaN           NaN            NaN           NaN   \n",
       "30          NaN             NaN           NaN            NaN           NaN   \n",
       "31          NaN             NaN           NaN            NaN           NaN   \n",
       "32          NaN             NaN           NaN            NaN           NaN   \n",
       "33          NaN             NaN           NaN            NaN           NaN   \n",
       "34          NaN             NaN           NaN            NaN           NaN   \n",
       "35          NaN             NaN           NaN            NaN           NaN   \n",
       "36     0.512069        0.615023      0.749009       0.445816       11.3898   \n",
       "37          NaN             NaN           NaN            NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second    total_flos  \n",
       "0                       NaN                    NaN           NaN  \n",
       "1                       NaN                    NaN           NaN  \n",
       "2                       NaN                    NaN           NaN  \n",
       "3                       NaN                    NaN           NaN  \n",
       "4                       NaN                    NaN           NaN  \n",
       "5                       NaN                    NaN           NaN  \n",
       "6                   484.012                 30.329           NaN  \n",
       "7                       NaN                    NaN           NaN  \n",
       "8                       NaN                    NaN           NaN  \n",
       "9                       NaN                    NaN           NaN  \n",
       "10                      NaN                    NaN           NaN  \n",
       "11                      NaN                    NaN           NaN  \n",
       "12                      NaN                    NaN           NaN  \n",
       "13                  494.025                 30.956           NaN  \n",
       "14                      NaN                    NaN           NaN  \n",
       "15                      NaN                    NaN           NaN  \n",
       "16                      NaN                    NaN           NaN  \n",
       "17                      NaN                    NaN           NaN  \n",
       "18                      NaN                    NaN           NaN  \n",
       "19                      NaN                    NaN           NaN  \n",
       "20                      NaN                    NaN           NaN  \n",
       "21                  488.792                 30.628           NaN  \n",
       "22                      NaN                    NaN           NaN  \n",
       "23                      NaN                    NaN           NaN  \n",
       "24                      NaN                    NaN           NaN  \n",
       "25                      NaN                    NaN           NaN  \n",
       "26                      NaN                    NaN           NaN  \n",
       "27                      NaN                    NaN           NaN  \n",
       "28                  487.639                 30.556           NaN  \n",
       "29                      NaN                    NaN           NaN  \n",
       "30                      NaN                    NaN           NaN  \n",
       "31                      NaN                    NaN           NaN  \n",
       "32                      NaN                    NaN           NaN  \n",
       "33                      NaN                    NaN           NaN  \n",
       "34                      NaN                    NaN           NaN  \n",
       "35                      NaN                    NaN           NaN  \n",
       "36                  476.389                 29.851           NaN  \n",
       "37                      NaN                    NaN  9.325846e+15  \n",
       "\n",
       "[38 rows x 23 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view train and validation metrics from training\n",
    "log_history = pd.DataFrame(trainer.state.log_history)\n",
    "log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9edd140f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2713</td>\n",
       "      <td>0.082922</td>\n",
       "      <td>0.551682</td>\n",
       "      <td>0.431865</td>\n",
       "      <td>0.763511</td>\n",
       "      <td>0.712997</td>\n",
       "      <td>0.415592</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5426</td>\n",
       "      <td>0.064464</td>\n",
       "      <td>0.681497</td>\n",
       "      <td>0.581267</td>\n",
       "      <td>0.823495</td>\n",
       "      <td>0.787900</td>\n",
       "      <td>0.553032</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.044740</td>\n",
       "      <td>0.801285</td>\n",
       "      <td>0.724837</td>\n",
       "      <td>0.895760</td>\n",
       "      <td>0.860568</td>\n",
       "      <td>0.690979</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10852</td>\n",
       "      <td>0.030515</td>\n",
       "      <td>0.871959</td>\n",
       "      <td>0.813786</td>\n",
       "      <td>0.939091</td>\n",
       "      <td>0.905735</td>\n",
       "      <td>0.781999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.025094</td>\n",
       "      <td>0.899882</td>\n",
       "      <td>0.856567</td>\n",
       "      <td>0.947811</td>\n",
       "      <td>0.927249</td>\n",
       "      <td>0.822890</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    loss  learning_rate  epoch   step  train_loss  train_f1  train_recall  \\\n",
       "5    NaN            NaN    1.0   2713    0.082922  0.551682      0.431865   \n",
       "12   NaN            NaN    2.0   5426    0.064464  0.681497      0.581267   \n",
       "20   NaN            NaN    3.0   8139    0.044740  0.801285      0.724837   \n",
       "27   NaN            NaN    4.0  10852    0.030515  0.871959      0.813786   \n",
       "35   NaN            NaN    5.0  13565    0.025094  0.899882      0.856567   \n",
       "\n",
       "    train_precision  train_roc_auc  train_accuracy  ...  eval_loss  eval_f1  \\\n",
       "5          0.763511       0.712997        0.415592  ...        NaN      NaN   \n",
       "12         0.823495       0.787900        0.553032  ...        NaN      NaN   \n",
       "20         0.895760       0.860568        0.690979  ...        NaN      NaN   \n",
       "27         0.939091       0.905735        0.781999  ...        NaN      NaN   \n",
       "35         0.947811       0.927249        0.822890  ...        NaN      NaN   \n",
       "\n",
       "    eval_recall  eval_precision  eval_roc_auc  eval_accuracy  eval_runtime  \\\n",
       "5           NaN             NaN           NaN            NaN           NaN   \n",
       "12          NaN             NaN           NaN            NaN           NaN   \n",
       "20          NaN             NaN           NaN            NaN           NaN   \n",
       "27          NaN             NaN           NaN            NaN           NaN   \n",
       "35          NaN             NaN           NaN            NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  total_flos  \n",
       "5                       NaN                    NaN         NaN  \n",
       "12                      NaN                    NaN         NaN  \n",
       "20                      NaN                    NaN         NaN  \n",
       "27                      NaN                    NaN         NaN  \n",
       "35                      NaN                    NaN         NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store train metrics in dataframe\n",
    "train_history = log_history[log_history['train_f1'].notna()]\n",
    "train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7c76fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089960</td>\n",
       "      <td>0.520852</td>\n",
       "      <td>0.408150</td>\n",
       "      <td>0.719536</td>\n",
       "      <td>0.700588</td>\n",
       "      <td>0.394213</td>\n",
       "      <td>11.2105</td>\n",
       "      <td>484.012</td>\n",
       "      <td>30.329</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086668</td>\n",
       "      <td>0.558734</td>\n",
       "      <td>0.473041</td>\n",
       "      <td>0.682342</td>\n",
       "      <td>0.731694</td>\n",
       "      <td>0.442868</td>\n",
       "      <td>10.9833</td>\n",
       "      <td>494.025</td>\n",
       "      <td>30.956</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090852</td>\n",
       "      <td>0.562280</td>\n",
       "      <td>0.488558</td>\n",
       "      <td>0.662205</td>\n",
       "      <td>0.738817</td>\n",
       "      <td>0.448581</td>\n",
       "      <td>11.1008</td>\n",
       "      <td>488.792</td>\n",
       "      <td>30.628</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101404</td>\n",
       "      <td>0.555124</td>\n",
       "      <td>0.493260</td>\n",
       "      <td>0.634732</td>\n",
       "      <td>0.740409</td>\n",
       "      <td>0.441209</td>\n",
       "      <td>11.1271</td>\n",
       "      <td>487.639</td>\n",
       "      <td>30.556</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108756</td>\n",
       "      <td>0.558844</td>\n",
       "      <td>0.512069</td>\n",
       "      <td>0.615023</td>\n",
       "      <td>0.749009</td>\n",
       "      <td>0.445816</td>\n",
       "      <td>11.3898</td>\n",
       "      <td>476.389</td>\n",
       "      <td>29.851</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    loss  learning_rate  epoch   step  train_loss  train_f1  train_recall  \\\n",
       "6    NaN            NaN    1.0   2713         NaN       NaN           NaN   \n",
       "13   NaN            NaN    2.0   5426         NaN       NaN           NaN   \n",
       "21   NaN            NaN    3.0   8139         NaN       NaN           NaN   \n",
       "28   NaN            NaN    4.0  10852         NaN       NaN           NaN   \n",
       "36   NaN            NaN    5.0  13565         NaN       NaN           NaN   \n",
       "\n",
       "    train_precision  train_roc_auc  train_accuracy  ...  eval_loss   eval_f1  \\\n",
       "6               NaN            NaN             NaN  ...   0.089960  0.520852   \n",
       "13              NaN            NaN             NaN  ...   0.086668  0.558734   \n",
       "21              NaN            NaN             NaN  ...   0.090852  0.562280   \n",
       "28              NaN            NaN             NaN  ...   0.101404  0.555124   \n",
       "36              NaN            NaN             NaN  ...   0.108756  0.558844   \n",
       "\n",
       "    eval_recall  eval_precision  eval_roc_auc  eval_accuracy  eval_runtime  \\\n",
       "6      0.408150        0.719536      0.700588       0.394213       11.2105   \n",
       "13     0.473041        0.682342      0.731694       0.442868       10.9833   \n",
       "21     0.488558        0.662205      0.738817       0.448581       11.1008   \n",
       "28     0.493260        0.634732      0.740409       0.441209       11.1271   \n",
       "36     0.512069        0.615023      0.749009       0.445816       11.3898   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  total_flos  \n",
       "6                   484.012                 30.329         NaN  \n",
       "13                  494.025                 30.956         NaN  \n",
       "21                  488.792                 30.628         NaN  \n",
       "28                  487.639                 30.556         NaN  \n",
       "36                  476.389                 29.851         NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store validation metrics in dataframe\n",
    "val_history = log_history[log_history['eval_f1'].notna()]\n",
    "val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1febf2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_x</th>\n",
       "      <th>learning_rate_x</th>\n",
       "      <th>epoch_x</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss_x</th>\n",
       "      <th>train_f1_x</th>\n",
       "      <th>train_recall_x</th>\n",
       "      <th>train_precision_x</th>\n",
       "      <th>train_roc_auc_x</th>\n",
       "      <th>train_accuracy_x</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss_y</th>\n",
       "      <th>eval_f1_y</th>\n",
       "      <th>eval_recall_y</th>\n",
       "      <th>eval_precision_y</th>\n",
       "      <th>eval_roc_auc_y</th>\n",
       "      <th>eval_accuracy_y</th>\n",
       "      <th>eval_runtime_y</th>\n",
       "      <th>eval_samples_per_second_y</th>\n",
       "      <th>eval_steps_per_second_y</th>\n",
       "      <th>total_flos_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2713</td>\n",
       "      <td>0.082922</td>\n",
       "      <td>0.551682</td>\n",
       "      <td>0.431865</td>\n",
       "      <td>0.763511</td>\n",
       "      <td>0.712997</td>\n",
       "      <td>0.415592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089960</td>\n",
       "      <td>0.520852</td>\n",
       "      <td>0.408150</td>\n",
       "      <td>0.719536</td>\n",
       "      <td>0.700588</td>\n",
       "      <td>0.394213</td>\n",
       "      <td>11.2105</td>\n",
       "      <td>484.012</td>\n",
       "      <td>30.329</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5426</td>\n",
       "      <td>0.064464</td>\n",
       "      <td>0.681497</td>\n",
       "      <td>0.581267</td>\n",
       "      <td>0.823495</td>\n",
       "      <td>0.787900</td>\n",
       "      <td>0.553032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086668</td>\n",
       "      <td>0.558734</td>\n",
       "      <td>0.473041</td>\n",
       "      <td>0.682342</td>\n",
       "      <td>0.731694</td>\n",
       "      <td>0.442868</td>\n",
       "      <td>10.9833</td>\n",
       "      <td>494.025</td>\n",
       "      <td>30.956</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.044740</td>\n",
       "      <td>0.801285</td>\n",
       "      <td>0.724837</td>\n",
       "      <td>0.895760</td>\n",
       "      <td>0.860568</td>\n",
       "      <td>0.690979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090852</td>\n",
       "      <td>0.562280</td>\n",
       "      <td>0.488558</td>\n",
       "      <td>0.662205</td>\n",
       "      <td>0.738817</td>\n",
       "      <td>0.448581</td>\n",
       "      <td>11.1008</td>\n",
       "      <td>488.792</td>\n",
       "      <td>30.628</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10852</td>\n",
       "      <td>0.030515</td>\n",
       "      <td>0.871959</td>\n",
       "      <td>0.813786</td>\n",
       "      <td>0.939091</td>\n",
       "      <td>0.905735</td>\n",
       "      <td>0.781999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101404</td>\n",
       "      <td>0.555124</td>\n",
       "      <td>0.493260</td>\n",
       "      <td>0.634732</td>\n",
       "      <td>0.740409</td>\n",
       "      <td>0.441209</td>\n",
       "      <td>11.1271</td>\n",
       "      <td>487.639</td>\n",
       "      <td>30.556</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.025094</td>\n",
       "      <td>0.899882</td>\n",
       "      <td>0.856567</td>\n",
       "      <td>0.947811</td>\n",
       "      <td>0.927249</td>\n",
       "      <td>0.822890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108756</td>\n",
       "      <td>0.558844</td>\n",
       "      <td>0.512069</td>\n",
       "      <td>0.615023</td>\n",
       "      <td>0.749009</td>\n",
       "      <td>0.445816</td>\n",
       "      <td>11.3898</td>\n",
       "      <td>476.389</td>\n",
       "      <td>29.851</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loss_x  learning_rate_x  epoch_x   step  train_loss_x  train_f1_x  \\\n",
       "0     NaN              NaN      1.0   2713      0.082922    0.551682   \n",
       "1     NaN              NaN      2.0   5426      0.064464    0.681497   \n",
       "2     NaN              NaN      3.0   8139      0.044740    0.801285   \n",
       "3     NaN              NaN      4.0  10852      0.030515    0.871959   \n",
       "4     NaN              NaN      5.0  13565      0.025094    0.899882   \n",
       "\n",
       "   train_recall_x  train_precision_x  train_roc_auc_x  train_accuracy_x  ...  \\\n",
       "0        0.431865           0.763511         0.712997          0.415592  ...   \n",
       "1        0.581267           0.823495         0.787900          0.553032  ...   \n",
       "2        0.724837           0.895760         0.860568          0.690979  ...   \n",
       "3        0.813786           0.939091         0.905735          0.781999  ...   \n",
       "4        0.856567           0.947811         0.927249          0.822890  ...   \n",
       "\n",
       "   eval_loss_y  eval_f1_y  eval_recall_y  eval_precision_y  eval_roc_auc_y  \\\n",
       "0     0.089960   0.520852       0.408150          0.719536        0.700588   \n",
       "1     0.086668   0.558734       0.473041          0.682342        0.731694   \n",
       "2     0.090852   0.562280       0.488558          0.662205        0.738817   \n",
       "3     0.101404   0.555124       0.493260          0.634732        0.740409   \n",
       "4     0.108756   0.558844       0.512069          0.615023        0.749009   \n",
       "\n",
       "   eval_accuracy_y  eval_runtime_y  eval_samples_per_second_y  \\\n",
       "0         0.394213         11.2105                    484.012   \n",
       "1         0.442868         10.9833                    494.025   \n",
       "2         0.448581         11.1008                    488.792   \n",
       "3         0.441209         11.1271                    487.639   \n",
       "4         0.445816         11.3898                    476.389   \n",
       "\n",
       "   eval_steps_per_second_y  total_flos_y  \n",
       "0                   30.329           NaN  \n",
       "1                   30.956           NaN  \n",
       "2                   30.628           NaN  \n",
       "3                   30.556           NaN  \n",
       "4                   29.851           NaN  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = pd.merge(train_history, val_history, on='step', how='outer')\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ebe0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting function to check for accuracy with graphs \n",
    "def plot_model_performance(history):\n",
    "\n",
    "    #getting train and validation accuracy\n",
    "    acc = history['train_accuracy_x']\n",
    "    val_acc = history['eval_accuracy_y']\n",
    "\n",
    "    #getting train and validation loss\n",
    "    loss = history['train_loss_x']\n",
    "    val_loss = history['eval_loss_y']\n",
    "\n",
    "    epochs_range = range(5)\n",
    "\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c97c350d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_model_performance(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1630a8",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5456e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to evaluate the model on the test set\n",
    "def evaluate_model(test_dataset):\n",
    "    test_res = trainer.evaluate(eval_dataset=test_dataset)\n",
    "    print(test_res)\n",
    "         \n",
    "    return trainer.predict(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7bb617c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09063799679279327, 'eval_f1': 0.5601380813953488, 'eval_recall': 0.48712276820982775, 'eval_precision': 0.6589014746740757, 'eval_roc_auc': 0.7380816310371449, 'eval_accuracy': 0.4429703335175972, 'eval_runtime': 7.5225, 'eval_samples_per_second': 721.433, 'eval_steps_per_second': 45.198, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "y_test = evaluate_model(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c172305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate each emotion label metrics on test set\n",
    "def calc_label_metrics(label, y_targets, y_preds, threshold):\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"accuracy\": metrics.accuracy_score(y_targets, y_preds),\n",
    "        \"precision\": metrics.precision_score(y_targets, y_preds, zero_division=0),\n",
    "        \"recall\": metrics.recall_score(y_targets, y_preds, zero_division=0),\n",
    "        \"f1\": metrics.f1_score(y_targets, y_preds, zero_division=0),\n",
    "        \"mcc\": metrics.matthews_corrcoef(y_targets, y_preds),\n",
    "        \"support\": y_targets.sum(),\n",
    "        \"threshold\": threshold,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78c37071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate overall metric on test set\n",
    "def calc_test_metrics(y_test, test_dataset, target_cols):\n",
    "    threshold = 0.5\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(y_test.predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= 0.5)] = 1\n",
    "    \n",
    "    # finally, compute metrics\n",
    "    y_true = df_test[target_cols].values\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'recall' : recall_micro,\n",
    "               'precision': precision_micro,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    metrics_df = pd.DataFrame.from_dict(metrics, orient='index', columns=['Value'])\n",
    "\n",
    "    display(metrics_df)\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for label_index, label in enumerate(target_cols):\n",
    "        y_targets, y_preds = y_true[:, label_index], y_pred[:, label_index]\n",
    "        results.append(calc_label_metrics(label, y_targets, y_preds, threshold))\n",
    "\n",
    "    per_label_results = pd.DataFrame(results, index=target_cols)\n",
    "    display(per_label_results.drop(columns=[\"label\"]).round(3))\n",
    "    \n",
    "    return y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "794fec4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.560138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.487123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.658901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc</th>\n",
       "      <td>0.738082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.442970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "f1         0.560138\n",
       "recall     0.487123\n",
       "precision  0.658901\n",
       "roc_auc    0.738082\n",
       "accuracy   0.442970"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>mcc</th>\n",
       "      <th>support</th>\n",
       "      <th>threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.936</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.617</td>\n",
       "      <td>504</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.982</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.808</td>\n",
       "      <td>264</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.966</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.504</td>\n",
       "      <td>198</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.939</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.256</td>\n",
       "      <td>320</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.935</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.346</td>\n",
       "      <td>351</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.974</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.366</td>\n",
       "      <td>135</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.973</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.456</td>\n",
       "      <td>153</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.948</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.372</td>\n",
       "      <td>284</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.987</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.469</td>\n",
       "      <td>83</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.972</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.244</td>\n",
       "      <td>151</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.947</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.358</td>\n",
       "      <td>267</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.982</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.484</td>\n",
       "      <td>123</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.995</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.481</td>\n",
       "      <td>37</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.982</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.382</td>\n",
       "      <td>103</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.990</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.653</td>\n",
       "      <td>78</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.989</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.908</td>\n",
       "      <td>352</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.999</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.975</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.523</td>\n",
       "      <td>161</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.981</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.776</td>\n",
       "      <td>238</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.995</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.219</td>\n",
       "      <td>23</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.973</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.525</td>\n",
       "      <td>186</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.998</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.446</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.974</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.266</td>\n",
       "      <td>145</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.998</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.993</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.651</td>\n",
       "      <td>56</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.977</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.547</td>\n",
       "      <td>156</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.977</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.510</td>\n",
       "      <td>141</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.769</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.452</td>\n",
       "      <td>1787</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                accuracy  precision  recall     f1    mcc  support  threshold\n",
       "admiration         0.936      0.663   0.641  0.652  0.617      504        0.5\n",
       "amusement          0.982      0.791   0.845  0.817  0.808      264        0.5\n",
       "anger              0.966      0.544   0.500  0.521  0.504      198        0.5\n",
       "annoyance          0.939      0.462   0.172  0.251  0.256      320        0.5\n",
       "approval           0.935      0.500   0.285  0.363  0.346      351        0.5\n",
       "caring             0.974      0.462   0.311  0.372  0.366      135        0.5\n",
       "confusion          0.973      0.520   0.425  0.468  0.456      153        0.5\n",
       "curiosity          0.948      0.509   0.310  0.385  0.372      284        0.5\n",
       "desire             0.987      0.667   0.337  0.448  0.469       83        0.5\n",
       "disappointment     0.972      0.488   0.132  0.208  0.244      151        0.5\n",
       "disapproval        0.947      0.445   0.333  0.381  0.358      267        0.5\n",
       "disgust            0.982      0.741   0.325  0.452  0.484      123        0.5\n",
       "embarrassment      0.995      0.786   0.297  0.431  0.481       37        0.5\n",
       "excitement         0.982      0.560   0.272  0.366  0.382      103        0.5\n",
       "fear               0.990      0.676   0.641  0.658  0.653       78        0.5\n",
       "gratitude          0.989      0.920   0.909  0.914  0.908      352        0.5\n",
       "grief              0.999      0.000   0.000  0.000  0.000        6        0.5\n",
       "joy                0.975      0.608   0.472  0.531  0.523      161        0.5\n",
       "love               0.981      0.778   0.794  0.786  0.776      238        0.5\n",
       "nervousness        0.995      0.375   0.130  0.194  0.219       23        0.5\n",
       "optimism           0.973      0.672   0.430  0.525  0.525      186        0.5\n",
       "pride              0.998      0.800   0.250  0.381  0.446       16        0.5\n",
       "realization        0.974      0.576   0.131  0.213  0.266      145        0.5\n",
       "relief             0.998      0.000   0.000  0.000  0.000       11        0.5\n",
       "remorse            0.993      0.649   0.661  0.655  0.651       56        0.5\n",
       "sadness            0.977      0.631   0.494  0.554  0.547      156        0.5\n",
       "surprise           0.977      0.556   0.489  0.521  0.510      141        0.5\n",
       "neutral            0.769      0.697   0.530  0.602  0.452     1787        0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "targets, outputs = calc_test_metrics(y_test, test_dataset, target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f1631d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results DataFrame:\n",
      "         Actual                Predicted\n",
      "0     [sadness]                   [love]\n",
      "1  [admiration]             [admiration]\n",
      "2  [excitement]               [optimism]\n",
      "3   [gratitude]              [gratitude]\n",
      "4     [neutral]                [neutral]\n",
      "5   [gratitude]              [gratitude]\n",
      "6   [gratitude]              [gratitude]\n",
      "7   [gratitude]  [admiration, gratitude]\n",
      "8     [remorse]       [remorse, sadness]\n",
      "9     [sadness]                [sadness]\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to store actual labels and predicted labels\n",
    "final_df = pd.DataFrame({\n",
    "    'Actual': [list(np.where(targets[i])[0]) for i in range(len(targets))],\n",
    "    'Predicted': [list(np.where(outputs[i])[0]) for i in range(len(outputs))]\n",
    "})\n",
    "\n",
    "# Map label indices to label names in the 'Actual' column\n",
    "final_df['Actual'] = final_df['Actual'].apply(lambda indices: [target_cols[idx] for idx in indices])\n",
    "\n",
    "# Map label indices to label names in the 'Predicted' column\n",
    "final_df['Predicted'] = final_df['Predicted'].apply(lambda indices: [target_cols[idx] for idx in indices])\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(\"Results DataFrame:\")\n",
    "print(final_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "301ef201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the label DataFrame with the original DataFramev\n",
    "val_df_terms = df_test['clean_text']\n",
    "result_df = pd.concat([val_df_terms, final_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a335fd5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am really sorry about your situation frown s...</td>\n",
       "      <td>[sadness]</td>\n",
       "      <td>[love]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it is wonderful because it is awful at not with</td>\n",
       "      <td>[admiration]</td>\n",
       "      <td>[admiration]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kings fan here good luck to you guys will be a...</td>\n",
       "      <td>[excitement]</td>\n",
       "      <td>[optimism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i did not know that thank you for teaching me ...</td>\n",
       "      <td>[gratitude]</td>\n",
       "      <td>[gratitude]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>they got bored from haunting earth for thousan...</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5422</th>\n",
       "      <td>thanks i was diagnosed with bp 1 after the hos...</td>\n",
       "      <td>[gratitude]</td>\n",
       "      <td>[gratitude]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5423</th>\n",
       "      <td>well that makes sense</td>\n",
       "      <td>[approval]</td>\n",
       "      <td>[approval]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5424</th>\n",
       "      <td>daddy issues name</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5425</th>\n",
       "      <td>so glad i discovered that subreddit a couple m...</td>\n",
       "      <td>[admiration]</td>\n",
       "      <td>[admiration, joy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5426</th>\n",
       "      <td>had to watch elmo in grouchland one time too m...</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5427 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_text        Actual  \\\n",
       "0     i am really sorry about your situation frown s...     [sadness]   \n",
       "1       it is wonderful because it is awful at not with  [admiration]   \n",
       "2     kings fan here good luck to you guys will be a...  [excitement]   \n",
       "3     i did not know that thank you for teaching me ...   [gratitude]   \n",
       "4     they got bored from haunting earth for thousan...     [neutral]   \n",
       "...                                                 ...           ...   \n",
       "5422  thanks i was diagnosed with bp 1 after the hos...   [gratitude]   \n",
       "5423                              well that makes sense    [approval]   \n",
       "5424                                  daddy issues name     [neutral]   \n",
       "5425  so glad i discovered that subreddit a couple m...  [admiration]   \n",
       "5426  had to watch elmo in grouchland one time too m...     [neutral]   \n",
       "\n",
       "              Predicted  \n",
       "0                [love]  \n",
       "1          [admiration]  \n",
       "2            [optimism]  \n",
       "3           [gratitude]  \n",
       "4             [neutral]  \n",
       "...                 ...  \n",
       "5422        [gratitude]  \n",
       "5423         [approval]  \n",
       "5424          [neutral]  \n",
       "5425  [admiration, joy]  \n",
       "5426          [neutral]  \n",
       "\n",
       "[5427 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14962f63",
   "metadata": {},
   "source": [
    "## 4. Save the output, tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5113b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('output_distilbert_m3.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b8b778c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer\n",
    "tokenizer.save_pretrained('./distilbert_M3_transformer/')\n",
    "\n",
    "# Save model\n",
    "trainer.save_model('./distilbert_M3_transformer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba9b95b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
