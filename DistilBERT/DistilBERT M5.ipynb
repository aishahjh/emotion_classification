{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3746d309",
   "metadata": {},
   "source": [
    "# DistilBERT M5 (with wordNet Augmented dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a30733",
   "metadata": {},
   "source": [
    "Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8380697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019c1d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn ,cuda\n",
    "from torch.utils.data import DataLoader,Dataset,RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "import nltk.corpus\n",
    "from sklearn import metrics\n",
    "from scipy.special import softmax\n",
    "\n",
    "import transformers\n",
    "from transformers import  AutoTokenizer, AutoModel\n",
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer,TrainerCallback, EarlyStoppingCallback\n",
    "import glob\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from transformers import EvalPrediction   \n",
    "import copy\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from typing import Optional\n",
    "from torch import FloatTensor\n",
    "from torch.nn import BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cae800",
   "metadata": {},
   "source": [
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9fc1af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_path = 'data/clean/'\n",
    "file_pattern = folder_path + '*.csv'\n",
    "csv_files = glob.glob(file_pattern)\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    if 'train' in csv_file:\n",
    "        df_train = pd.read_csv(csv_file)\n",
    "    elif 'val' in csv_file:\n",
    "        df_val = pd.read_csv(csv_file)\n",
    "    else:\n",
    "        df_test = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74ee6884",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the augmented dataset in the train dataframe\n",
    "df_train = pd.read_csv('data/clean/augmented/augmented_wordnet_subs_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0b05fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>disappointment</th>\n",
       "      <th>...</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>my best loved food is anything i did not have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>now if helium does off himself everyone will g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>why the piece of tail be bayless isoing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>to make her feel threaten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dirty southerly wankers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   admiration  amusement  anger  annoyance  approval  caring  confusion  \\\n",
       "0           0          0      0          0         0       0          0   \n",
       "1           0          0      0          0         0       0          0   \n",
       "2           0          0      1          0         0       0          0   \n",
       "3           0          0      0          0         0       0          0   \n",
       "4           0          0      0          1         0       0          0   \n",
       "\n",
       "   curiosity  desire  disappointment  ...  nervousness  optimism  pride  \\\n",
       "0          0       0               0  ...            0         0      0   \n",
       "1          0       0               0  ...            0         0      0   \n",
       "2          0       0               0  ...            0         0      0   \n",
       "3          0       0               0  ...            0         0      0   \n",
       "4          0       0               0  ...            0         0      0   \n",
       "\n",
       "   realization  relief  remorse  sadness  surprise  neutral  \\\n",
       "0            0       0        0        0         0        1   \n",
       "1            0       0        0        0         0        1   \n",
       "2            0       0        0        0         0        0   \n",
       "3            0       0        0        0         0        0   \n",
       "4            0       0        0        0         0        0   \n",
       "\n",
       "                                          clean_text  \n",
       "0  my best loved food is anything i did not have ...  \n",
       "1  now if helium does off himself everyone will g...  \n",
       "2            why the piece of tail be bayless isoing  \n",
       "3                          to make her feel threaten  \n",
       "4                            dirty southerly wankers  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fb14f6",
   "metadata": {},
   "source": [
    "## 2. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46926bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections of config\n",
    "\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 200\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 0.2\n",
    "MODEL_NAME = 'distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ac2a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd0c19f",
   "metadata": {},
   "source": [
    "#### Checking the max token length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d47c9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = df_train['clean_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a834a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  149.0\n"
     ]
    }
   ],
   "source": [
    "max_len = np.zeros(len(text_data))\n",
    "for i in range(len(text_data)):\n",
    "    input_ids = tokenizer.encode(text_data[i], add_special_tokens=True)\n",
    "    max_len[i] = len(input_ids)\n",
    "print('Max length: ', max_len.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aaa441b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbab4d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7d3d45",
   "metadata": {},
   "source": [
    "#### Storing all 28 labels into variable target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4af580be",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [col for col in df_train.columns if col not in ['clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43261378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b7c54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7194553b",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42c58afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb3d783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the training set\n",
    "train_encodings = tokenizer(list(df_train['clean_text']), padding=True, truncation=True, return_tensors='pt')\n",
    "train_dataset = Dataset.from_dict({'input_ids': train_encodings['input_ids'],\n",
    "                                   'attention_mask': train_encodings['attention_mask'],\n",
    "                                   'labels': torch.tensor(df_train[target_cols].values, dtype=torch.float32)})\n",
    "\n",
    "# Tokenize the validation set\n",
    "val_encodings = tokenizer(list(df_val['clean_text']), padding=True, truncation=True, return_tensors='pt')\n",
    "valid_dataset = Dataset.from_dict({'input_ids': val_encodings['input_ids'],\n",
    "                                 'attention_mask': val_encodings['attention_mask'],\n",
    "                                 'labels': torch.tensor(df_val[target_cols].values, dtype=torch.float32)})\n",
    "\n",
    "\n",
    "test_encodings = tokenizer(list(df_test['clean_text']), padding=True, truncation=True, return_tensors='pt')\n",
    "test_dataset = Dataset.from_dict({'input_ids': test_encodings['input_ids'],\n",
    "                                 'attention_mask': test_encodings['attention_mask'],\n",
    "                                 'labels': torch.tensor(df_test[target_cols].values, dtype=torch.float32)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda6801f",
   "metadata": {},
   "source": [
    "### Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "02efe9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Model\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(target_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e91901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom callback to get train and validation info during training\n",
    "class CustomCallback(TrainerCallback):\n",
    "    \n",
    "    def __init__(self, trainer) -> None:\n",
    "        super().__init__()\n",
    "        self._trainer = trainer\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if control.should_evaluate:\n",
    "            control_copy = copy.deepcopy(control)\n",
    "            self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\")\n",
    "            return control_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1980801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='data/output/dis/copy/',\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=VALID_BATCH_SIZE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    logging_dir=\"data/output/dis/copy/logs\",\n",
    "    learning_rate=float(LEARNING_RATE),\n",
    "    weight_decay=0.2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0eae176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom function to calculate the metrics for multi label classification\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'recall' : recall_micro,\n",
    "               'precision': precision_micro,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0051d553",
   "metadata": {},
   "source": [
    "#### Compute class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5cdb39da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "num_classes = len(target_cols)\n",
    "class_counts = np.sum(train_dataset['labels'], axis=0)\n",
    "class_weights = 1.0 / class_counts\n",
    "\n",
    "# Normalize weights\n",
    "class_weights /= class_weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "82eeccf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00434095, 0.00770108, 0.01144105, 0.00725835, 0.00610007,\n",
       "       0.01649321, 0.01310535, 0.00818262, 0.02796899, 0.0141389 ,\n",
       "       0.00886653, 0.02260797, 0.05936463, 0.02101773, 0.03013129,\n",
       "       0.00673483, 0.23283272, 0.01234719, 0.0085945 , 0.10998846,\n",
       "       0.01133973, 0.16151459, 0.01615146, 0.11717725, 0.03289563,\n",
       "       0.01353066, 0.01691332, 0.00126095])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e6f99c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom trainer to incorporate class weights\n",
    "class MultiLabelTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights: Optional[FloatTensor] = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            class_weights = class_weights.to(self.args.device)\n",
    "            logging.info(f\"Using multi-label classification with class weights\", class_weights)\n",
    "        self.loss_fct = BCEWithLogitsLoss(weight=class_weights)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        try:\n",
    "            loss = self.loss_fct(outputs.logits.view(-1, model.num_labels), labels.view(-1,model.num_labels))\n",
    "        except AttributeError:  # DataParallel\n",
    "            loss = self.loss_fct(outputs.logits.view(-1, model.module.num_labels), labels.view(-1, model.num_labels))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b3287773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# create Trainer instance\n",
    "trainer = MultiLabelTrainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.add_callback(CustomCallback(trainer)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e1c6498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/aishah/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13565' max='13565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13565/13565 48:14, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.102400</td>\n",
       "      <td>0.089855</td>\n",
       "      <td>0.505126</td>\n",
       "      <td>0.379455</td>\n",
       "      <td>0.755259</td>\n",
       "      <td>0.687029</td>\n",
       "      <td>0.363297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.102400</td>\n",
       "      <td>0.092409</td>\n",
       "      <td>0.485121</td>\n",
       "      <td>0.364107</td>\n",
       "      <td>0.726619</td>\n",
       "      <td>0.679051</td>\n",
       "      <td>0.350903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.068713</td>\n",
       "      <td>0.655796</td>\n",
       "      <td>0.547840</td>\n",
       "      <td>0.816742</td>\n",
       "      <td>0.771223</td>\n",
       "      <td>0.522507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.088085</td>\n",
       "      <td>0.541905</td>\n",
       "      <td>0.445925</td>\n",
       "      <td>0.690534</td>\n",
       "      <td>0.718582</td>\n",
       "      <td>0.422779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.068600</td>\n",
       "      <td>0.048141</td>\n",
       "      <td>0.782312</td>\n",
       "      <td>0.697301</td>\n",
       "      <td>0.890928</td>\n",
       "      <td>0.846777</td>\n",
       "      <td>0.665615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.068600</td>\n",
       "      <td>0.091064</td>\n",
       "      <td>0.554808</td>\n",
       "      <td>0.478370</td>\n",
       "      <td>0.660320</td>\n",
       "      <td>0.733792</td>\n",
       "      <td>0.440656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>0.033418</td>\n",
       "      <td>0.860963</td>\n",
       "      <td>0.800262</td>\n",
       "      <td>0.931628</td>\n",
       "      <td>0.898842</td>\n",
       "      <td>0.767370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>0.100460</td>\n",
       "      <td>0.555409</td>\n",
       "      <td>0.496082</td>\n",
       "      <td>0.630855</td>\n",
       "      <td>0.741679</td>\n",
       "      <td>0.445264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.027532</td>\n",
       "      <td>0.889934</td>\n",
       "      <td>0.842300</td>\n",
       "      <td>0.943279</td>\n",
       "      <td>0.920039</td>\n",
       "      <td>0.809505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.107910</td>\n",
       "      <td>0.555393</td>\n",
       "      <td>0.506426</td>\n",
       "      <td>0.614843</td>\n",
       "      <td>0.746260</td>\n",
       "      <td>0.443421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13565, training_loss=0.07331778880990039, metrics={'train_runtime': 2895.4486, 'train_samples_per_second': 74.959, 'train_steps_per_second': 4.685, 'total_flos': 8370789484513920.0, 'train_loss': 0.07331778880990039, 'epoch': 5.0})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9dffa6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1658</td>\n",
       "      <td>4.815702e-05</td>\n",
       "      <td>0.18</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1199</td>\n",
       "      <td>4.631404e-05</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1091</td>\n",
       "      <td>4.447107e-05</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1050</td>\n",
       "      <td>4.262809e-05</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1024</td>\n",
       "      <td>4.078511e-05</td>\n",
       "      <td>0.92</td>\n",
       "      <td>2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2713</td>\n",
       "      <td>0.089855</td>\n",
       "      <td>0.505126</td>\n",
       "      <td>0.379455</td>\n",
       "      <td>0.755259</td>\n",
       "      <td>0.687029</td>\n",
       "      <td>0.363297</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092409</td>\n",
       "      <td>0.485121</td>\n",
       "      <td>0.364107</td>\n",
       "      <td>0.726619</td>\n",
       "      <td>0.679051</td>\n",
       "      <td>0.350903</td>\n",
       "      <td>11.4897</td>\n",
       "      <td>472.249</td>\n",
       "      <td>29.592</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0950</td>\n",
       "      <td>3.894213e-05</td>\n",
       "      <td>1.11</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0889</td>\n",
       "      <td>3.709915e-05</td>\n",
       "      <td>1.29</td>\n",
       "      <td>3500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0897</td>\n",
       "      <td>3.525617e-05</td>\n",
       "      <td>1.47</td>\n",
       "      <td>4000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0885</td>\n",
       "      <td>3.341320e-05</td>\n",
       "      <td>1.66</td>\n",
       "      <td>4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0870</td>\n",
       "      <td>3.157022e-05</td>\n",
       "      <td>1.84</td>\n",
       "      <td>5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5426</td>\n",
       "      <td>0.068713</td>\n",
       "      <td>0.655796</td>\n",
       "      <td>0.547840</td>\n",
       "      <td>0.816742</td>\n",
       "      <td>0.771223</td>\n",
       "      <td>0.522507</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088085</td>\n",
       "      <td>0.541905</td>\n",
       "      <td>0.445925</td>\n",
       "      <td>0.690534</td>\n",
       "      <td>0.718582</td>\n",
       "      <td>0.422779</td>\n",
       "      <td>11.0566</td>\n",
       "      <td>490.746</td>\n",
       "      <td>30.751</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0843</td>\n",
       "      <td>2.972724e-05</td>\n",
       "      <td>2.03</td>\n",
       "      <td>5500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0707</td>\n",
       "      <td>2.788426e-05</td>\n",
       "      <td>2.21</td>\n",
       "      <td>6000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0698</td>\n",
       "      <td>2.604128e-05</td>\n",
       "      <td>2.40</td>\n",
       "      <td>6500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0688</td>\n",
       "      <td>2.419830e-05</td>\n",
       "      <td>2.58</td>\n",
       "      <td>7000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0692</td>\n",
       "      <td>2.235533e-05</td>\n",
       "      <td>2.76</td>\n",
       "      <td>7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0686</td>\n",
       "      <td>2.051235e-05</td>\n",
       "      <td>2.95</td>\n",
       "      <td>8000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.048141</td>\n",
       "      <td>0.782312</td>\n",
       "      <td>0.697301</td>\n",
       "      <td>0.890928</td>\n",
       "      <td>0.846777</td>\n",
       "      <td>0.665615</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>8139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091064</td>\n",
       "      <td>0.554808</td>\n",
       "      <td>0.478370</td>\n",
       "      <td>0.660320</td>\n",
       "      <td>0.733792</td>\n",
       "      <td>0.440656</td>\n",
       "      <td>11.4638</td>\n",
       "      <td>473.315</td>\n",
       "      <td>29.659</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0555</td>\n",
       "      <td>1.866937e-05</td>\n",
       "      <td>3.13</td>\n",
       "      <td>8500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0517</td>\n",
       "      <td>1.682639e-05</td>\n",
       "      <td>3.32</td>\n",
       "      <td>9000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0512</td>\n",
       "      <td>1.498341e-05</td>\n",
       "      <td>3.50</td>\n",
       "      <td>9500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0499</td>\n",
       "      <td>1.314043e-05</td>\n",
       "      <td>3.69</td>\n",
       "      <td>10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0515</td>\n",
       "      <td>1.129746e-05</td>\n",
       "      <td>3.87</td>\n",
       "      <td>10500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.00</td>\n",
       "      <td>10852</td>\n",
       "      <td>0.033418</td>\n",
       "      <td>0.860963</td>\n",
       "      <td>0.800262</td>\n",
       "      <td>0.931628</td>\n",
       "      <td>0.898842</td>\n",
       "      <td>0.767370</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.00</td>\n",
       "      <td>10852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100460</td>\n",
       "      <td>0.555409</td>\n",
       "      <td>0.496082</td>\n",
       "      <td>0.630855</td>\n",
       "      <td>0.741679</td>\n",
       "      <td>0.445264</td>\n",
       "      <td>10.9234</td>\n",
       "      <td>496.732</td>\n",
       "      <td>31.126</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0487</td>\n",
       "      <td>9.454478e-06</td>\n",
       "      <td>4.05</td>\n",
       "      <td>11000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0377</td>\n",
       "      <td>7.611500e-06</td>\n",
       "      <td>4.24</td>\n",
       "      <td>11500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0394</td>\n",
       "      <td>5.768522e-06</td>\n",
       "      <td>4.42</td>\n",
       "      <td>12000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0395</td>\n",
       "      <td>3.925544e-06</td>\n",
       "      <td>4.61</td>\n",
       "      <td>12500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0384</td>\n",
       "      <td>2.082565e-06</td>\n",
       "      <td>4.79</td>\n",
       "      <td>13000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0381</td>\n",
       "      <td>2.395872e-07</td>\n",
       "      <td>4.98</td>\n",
       "      <td>13500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.027532</td>\n",
       "      <td>0.889934</td>\n",
       "      <td>0.842300</td>\n",
       "      <td>0.943279</td>\n",
       "      <td>0.920039</td>\n",
       "      <td>0.809505</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107910</td>\n",
       "      <td>0.555393</td>\n",
       "      <td>0.506426</td>\n",
       "      <td>0.614843</td>\n",
       "      <td>0.746260</td>\n",
       "      <td>0.443421</td>\n",
       "      <td>11.2842</td>\n",
       "      <td>480.848</td>\n",
       "      <td>30.131</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.073318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.370789e+15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      loss  learning_rate  epoch   step  train_loss  train_f1  train_recall  \\\n",
       "0   0.1658   4.815702e-05   0.18    500         NaN       NaN           NaN   \n",
       "1   0.1199   4.631404e-05   0.37   1000         NaN       NaN           NaN   \n",
       "2   0.1091   4.447107e-05   0.55   1500         NaN       NaN           NaN   \n",
       "3   0.1050   4.262809e-05   0.74   2000         NaN       NaN           NaN   \n",
       "4   0.1024   4.078511e-05   0.92   2500         NaN       NaN           NaN   \n",
       "5      NaN            NaN   1.00   2713    0.089855  0.505126      0.379455   \n",
       "6      NaN            NaN   1.00   2713         NaN       NaN           NaN   \n",
       "7   0.0950   3.894213e-05   1.11   3000         NaN       NaN           NaN   \n",
       "8   0.0889   3.709915e-05   1.29   3500         NaN       NaN           NaN   \n",
       "9   0.0897   3.525617e-05   1.47   4000         NaN       NaN           NaN   \n",
       "10  0.0885   3.341320e-05   1.66   4500         NaN       NaN           NaN   \n",
       "11  0.0870   3.157022e-05   1.84   5000         NaN       NaN           NaN   \n",
       "12     NaN            NaN   2.00   5426    0.068713  0.655796      0.547840   \n",
       "13     NaN            NaN   2.00   5426         NaN       NaN           NaN   \n",
       "14  0.0843   2.972724e-05   2.03   5500         NaN       NaN           NaN   \n",
       "15  0.0707   2.788426e-05   2.21   6000         NaN       NaN           NaN   \n",
       "16  0.0698   2.604128e-05   2.40   6500         NaN       NaN           NaN   \n",
       "17  0.0688   2.419830e-05   2.58   7000         NaN       NaN           NaN   \n",
       "18  0.0692   2.235533e-05   2.76   7500         NaN       NaN           NaN   \n",
       "19  0.0686   2.051235e-05   2.95   8000         NaN       NaN           NaN   \n",
       "20     NaN            NaN   3.00   8139    0.048141  0.782312      0.697301   \n",
       "21     NaN            NaN   3.00   8139         NaN       NaN           NaN   \n",
       "22  0.0555   1.866937e-05   3.13   8500         NaN       NaN           NaN   \n",
       "23  0.0517   1.682639e-05   3.32   9000         NaN       NaN           NaN   \n",
       "24  0.0512   1.498341e-05   3.50   9500         NaN       NaN           NaN   \n",
       "25  0.0499   1.314043e-05   3.69  10000         NaN       NaN           NaN   \n",
       "26  0.0515   1.129746e-05   3.87  10500         NaN       NaN           NaN   \n",
       "27     NaN            NaN   4.00  10852    0.033418  0.860963      0.800262   \n",
       "28     NaN            NaN   4.00  10852         NaN       NaN           NaN   \n",
       "29  0.0487   9.454478e-06   4.05  11000         NaN       NaN           NaN   \n",
       "30  0.0377   7.611500e-06   4.24  11500         NaN       NaN           NaN   \n",
       "31  0.0394   5.768522e-06   4.42  12000         NaN       NaN           NaN   \n",
       "32  0.0395   3.925544e-06   4.61  12500         NaN       NaN           NaN   \n",
       "33  0.0384   2.082565e-06   4.79  13000         NaN       NaN           NaN   \n",
       "34  0.0381   2.395872e-07   4.98  13500         NaN       NaN           NaN   \n",
       "35     NaN            NaN   5.00  13565    0.027532  0.889934      0.842300   \n",
       "36     NaN            NaN   5.00  13565         NaN       NaN           NaN   \n",
       "37     NaN            NaN   5.00  13565    0.073318       NaN           NaN   \n",
       "\n",
       "    train_precision  train_roc_auc  train_accuracy  ...  eval_loss   eval_f1  \\\n",
       "0               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "1               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "2               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "3               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "4               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "5          0.755259       0.687029        0.363297  ...        NaN       NaN   \n",
       "6               NaN            NaN             NaN  ...   0.092409  0.485121   \n",
       "7               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "8               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "9               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "10              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "11              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "12         0.816742       0.771223        0.522507  ...        NaN       NaN   \n",
       "13              NaN            NaN             NaN  ...   0.088085  0.541905   \n",
       "14              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "15              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "16              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "17              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "18              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "19              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "20         0.890928       0.846777        0.665615  ...        NaN       NaN   \n",
       "21              NaN            NaN             NaN  ...   0.091064  0.554808   \n",
       "22              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "23              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "24              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "25              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "26              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "27         0.931628       0.898842        0.767370  ...        NaN       NaN   \n",
       "28              NaN            NaN             NaN  ...   0.100460  0.555409   \n",
       "29              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "30              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "31              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "32              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "33              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "34              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "35         0.943279       0.920039        0.809505  ...        NaN       NaN   \n",
       "36              NaN            NaN             NaN  ...   0.107910  0.555393   \n",
       "37              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "\n",
       "    eval_recall  eval_precision  eval_roc_auc  eval_accuracy  eval_runtime  \\\n",
       "0           NaN             NaN           NaN            NaN           NaN   \n",
       "1           NaN             NaN           NaN            NaN           NaN   \n",
       "2           NaN             NaN           NaN            NaN           NaN   \n",
       "3           NaN             NaN           NaN            NaN           NaN   \n",
       "4           NaN             NaN           NaN            NaN           NaN   \n",
       "5           NaN             NaN           NaN            NaN           NaN   \n",
       "6      0.364107        0.726619      0.679051       0.350903       11.4897   \n",
       "7           NaN             NaN           NaN            NaN           NaN   \n",
       "8           NaN             NaN           NaN            NaN           NaN   \n",
       "9           NaN             NaN           NaN            NaN           NaN   \n",
       "10          NaN             NaN           NaN            NaN           NaN   \n",
       "11          NaN             NaN           NaN            NaN           NaN   \n",
       "12          NaN             NaN           NaN            NaN           NaN   \n",
       "13     0.445925        0.690534      0.718582       0.422779       11.0566   \n",
       "14          NaN             NaN           NaN            NaN           NaN   \n",
       "15          NaN             NaN           NaN            NaN           NaN   \n",
       "16          NaN             NaN           NaN            NaN           NaN   \n",
       "17          NaN             NaN           NaN            NaN           NaN   \n",
       "18          NaN             NaN           NaN            NaN           NaN   \n",
       "19          NaN             NaN           NaN            NaN           NaN   \n",
       "20          NaN             NaN           NaN            NaN           NaN   \n",
       "21     0.478370        0.660320      0.733792       0.440656       11.4638   \n",
       "22          NaN             NaN           NaN            NaN           NaN   \n",
       "23          NaN             NaN           NaN            NaN           NaN   \n",
       "24          NaN             NaN           NaN            NaN           NaN   \n",
       "25          NaN             NaN           NaN            NaN           NaN   \n",
       "26          NaN             NaN           NaN            NaN           NaN   \n",
       "27          NaN             NaN           NaN            NaN           NaN   \n",
       "28     0.496082        0.630855      0.741679       0.445264       10.9234   \n",
       "29          NaN             NaN           NaN            NaN           NaN   \n",
       "30          NaN             NaN           NaN            NaN           NaN   \n",
       "31          NaN             NaN           NaN            NaN           NaN   \n",
       "32          NaN             NaN           NaN            NaN           NaN   \n",
       "33          NaN             NaN           NaN            NaN           NaN   \n",
       "34          NaN             NaN           NaN            NaN           NaN   \n",
       "35          NaN             NaN           NaN            NaN           NaN   \n",
       "36     0.506426        0.614843      0.746260       0.443421       11.2842   \n",
       "37          NaN             NaN           NaN            NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second    total_flos  \n",
       "0                       NaN                    NaN           NaN  \n",
       "1                       NaN                    NaN           NaN  \n",
       "2                       NaN                    NaN           NaN  \n",
       "3                       NaN                    NaN           NaN  \n",
       "4                       NaN                    NaN           NaN  \n",
       "5                       NaN                    NaN           NaN  \n",
       "6                   472.249                 29.592           NaN  \n",
       "7                       NaN                    NaN           NaN  \n",
       "8                       NaN                    NaN           NaN  \n",
       "9                       NaN                    NaN           NaN  \n",
       "10                      NaN                    NaN           NaN  \n",
       "11                      NaN                    NaN           NaN  \n",
       "12                      NaN                    NaN           NaN  \n",
       "13                  490.746                 30.751           NaN  \n",
       "14                      NaN                    NaN           NaN  \n",
       "15                      NaN                    NaN           NaN  \n",
       "16                      NaN                    NaN           NaN  \n",
       "17                      NaN                    NaN           NaN  \n",
       "18                      NaN                    NaN           NaN  \n",
       "19                      NaN                    NaN           NaN  \n",
       "20                      NaN                    NaN           NaN  \n",
       "21                  473.315                 29.659           NaN  \n",
       "22                      NaN                    NaN           NaN  \n",
       "23                      NaN                    NaN           NaN  \n",
       "24                      NaN                    NaN           NaN  \n",
       "25                      NaN                    NaN           NaN  \n",
       "26                      NaN                    NaN           NaN  \n",
       "27                      NaN                    NaN           NaN  \n",
       "28                  496.732                 31.126           NaN  \n",
       "29                      NaN                    NaN           NaN  \n",
       "30                      NaN                    NaN           NaN  \n",
       "31                      NaN                    NaN           NaN  \n",
       "32                      NaN                    NaN           NaN  \n",
       "33                      NaN                    NaN           NaN  \n",
       "34                      NaN                    NaN           NaN  \n",
       "35                      NaN                    NaN           NaN  \n",
       "36                  480.848                 30.131           NaN  \n",
       "37                      NaN                    NaN  8.370789e+15  \n",
       "\n",
       "[38 rows x 23 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view train and validation metrics from training\n",
    "log_history = pd.DataFrame(trainer.state.log_history)\n",
    "log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9edd140f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2713</td>\n",
       "      <td>0.089855</td>\n",
       "      <td>0.505126</td>\n",
       "      <td>0.379455</td>\n",
       "      <td>0.755259</td>\n",
       "      <td>0.687029</td>\n",
       "      <td>0.363297</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5426</td>\n",
       "      <td>0.068713</td>\n",
       "      <td>0.655796</td>\n",
       "      <td>0.547840</td>\n",
       "      <td>0.816742</td>\n",
       "      <td>0.771223</td>\n",
       "      <td>0.522507</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.048141</td>\n",
       "      <td>0.782312</td>\n",
       "      <td>0.697301</td>\n",
       "      <td>0.890928</td>\n",
       "      <td>0.846777</td>\n",
       "      <td>0.665615</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10852</td>\n",
       "      <td>0.033418</td>\n",
       "      <td>0.860963</td>\n",
       "      <td>0.800262</td>\n",
       "      <td>0.931628</td>\n",
       "      <td>0.898842</td>\n",
       "      <td>0.767370</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.027532</td>\n",
       "      <td>0.889934</td>\n",
       "      <td>0.842300</td>\n",
       "      <td>0.943279</td>\n",
       "      <td>0.920039</td>\n",
       "      <td>0.809505</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    loss  learning_rate  epoch   step  train_loss  train_f1  train_recall  \\\n",
       "5    NaN            NaN    1.0   2713    0.089855  0.505126      0.379455   \n",
       "12   NaN            NaN    2.0   5426    0.068713  0.655796      0.547840   \n",
       "20   NaN            NaN    3.0   8139    0.048141  0.782312      0.697301   \n",
       "27   NaN            NaN    4.0  10852    0.033418  0.860963      0.800262   \n",
       "35   NaN            NaN    5.0  13565    0.027532  0.889934      0.842300   \n",
       "\n",
       "    train_precision  train_roc_auc  train_accuracy  ...  eval_loss  eval_f1  \\\n",
       "5          0.755259       0.687029        0.363297  ...        NaN      NaN   \n",
       "12         0.816742       0.771223        0.522507  ...        NaN      NaN   \n",
       "20         0.890928       0.846777        0.665615  ...        NaN      NaN   \n",
       "27         0.931628       0.898842        0.767370  ...        NaN      NaN   \n",
       "35         0.943279       0.920039        0.809505  ...        NaN      NaN   \n",
       "\n",
       "    eval_recall  eval_precision  eval_roc_auc  eval_accuracy  eval_runtime  \\\n",
       "5           NaN             NaN           NaN            NaN           NaN   \n",
       "12          NaN             NaN           NaN            NaN           NaN   \n",
       "20          NaN             NaN           NaN            NaN           NaN   \n",
       "27          NaN             NaN           NaN            NaN           NaN   \n",
       "35          NaN             NaN           NaN            NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  total_flos  \n",
       "5                       NaN                    NaN         NaN  \n",
       "12                      NaN                    NaN         NaN  \n",
       "20                      NaN                    NaN         NaN  \n",
       "27                      NaN                    NaN         NaN  \n",
       "35                      NaN                    NaN         NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store train metrics in dataframe\n",
    "train_history = log_history[log_history['train_f1'].notna()]\n",
    "train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d7c76fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092409</td>\n",
       "      <td>0.485121</td>\n",
       "      <td>0.364107</td>\n",
       "      <td>0.726619</td>\n",
       "      <td>0.679051</td>\n",
       "      <td>0.350903</td>\n",
       "      <td>11.4897</td>\n",
       "      <td>472.249</td>\n",
       "      <td>29.592</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088085</td>\n",
       "      <td>0.541905</td>\n",
       "      <td>0.445925</td>\n",
       "      <td>0.690534</td>\n",
       "      <td>0.718582</td>\n",
       "      <td>0.422779</td>\n",
       "      <td>11.0566</td>\n",
       "      <td>490.746</td>\n",
       "      <td>30.751</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091064</td>\n",
       "      <td>0.554808</td>\n",
       "      <td>0.478370</td>\n",
       "      <td>0.660320</td>\n",
       "      <td>0.733792</td>\n",
       "      <td>0.440656</td>\n",
       "      <td>11.4638</td>\n",
       "      <td>473.315</td>\n",
       "      <td>29.659</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100460</td>\n",
       "      <td>0.555409</td>\n",
       "      <td>0.496082</td>\n",
       "      <td>0.630855</td>\n",
       "      <td>0.741679</td>\n",
       "      <td>0.445264</td>\n",
       "      <td>10.9234</td>\n",
       "      <td>496.732</td>\n",
       "      <td>31.126</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107910</td>\n",
       "      <td>0.555393</td>\n",
       "      <td>0.506426</td>\n",
       "      <td>0.614843</td>\n",
       "      <td>0.746260</td>\n",
       "      <td>0.443421</td>\n",
       "      <td>11.2842</td>\n",
       "      <td>480.848</td>\n",
       "      <td>30.131</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    loss  learning_rate  epoch   step  train_loss  train_f1  train_recall  \\\n",
       "6    NaN            NaN    1.0   2713         NaN       NaN           NaN   \n",
       "13   NaN            NaN    2.0   5426         NaN       NaN           NaN   \n",
       "21   NaN            NaN    3.0   8139         NaN       NaN           NaN   \n",
       "28   NaN            NaN    4.0  10852         NaN       NaN           NaN   \n",
       "36   NaN            NaN    5.0  13565         NaN       NaN           NaN   \n",
       "\n",
       "    train_precision  train_roc_auc  train_accuracy  ...  eval_loss   eval_f1  \\\n",
       "6               NaN            NaN             NaN  ...   0.092409  0.485121   \n",
       "13              NaN            NaN             NaN  ...   0.088085  0.541905   \n",
       "21              NaN            NaN             NaN  ...   0.091064  0.554808   \n",
       "28              NaN            NaN             NaN  ...   0.100460  0.555409   \n",
       "36              NaN            NaN             NaN  ...   0.107910  0.555393   \n",
       "\n",
       "    eval_recall  eval_precision  eval_roc_auc  eval_accuracy  eval_runtime  \\\n",
       "6      0.364107        0.726619      0.679051       0.350903       11.4897   \n",
       "13     0.445925        0.690534      0.718582       0.422779       11.0566   \n",
       "21     0.478370        0.660320      0.733792       0.440656       11.4638   \n",
       "28     0.496082        0.630855      0.741679       0.445264       10.9234   \n",
       "36     0.506426        0.614843      0.746260       0.443421       11.2842   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  total_flos  \n",
       "6                   472.249                 29.592         NaN  \n",
       "13                  490.746                 30.751         NaN  \n",
       "21                  473.315                 29.659         NaN  \n",
       "28                  496.732                 31.126         NaN  \n",
       "36                  480.848                 30.131         NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store validation metrics in dataframe\n",
    "val_history = log_history[log_history['eval_f1'].notna()]\n",
    "val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e1febf2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_x</th>\n",
       "      <th>learning_rate_x</th>\n",
       "      <th>epoch_x</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss_x</th>\n",
       "      <th>train_f1_x</th>\n",
       "      <th>train_recall_x</th>\n",
       "      <th>train_precision_x</th>\n",
       "      <th>train_roc_auc_x</th>\n",
       "      <th>train_accuracy_x</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss_y</th>\n",
       "      <th>eval_f1_y</th>\n",
       "      <th>eval_recall_y</th>\n",
       "      <th>eval_precision_y</th>\n",
       "      <th>eval_roc_auc_y</th>\n",
       "      <th>eval_accuracy_y</th>\n",
       "      <th>eval_runtime_y</th>\n",
       "      <th>eval_samples_per_second_y</th>\n",
       "      <th>eval_steps_per_second_y</th>\n",
       "      <th>total_flos_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2713</td>\n",
       "      <td>0.089855</td>\n",
       "      <td>0.505126</td>\n",
       "      <td>0.379455</td>\n",
       "      <td>0.755259</td>\n",
       "      <td>0.687029</td>\n",
       "      <td>0.363297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092409</td>\n",
       "      <td>0.485121</td>\n",
       "      <td>0.364107</td>\n",
       "      <td>0.726619</td>\n",
       "      <td>0.679051</td>\n",
       "      <td>0.350903</td>\n",
       "      <td>11.4897</td>\n",
       "      <td>472.249</td>\n",
       "      <td>29.592</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5426</td>\n",
       "      <td>0.068713</td>\n",
       "      <td>0.655796</td>\n",
       "      <td>0.547840</td>\n",
       "      <td>0.816742</td>\n",
       "      <td>0.771223</td>\n",
       "      <td>0.522507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088085</td>\n",
       "      <td>0.541905</td>\n",
       "      <td>0.445925</td>\n",
       "      <td>0.690534</td>\n",
       "      <td>0.718582</td>\n",
       "      <td>0.422779</td>\n",
       "      <td>11.0566</td>\n",
       "      <td>490.746</td>\n",
       "      <td>30.751</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.048141</td>\n",
       "      <td>0.782312</td>\n",
       "      <td>0.697301</td>\n",
       "      <td>0.890928</td>\n",
       "      <td>0.846777</td>\n",
       "      <td>0.665615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091064</td>\n",
       "      <td>0.554808</td>\n",
       "      <td>0.478370</td>\n",
       "      <td>0.660320</td>\n",
       "      <td>0.733792</td>\n",
       "      <td>0.440656</td>\n",
       "      <td>11.4638</td>\n",
       "      <td>473.315</td>\n",
       "      <td>29.659</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10852</td>\n",
       "      <td>0.033418</td>\n",
       "      <td>0.860963</td>\n",
       "      <td>0.800262</td>\n",
       "      <td>0.931628</td>\n",
       "      <td>0.898842</td>\n",
       "      <td>0.767370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100460</td>\n",
       "      <td>0.555409</td>\n",
       "      <td>0.496082</td>\n",
       "      <td>0.630855</td>\n",
       "      <td>0.741679</td>\n",
       "      <td>0.445264</td>\n",
       "      <td>10.9234</td>\n",
       "      <td>496.732</td>\n",
       "      <td>31.126</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.027532</td>\n",
       "      <td>0.889934</td>\n",
       "      <td>0.842300</td>\n",
       "      <td>0.943279</td>\n",
       "      <td>0.920039</td>\n",
       "      <td>0.809505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107910</td>\n",
       "      <td>0.555393</td>\n",
       "      <td>0.506426</td>\n",
       "      <td>0.614843</td>\n",
       "      <td>0.746260</td>\n",
       "      <td>0.443421</td>\n",
       "      <td>11.2842</td>\n",
       "      <td>480.848</td>\n",
       "      <td>30.131</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loss_x  learning_rate_x  epoch_x   step  train_loss_x  train_f1_x  \\\n",
       "0     NaN              NaN      1.0   2713      0.089855    0.505126   \n",
       "1     NaN              NaN      2.0   5426      0.068713    0.655796   \n",
       "2     NaN              NaN      3.0   8139      0.048141    0.782312   \n",
       "3     NaN              NaN      4.0  10852      0.033418    0.860963   \n",
       "4     NaN              NaN      5.0  13565      0.027532    0.889934   \n",
       "\n",
       "   train_recall_x  train_precision_x  train_roc_auc_x  train_accuracy_x  ...  \\\n",
       "0        0.379455           0.755259         0.687029          0.363297  ...   \n",
       "1        0.547840           0.816742         0.771223          0.522507  ...   \n",
       "2        0.697301           0.890928         0.846777          0.665615  ...   \n",
       "3        0.800262           0.931628         0.898842          0.767370  ...   \n",
       "4        0.842300           0.943279         0.920039          0.809505  ...   \n",
       "\n",
       "   eval_loss_y  eval_f1_y  eval_recall_y  eval_precision_y  eval_roc_auc_y  \\\n",
       "0     0.092409   0.485121       0.364107          0.726619        0.679051   \n",
       "1     0.088085   0.541905       0.445925          0.690534        0.718582   \n",
       "2     0.091064   0.554808       0.478370          0.660320        0.733792   \n",
       "3     0.100460   0.555409       0.496082          0.630855        0.741679   \n",
       "4     0.107910   0.555393       0.506426          0.614843        0.746260   \n",
       "\n",
       "   eval_accuracy_y  eval_runtime_y  eval_samples_per_second_y  \\\n",
       "0         0.350903         11.4897                    472.249   \n",
       "1         0.422779         11.0566                    490.746   \n",
       "2         0.440656         11.4638                    473.315   \n",
       "3         0.445264         10.9234                    496.732   \n",
       "4         0.443421         11.2842                    480.848   \n",
       "\n",
       "   eval_steps_per_second_y  total_flos_y  \n",
       "0                   29.592           NaN  \n",
       "1                   30.751           NaN  \n",
       "2                   29.659           NaN  \n",
       "3                   31.126           NaN  \n",
       "4                   30.131           NaN  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = pd.merge(train_history, val_history, on='step', how='outer')\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9ebe0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting function to check for accuracy with graphs \n",
    "def plot_model_performance(history):\n",
    "\n",
    "    #getting train and validation accuracy\n",
    "    acc = history['train_accuracy_x']\n",
    "    val_acc = history['eval_accuracy_y']\n",
    "\n",
    "    #getting train and validation loss\n",
    "    loss = history['train_loss_x']\n",
    "    val_loss = history['eval_loss_y']\n",
    "\n",
    "    epochs_range = range(5)\n",
    "\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c97c350d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABZgAAAJOCAYAAAA6bWJ4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAADCfElEQVR4nOzdd5hU5d3G8e+zhV3K0pt0EKQjZSlij7H3LiKKXSxJTDPdJJrom5hmoqBiF8Uea8QWS0SpgrD0JnXpHZZt5/1jBlkREXDhbPl+rmsuds6cmblnUDhz88zvhCiKkCRJkiRJkiRpb6XEHUCSJEmSJEmSVD5ZMEuSJEmSJEmS9okFsyRJkiRJkiRpn1gwS5IkSZIkSZL2iQWzJEmSJEmSJGmfWDBLkiRJkiRJkvaJBbOkPRZC+E8I4bLS3jdOIYQFIYTv7ofHfS+EcFXy54EhhDf3ZN99eJ4WIYRNIYTUfc0qSZIk7S0/G+zV4/rZQFKFZsEsVXDJA4ztl+IQwtYS1wfuzWNFUXRyFEWPlva+ZVEI4echhA92sb1+CCE/hNBlTx8riqIRURSdUEq5vnTQG0XRwiiKakRRVFQaj7+L5wshhHkhhGn74/ElSZJ04PjZYN/42QBCCFEIoW1pP66kisGCWargkgcYNaIoqgEsBE4vsW3E9v1CCGnxpSyTHgf6hxBa77T9ImBKFEVTY8gUh6OAhkCbEELvA/nE/jcpSZJUuvxssM/8bCBJu2HBLFVSIYRjQgiLQwi3hBBygYdDCHVCCK+GEFaGENYmf25W4j4lv9o1OITwvxDCXcl954cQTt7HfVuHED4IIWwMIbwdQrgnhPDE1+Tek4y3hRA+Sj7emyGE+iVuHxRC+DyEsDqE8Muve3+iKFoMvAsM2ummS4FHvynHTpkHhxD+V+L68SGEGSGE9SGEfwGhxG0HhxDeTeZbFUIYEUKonbztcaAF8EpylclPQwitkqsJ0pL7NAkhvBxCWBNCmBNCuLrEY/82hPBMCOGx5HuTE0LI/rr3IOky4CXg9eTPJV9X5xDCW8nnWh5C+EVye2oI4RchhLnJ55kQQmi+c9bkvjv/d/JRCOFvIYQ1wG93934k79M8hPBC8vdhdQjhXyGEjGSmriX2axgSK3QafMPrlSRJqnT8bOBngz38bLCr11Mr+Rgrk+/lr0IIKcnb2oYQ3k++tlUhhKeT20PymH9F8rbPwl6sApdU9lgwS5VbY6Au0BK4hsSfCQ8nr7cAtgL/2s39+wIzgfrAn4AHQwhhH/Z9EhgL1AN+y1cP3Erak4wXA5eTWHlbBfgxQAihEzA0+fhNks+3ywO/pEdLZgkhtAe6A0/tYY6vSB7QPg/8isR7MRc4vOQuwB3JfB2B5iTeE6IoGsSXV5r8aRdP8RSwOHn/84A/hhCOK3H7GcBIoDbw8u4yhxCqJR9jRPJyUQihSvK2LOBt4I3kc7UF3kne9YfAAOAUoCZwBbBld+9LCX2BeSR+7/7Abt6PkJgt9yrwOdAKaAqMjKJoW/I1XlLicQcAb0dRtHIPc0iSJFU2fjbws8E3Zt6FfwK1gDbA0SRK98uTt90GvAnUIfHe/jO5/QQS35Q8JPncFwKr9+G5JZURFsxS5VYM3BpF0bYoirZGUbQ6iqLnoyjaEkXRRhIF39G7uf/nURQ9kJzx9ShwENBob/YNIbQAegO/iaIoP4qi/5E4uNmlPcz4cBRFs6Io2go8Q+LADxIHVa9GUfRBsoT8dfI9+DovJjP2T16/FPhPFEUr9+G92u4UYFoURc9FUVQA/B3ILfH65kRR9Fby92Ql8Nc9fFxCCM2BI4BboijKi6JoEjCcLx+U/y+KoteTvw+PA4fu5iHPAbaROCh8FUgDTk3edhqQG0XRX5LPtTGKojHJ264CfhVF0cwoYXIURXt6wLg0iqJ/RlFUmPxvcnfvRx8SB8s/iaJoczLH9tUgjwIXb189kXwPHt/DDJIkSZWRnw38bLC7zwa7eo5UEuXwz5OfBxYAfynxHAUkSvcmOx2rFwBZQAcgRFE0PYqiZXvz3JLKFgtmqXJbGUVR3vYrIYRqIYT7kl9t2gB8ANQOX38W4pIHP9tXqNbYy32bAGtKbANY9HWB9zBjbomft5TI1KTkY0dRtJnd/Et5MtOzwKXJFRUDSRwA78t7td3OGaKS10NilMPIEMKS5OM+QWI1w57Y/l5uLLHtcxIre7fb+b3JDF8/Y+8y4Jlk2bsNeIEdYzKak1hhsSu7u+2bfOn3/hvej+YkPpwU7vwgybJ7M3B0CKEDiRXWX/vhRJIkSX42wM8Gu/tssCv1SawK//xrnuOnJFZhj02O4LgCIIqid0mslr4HWB5CuD+EUHMvnldSGWPBLFVu0U7XfwS0B/pGUVSTxNeWoMQcsP1gGVA3OY5hu+a72f/bZFxW8rGTz1nvG+7zKHABcDyJf2V/9Vvm2DlD4Muv9w4Svy/dko97yU6PufPvWUlLSbyXWSW2tQCWfEOmrwiJmXHfAS4JIeSGxCy+84BTkl/lWwQc/DV3/7rbNid/Lfl73XinfXZ+fbt7PxYBLXZzEPxocv9BwHMlPzBJkiTpK/xs4GeDvbWKHauUv/IcURTlRlF0dRRFTYBrgXtDCG2Tt90dRVEvoDOJURk/KcVckg4wC2ZJJWWRmBe2LoRQF7h1fz9hFEWfA+NJnNCtSgjhMOD0/ZTxOeC0EMIRyVnCv+eb/xz8EFgH3E9ivm/+t8zxGtA5hHBOshj9Hl8uWbOATcnHbcpXD7SWk5hv9hVRFC0CRgN3hBAyQwjdgCtJzE/eW4OAWSQOlLsnL4eQmOE2gMTBdOMQwg9C4qR6WSGEvsn7DgduCyG0CwndQgj1kl/rW0KitE5NrmD4upJ6u929H2NJHJTfGUKonnzNJWfWPQ6cTeJA/LF9eA8kSZIqMz8bfFVl/WywXZXkY2WGEDKT254B/pD8PNCSxPlYngAIIZwfdpzscC2JQrwohNA7hNA3hJBOYhFKHlD0LXJJipkFs6SS/g5UJfEv0Z+QOIHbgTAQOIzEV9JuB54mMft3V/7OPmaMoigHuIHEiUOWkTjIWfwN94lIlJMt+XJJuU85oihaBZwP3Eni9bYDPiqxy++AnsB6EgecL+z0EHcAvwohrAsh/HgXTzGAxAnvlpKYE3drFEVv7Um2nVwG3JtcdfDFBRgGXJb8qt3xJA74c4HZwLHJ+/6VxIHmm8AG4EES7xXA1SQOjFeTWK0w+htyfO37kZwVdzqJ8RcLSfxeXlji9sXARBIHsh/u/VsgSZJUqf0dPxvsfJ/K+tlguxwSRfr2y+XATSRK4nnA/0i8nw8l9+8NjAkhbCIxru77URTNJ3Ei8AdIvOefk3jtd32LXJJiFhJ/PkpS2RFCeBqYEUXRfl8loYothPAQiRMH/iruLJIkSdp7fjaQpLLPFcySYpf8itTBIYSUEMJJwJnAv2OOpXIuhNAKOIfECmpJkiSVA342kKTyZ2/ODipJ+0tjEl/3qkfia2lDoij6NN5IKs9CCLcBNwN3JL+GJ0mSpPLBzwaSVM44IkOSJEmSJEmStE8ckSFJkiRJkiRJ2iexjcioX79+1KpVq7ieXpIkSfvJhAkTVkVR1CDuHDrwPMaXJEmqmHZ3jB9bwdyqVSvGjx8f19NLkiRpPwkhfB53BsXDY3xJkqSKaXfH+I7IkCRJkiRJkiTtEwtmSZIkSZIkSdI+sWCWJEmSJEmSJO2T2GYwS5IkSZIkSaq4CgoKWLx4MXl5eXFH0R7KzMykWbNmpKen7/F9LJglSZIkSZIklbrFixeTlZVFq1atCCHEHUffIIoiVq9ezeLFi2nduvUe388RGZIkSZIkSZJKXV5eHvXq1bNcLidCCNSrV2+vV5xbMEuSJEmSJEnaLyyXy5d9+f2yYJYkSZIqqBDCSSGEmSGEOSGEn+3i9g4hhI9DCNtCCD/e6baHQggrQghTD1xiSZIklTcWzJIkSVIFFEJIBe4BTgY6AQNCCJ122m0N8D3grl08xCPASfszoyRJ0v6yevVqunfvTvfu3WncuDFNmzb94np+fv5u7zt+/Hi+973vfeNz9O/fv1Syvvfee5x22mml8lhx8CR/kiRJUsXUB5gTRdE8gBDCSOBMYNr2HaIoWgGsCCGcuvOdoyj6IITQ6gBllSRJKlX16tVj0qRJAPz2t7+lRo0a/PjHO76wVVhYSFrarqvR7OxssrOzv/E5Ro8eXSpZyztXMEuSJEkVU1NgUYnri5PbSlUI4ZoQwvgQwviVK1eW9sNLkiSVmsGDB/PDH/6QY489lltuuYWxY8fSv39/evToQf/+/Zk5cybw5RXFv/3tb7niiis45phjaNOmDXffffcXj1ejRo0v9j/mmGM477zz6NChAwMHDiSKIgBef/11OnTowBFHHMH3vve9vVqp/NRTT9G1a1e6dOnCLbfcAkBRURGDBw+mS5cudO3alb/97W8A3H333XTq1Ilu3bpx0UUXffs3ay+4glmSJEmqmHZ1hpaotJ8kiqL7gfsBsrOzS/3xJUlSxfC7V3KYtnRDqT5mpyY1ufX0znt1n1mzZvH222+TmprKhg0b+OCDD0hLS+Ptt9/mF7/4Bc8///xX7jNjxgz++9//snHjRtq3b8+QIUNIT0//0j6ffvopOTk5NGnShMMPP5yPPvqI7Oxsrr32Wj744ANat27NgAED9jjn0qVLueWWW5gwYQJ16tThhBNO4N///jfNmzdnyZIlTJ2aOE3GunXrALjzzjuZP38+GRkZX2w7UFzBLEmSJFVMi4HmJa43A5bGlEWSJKlMOP/880lNTQVg/fr1nH/++XTp0oWbb76ZnJycXd7n1FNPJSMjg/r169OwYUOWL1/+lX369OlDs2bNSElJoXv37ixYsIAZM2bQpk0bWrduDbBXBfO4ceM45phjaNCgAWlpaQwcOJAPPviANm3aMG/ePG666SbeeOMNatasCUC3bt0YOHAgTzzxxNeO/thfXMEsSZIkVUzjgHYhhNbAEuAi4OJ4I0mSpMpqb1ca7y/Vq1f/4udf//rXHHvssbz44ossWLCAY445Zpf3ycjI+OLn1NRUCgsL92if7WMy9sXX3bdOnTpMnjyZUaNGcc899/DMM8/w0EMP8dprr/HBBx/w8ssvc9ttt5GTk3PAimZXMEuSJEkVUBRFhcCNwChgOvBMFEU5IYTrQgjXAYQQGocQFgM/BH4VQlgcQqiZvO0p4GOgfXL7lfG8EkmSpP1j/fr1NG2aOEXFI488UuqP36FDB+bNm8eCBQsAePrpp/f4vn379uX9999n1apVFBUV8dRTT3H00UezatUqiouLOffcc7ntttuYOHEixcXFLFq0iGOPPZY//elPrFu3jk2bNpX66/k6rmCWJEmSKqgoil4HXt9p27ASP+eSGJ2xq/vu+Xc4JUmSyqGf/vSnXHbZZfz1r3/lO9/5Tqk/ftWqVbn33ns56aSTqF+/Pn369Pnafd955x2aNdtxWPbss89yxx13cOyxxxJFEaeccgpnnnkmkydP5vLLL6e4uBiAO+64g6KiIi655BLWr19PFEXcfPPN1K5du9Rfz9cJ32ap9reRnZ0djR8/PpbnliRJ0v4TQpgQRVF23Dl04HmML0mSSpo+fTodO3aMO0asNm3aRI0aNYiiiBtuuIF27dpx8803xx1rt3b1+7a7Y3xHZEiSJEmSJEnSfvDAAw/QvXt3OnfuzPr167n22mvjjlTqHJEhSZIkSZIkSfvBzTffXOZXLH9brmCWJEmSJEmSJO0TC2ZJkiRJkiRJ0j6xYJYkSarA4jqhsyRJkqT9pIwd41swS5IkVTAFRcV8NGcVt740lcPvfJdFa7bEHUmSJEnStxEVQ94GWLcQludAcWHcib5gwSxJklQBbM0v4o2pufzwmUlk3/42A4eP4enxi+jStBZbC4rijidJkiQdcMcccwyjRo360ra///3vXH/99bu9z/jx4wE45ZRTWLdu3Vf2+e1vf8tdd9212+f+97//zbRp0764/pvf/Ia33357L9IDxUWwdS2sWQC5U2HNXN57501Ou+x7idvKiLS4A0iSJGnfrNuSz9vTVzAqJ5cPZ68kr6CYWlXTOa5jQ07s3Jij2jWgapXUuGNKkiRJsRgwYAAjR47kxBNP/GLbyJEj+fOf/7xH93/99df3+bn//e9/c9ppp9GpUycAfv/73+/ZHYsKYdt62LoOtm0EIkhJg6q1ILM21FkL6dUgLWOfs5U2VzBLkiSVI0vXbeWRj+Zz8QOf0Ov2t/nxs5OZumQ9F2Y358mr+jL+V9/lrxd058TOjS2XJUmSVKmdd955vPrqq2zbtg2ABQsWsHTpUo444giGDBlCdnY2nTt35tZbb93l/Vu1asWqVasA+MMf/kD79u357ne/y8yZM7/Y54EHHqB3794ceuihnHvuuWzZsoXRo0fz8ssv85Of/ITu3bszd+5cBg8ezHPPPQfAO++8Q48ePejatStXXHEF2zZvhE0radWiGbf+5Hp69j2CrkecxIwl66BeW2jUBWq3hMxakLLrOvepp56ia9eudOnShVtuuQWAoqIiBg8eTJcuXejatSt/+9vfALj77rvp1KkT3bp146KLLvrW77MrmCVJksqwKIqYs2ITo3JyGZWznClL1gPQtmENrju6DSd2bkzXprUIIcScVJIkSdqN//wMcqeU7mM27gon3/m1N9erV48+ffrwxhtvcOaZZzJy5EguvPBCQgj84Q9/oG7duhQVFXHcccfx2Wef0a1bt10+zoQJExg5ciSffvophYWF9OzZk169egFwzjnncPXVVwPwq1/9igcffJCbbrqJM844g9NOO43zzjvvS4+Vl5fH4MGDeeeN1zmkRQMuvfI6ht71O35w9UAgov5BLZg4YQL3PvAwd933BMOHH/ONb8PSpUu55ZZbmDBhAnXq1OGEE07g3//+N82bN2fJkiVMnToV4ItxH3feeSfz588nIyNjlyNA9pYrmCVJksqY4uKIiQvXcsd/pvOdv7zP8X/7gLvenEVqSuCWkzrwzo+O5u0fHs1PTuxAt2a1LZclSZKkr7F9TAYkxmMMGDAAgGeeeYaePXvSo0cPcnJyvjQveWcffvghZ599NtWqVaNmzZqcccYZX9w2depUjjzySLp27cqIESPIycn5+jCF25g54UNaN2vEIXUKYeMyLrvwLD6YMA0adISUdM65+HKoUo1e2dksWLBgj17juHHjOOaYY2jQoAFpaWkMHDiQDz74gDZt2jBv3jxuuukm3njjDWrWrAlAt27dGDhwIE888QRpad9+/bErmCVJksqA/MJiPpm3mlE5ubw1bTkrNm4jLSVw2MH1uOKI1pzQqRGNambGHVOSJEnaN7tZabw/nXXWWfzwhz9k4sSJbN26lZ49ezJ//nzuuusuxo0bR506dRg8eDB5eXm7fZyvW9QxePBg/v3vf3PooYfyyCOP8N577+24MYpg2ybIW5eYqbxhKdHmVCBAzWaJkRe110BaJqQnjvUzMhKzlVNTUyksLNyj1xhF0S6316lTh8mTJzNq1CjuuecennnmGR566CFee+01PvjgA15++WVuu+02cnJyvlXR7ApmSZKkmGzeVsjrU5bx/ZGf0uv2t7j0obG8+OkSslvV4e8XdmfCr4/n8Sv7MqhfS8tlSZIkaR/UqFGDY445hiuuuOKL1csbNmygevXq1KpVi+XLl/Of//xnt49x1FFH8eKLL7J161Y2btzIK6+88sVtGzdu5KCDDqKgoIARI0YAEeStJ6sKbFwyA1bPhs2rICUVqtenw+GnsWDJcubkroe0Kjz++OMcffTR3+o19u3bl/fff59Vq1ZRVFTEU089xdFHH82qVasoLi7m3HPP5bbbbmPixIkUFxezaNEijj32WP70pz+xbt06Nm3a9K2ef4+q6RDCScA/gFRgeBRFd+50ey3gCaBF8jHviqLo4W+VTJIkqQJavWkb70xfwaicXD6cs4r8wmLqVq/CyV0ac0KnxhzRrj6Z6Z6cT5IkSSotAwYM4JxzzvliVMahhx5Kjx496Ny5M23atOHwww/f7f179uzJhRdeSPfu3WnZsiVHHnnkF7fddttt9O3bl5bNm9K1/cFs3LAW1szjolOP4eqf/oG7H342cXK/jCzIyCKzeg0efvhhzj//fAoLC+nduzfXXXfdXr2ed955h2bNmn1x/dlnn+WOO+7g2GOPJYoiTjnlFM4880wmT57M5ZdfTnFxMQB33HEHRUVFXHLJJaxfv54oirj55pupXbv2Xj3/zsLXLaH+YocQUoFZwPHAYmAcMCCKomkl9vkFUCuKoltCCA2AmUDjKIryv+5xs7Ozo/Hjx3+r8JIkSeXBojVbeHPackbl5DJ+wRqKI2hauyondm7MCZ0bkd2yDmmpFeeLZSGECVEUZcedQweex/iSJKmk6dOn07Fjx7hj7B9FhZC3PjH+YttGIIKUtMTYi8zakFEDQvk8xt/V79vujvH3ZAVzH2BOFEXzkg82EjgTKDn5OgKyQmIYSQ1gDbBnQ0IkSZIqmCiKmJG7kTdzEqXytGUbAOjQOIsbv9OOEzo1onOTmp6cT5IkSSpPCvMThXLeeshPjpVIrQLV6ydK5SrVoRIe4+9JwdwUWFTi+mKg7077/At4GVgKZAEXRlFUvPMDhRCuAa4BaNGixb7klSRJKpOKiiMmLlzLqKm5vDltOQvXbCEE6NWiDr88pSPHd2pEq/rV444pSZIkaW8U5O0olQu2JLalZUKNRolSOb1qpSyVS9qTgnlX79DOczVOBCYB3wEOBt4KIXwYRdGGL90piu4H7ofE1+f2Oq0kSVIZsq2wiNFzVvPmtFzemracVZvyqZKaQv+29RhyzMEc17EhDbM8OZ8kSZIqryiKytc396IoUSRvH39RuC2xPb0aZDVJjMBIr7jH+N80TnlX9qRgXgw0L3G9GYmVyiVdDtwZJRLMCSHMBzoAY/c6kSRJUhm2Ma+A/85cyaicXN6bsYLN+UXUyEjjmPYNOLFzY45p34CszPS4Y0qSJEmxy8zMZPXq1dSrV69sl8xRlBh5kbcetq6D4oLE9io1oFaDRKmcWiXWiAdCFEWsXr2azMy9K9D3pGAeB7QLIbQGlgAXARfvtM9C4DjgwxBCI6A9MG+vkkiSJJVRKzdu461py3lzWi6j56wmv6iY+jWqcEb3JpzQuTH9D65HRlpq3DElSZKkMqVZs2YsXryYlStXxh3lq6IICvOgYGviEhUlRl2kVU2MvUirCimFwKrkpXLIzMykWbNme3WfbyyYoygqDCHcCIwCUoGHoijKCSFcl7x9GHAb8EgIYQqJkRq3RFFUed55SZJU4Xy+ejOjcnIZlbOciQvXEkXQom41LuvfkhM7N6ZHizqkppThVRiSJElSzNLT02ndunXcMXbI2wCz34Tpr8CctxOrljNqwiEnQsfToe13Eyfq017ZkxXMRFH0OvD6TtuGlfh5KXBC6UaTJEk6cKIoImfpBt5Mlsozl28EoHOTmvzguEM4sUsj2jfKKttf7ZMkSZL0ZZtWwszXE6Xy/PehKB+qN4Su5yVK5VZHQVrFH3+xP+1RwSxJklQRFRYVM/7ztYzKyeXNnOUsWbeVlAC9W9Xl16d14oROjWhet1rcMSVJkiTtjXULYfqriVJ50ScQFUPtltDnmkSp3Kw3pDjirrRYMEuSpEolr6CI/81exaicXN6ZsYI1m/OpkpbCUe3q8/3j2nFcx4bUq5ERd0xJkiRJeyqKYOWMRKk84xVYNjmxvWFnOOoniVK5UZfEjGWVOgtmSZJU4a3fWsB/Z6xgVE4u789ayZb8IrIy0ziuQ0NO6NyYow9pQPUMD4skSZKkcqO4GJZOTKxSnvEqrJ6T2N6sDxz/e+hwGtQ7ON6MlYSfpCRJUoW0fEMeb+bk8ua05Xw8dzWFxRENszI4p2dTTujUmH5t6lElLSXumJIkSZL2VFEBfP5RcqXya7BxKaSkQasjod8QaH8q1Dwo7pSVjgWzJEmqMOau3PTFPOVJi9YB0KZ+da46sg0ndG5E92a1SUnxa3GSJElSuVGwFeb+N7FSedZ/YOtaSKsKbY+DjrfCISdC1Tpxp6zULJglSVK5FUURny1enyiVpy1nzopNAHRrVoufnNieEzo1om3DGgRnrUmSJEnlR956mPUmTH8Z5rwDBZshsxYcclJinvLBx0EVT8ZdVlgwS5KkcqWgqJix89cwKieXt6YtZ9n6PFJTAn1b12VQv5Yc36kRTWpXjTumJEmSpL2xaUVi7MWMV2He+1BcADUawaEXJkrlVkdCanrcKbULFsySJKnM25pfxPuzVvJmTi7vzFjB+q0FZKancFS7Bvz4hPZ8p0ND6lSvEndMSZIkSXtj7YLkPOVXYeEnQAR1WkO/66DjGdA0G1I8b0pZZ8EsSZLKpLWb83lnxgpG5eTy4eyV5BUUU6tqOsd1bMiJnRtzVLsGVK2SGndMSZIkSXsqimDFtGSp/ArkTklsb9QVjvkZdDgNGnUGR9yVKxbMkiSpzFi6bitv5uQyKmc5Yxesoag44qBamVyY3ZwTOzemd+u6pKe6gkGSJEkqN4qLYcmExDzlGa/CmnlAgOZ94YTbE6Vy3dZxp9S3YMEsSZJiE0URc1ZsYlSyVJ6yZD0A7RrW4Lqj23Bi58Z0bVrLk/RJkiRJ5UlRASz4H0x/JTFXeVMupKRB66Og/03Q/lTIahR3SpUSC2ZJknRAFRdHTFq8jlE5ubyZs5z5qzYD0KNFbW45qQMndm5EmwY1Yk4pSZIkaa/kb4G57yZK5VlvQN46SK8GbY9LzFNudwJUrR13Su0HFsySJGm/yy8s5pN5qxmVk8tb05azYuM20lIChx1cjyuOaM0JnRrRqGZm3DElSZIk7Y2t62DWqMT4iznvQOFWyKwN7U+GjqdDm2OhSrW4U2o/s2CWJEn7xeZthbw/ayWjcnJ5d8YKNuYVUq1KKse0b8AJnRpzbIeG1KqaHndMSZIkSXtjY25i7MX0V2DBh1BcCFkHQY+BiVK55eGQ6nF+ZWLBLEmSSs3qTdt4Z/oKRuXk8uGcVeQXFlO3ehVO7tKYEzo15oh29clMT407piRJkqS9sWYeTH81cZK+RWOBCOq2gcNuSIy/aNITUjwZd2VlwSxJkr6VRWu28Oa05YzKyWX8gjUUR9C0dlUu6duSEzo3IrtlHdJSPdiUJEmSyo0oguVTd5TKy6cmtjfuBsf+AjqcBg07gifjFhbMkiRpL0VRxIzcjbyZkyiVpy3bAECHxlnc+J12nNCpEZ2b1CR4sClJkiSVH8XFsHhsYvTFjFdh7QIgQIvD4MQ/QodToU6rmEOqLLJgliRJ36ioOGLiwrWMmprLm9OWs3DNFkKAXi3q8MtTOnJC50a0rFc97piSJEmS9kZhfmKO8vRXYObrsGk5pKRDm6PhiJuh/SlQo2HcKVXGWTBLkqRd2lZYxOg5qxmVk8vb05ezalM+VVJT6N+2HkOOOZjjOjakYVZm3DElSZIk7Y38zTDnnUSpPGsUbFsP6dWh3XcT85TbHQ+ZteJOqXLEglmSJH1hY14B/525klE5ubw3YwWb84uokZHGMe0bcGLnxhzTvgFZmZ4RWpIkSSpXtqxJlMnTX4G570BhHlStAx1Pg46nQ5tjIL1q3ClVTlkwS5JUya3cuI23kifpGz13FQVFEfVrVOGM7k04oXNj+h9cj4y01LhjSpIkSdobG5YlZinPeBXmfwhREWQ1gZ6XJk7S1/JwSLUa1Lfnf0WSJFVCn6/ezKicXEblLGfiwrVEEbSoW43B/VtxYufG9GhRh9QUT9InSZIklSur5+44Sd/icYlt9drC4d+DDqdDkx6QkhJvRlU4FsySJFUCURSRs3QDbyZL5ZnLNwLQuUlNfnDcIZzYpRHtG2URgqWyJEmSVG5EEeRO2VEqr5iW2H5Qd/jOrxKlcoP24HG+9iMLZkmSKqjComLGf76WUTm5vJmznCXrtpISoHeruvz6tE6c0KkRzetWizumJEmSpL1RXASLxsD0V2HGK7BuIYQUaHEYnHQndDgVareIO6UqEQtmSZIqkLyCIv43exWjcnJ5e/py1m4poEpaCke1q8/3j2vHcR0bUq9GRtwxJUmSJO2NwnyY/wFMfxlmvg6bV0JqlcTJ+Y76CbQ/BarXjzulKikLZkmSyrn1Wwv474wVjMrJ5f1ZK9mSX0RWZhrHdWjICZ0bc/QhDaie4V/5kiRJUrmybRPMeTsx/mL2m7BtA1SpAe2Oh46nQ9vjIbNm3CklC2ZJksqj5RvyeDMnlzenLefjuaspLI5omJXBOT2bckKnxvRrU48qaZ68Q5IkSSpXtqyBmf9JlMpz34WibVCtHnQ6AzqeAa2PhvTMuFNKX2LBLElSOTF35aYv5ilPWrQOgDb1q3PVkW04oXMjujerTUqKJ++QJEmSypX1S2DGa4nxF5+PhqgIajaD7Muhw2mJ2cqpVngqu/yvU5KkMmzVpm08/NF8RuUsZ86KTQB0a1aLn5zYnhM7N+LgBjUInhFakiRJKl9WzU6sUp7xKiyZkNhWvz0c8YNEqdykB3icr3LCglmSpDJq7Pw13PjkRFZvzqdv67oM6teS4zs1okntqnFHkyRJkrS3tm2Cqc/DhEdg6cTEtiY94bjfQIfTocEhscaT9pUFsyRJZUxxccT9H87jz6Nm0qJuNR65vA+dmnjyDkl7L4RwEvAPIBUYHkXRnTvd3gF4GOgJ/DKKorv29L6SJGkPLfsMJjwMnz0L+RuhQQc48Y/Q6Uyo1SzudNK3ZsEsSVIZsm5LPj9+djJvT1/BKV0b83/ndiMrMz3uWJLKoRBCKnAPcDywGBgXQng5iqJpJXZbA3wPOGsf7itJkr5O/uYdq5WXTIDUDOh8dmKucvO+jr9QhWLBLElSGTF50TquHzGRFRvz+O3pnbisfyvnK0v6NvoAc6IomgcQQhgJnAl8URJHUbQCWBFCOHVv7ytJknYhdwqMfxg+eyaxWrl+ezjpTuh2IVSrG3c6ab+wYJYkKWZRFPH4J59z+6vTaZCVwTPXHkaPFnXijiWp/GsKLCpxfTHQt7TvG0K4BrgGoEWLFnufUpKk8i5/M0x9IblaefyO1cq9BkOLfq5WVoVnwSxJUow2bSvkZ89/xqufLePY9g346wXdqVO9StyxJFUMu/o0G5X2faMouh+4HyA7O3tPH1+SpPIvd2pytvIzsG0D1D8ETrwDDr3I1cqqVCyYJUmKyYzcDVz/xEQWrN7MT09qz3VHHUxKiqsbJJWaxUDzEtebAUsPwH0lSaq48rdATnK18uJxidXKnc5MzFZucZirlVUpWTBLkhSDZ8cv4tcvTSUrM50nr+5Hvzb14o4kqeIZB7QLIbQGlgAXARcfgPtKklTxLM9JlMqTn4Zt65Orlf8Ihw5wtbIqPQtmSZIOoLyCIn7z0lSeGb+Yw9rU4x8DutMwKzPuWJIqoCiKCkMINwKjgFTgoSiKckII1yVvHxZCaAyMB2oCxSGEHwCdoijasKv7xvJCJEmKS/4WyHkxuVp5LKRWSaxW7nU5tOzvamUpyYJZkqQDZN7KTVw/YiIzcjdy47Ftufn4Q0h1JIak/SiKoteB13faNqzEz7kkxl/s0X0lSaoUlk9LzFbevlq5Xjs44Q+J1crV/eahtDMLZkmSDoDXPlvGLc9/Rnpq4OHLe3Ns+4ZxR5IkSZK0XcHWHauVF41JrFbueEZitnLLw12tLO2GBbMkSftRfmExf3x9Oo+MXkCPFrW55+KeNKldNe5YkiRJkgBWTE/OVn4K8tZDvbZwwu1w6MWuVpb2kAWzJEn7yeK1W7jhyU+ZvGgdVxzemp+d3IEqaSlxx5IkSZIqt4KtkPPv5GrlTyAlHTqdkZit3OoIVytLe8mCWZKk/eDdGcu5+enJFBdHDB3Yk5O7HhR3JEmSJKlyWzGjxGrldVD3YDj+Nuh+MVSvH3c6qdyyYJYkqRQVFhXzl7dmMfS9uXQ6qCb3DuxJq/rV444lSZIkVU4FW2HaS4lieeHHidXKHU9PzFZudaSrlaVSYMEsSVIpWbEhj5ue+pQx89cwoE9zbj29M5npqXHHkiRJkiqflTMTpfKkJ5OrldvA8b+H7gNdrSyVMgtmSZJKweg5q/jeyE/ZvK2Iv15wKOf0bBZ3JEmSJKlyKciD6S/D+Idh4ejkauXTkrOVj4QUz4ci7Q8WzJIkfQvFxRH3/HcOf3t7Fq3rV+fJq/txSKOsuGNJkiRJlcfKWcnZyk/C1rVQpzV893eJ1co1GsSdTqrwLJglSdpHazbnc/PTk3h/1krO7N6EP57dleoZ/tUqSZIk7XcFeTD9FZjwMHz+EaSkQYfTkrOVj3K1snQA+SlYkqR9MOHztdz45ERWb8rn9rO6MLBvC4InCJEkSZL2r1Wzd8xW3roG6rSC7/42uVq5YczhpMrJglmSpL0QRREPfbSAO16fzkG1M3l+SH+6NqsVdyxJkiSp4ircllitPP5h+Px/ydXKpyZmK7c+2tXKUswsmCVJ2kMb8gr46bOf8UZOLsd3asRd5x1KrWrpcceSJEmSKqZVcxIjMLavVq7dEo67NbFaOatR3OkkJVkwS5K0B6YuWc8NT05k8dqt/PKUjlx1ZGtHYkiSJEmlbftq5QmPwIIPE6uV25+SmK3c+hhXK0tlkAWzJEm7EUURT41dxG9fyaFutSo8fU0/slvVjTuWJEmSVLGsnrtjtfKW1cnVyr+B7pe4Wlkq4yyYJUn6GlvyC/nVi1N54dMlHNmuPn+/sDv1amTEHUuSJEmqGArzYUZytvKCDyGkQodTErOV2xzramWpnLBgliRpF+as2MiQJyYyZ+Umbv7uIdz4nbakpjgSQ5IkSfrWVs9NjMCY9CRsWQW1W8B3fg09LoGsxnGnk7SXLJglSdrJS5OW8PMXplA1PZXHr+jLEe3qxx1JkiRJKt8K82HGq4kxGPM/SKxWbn9yYrZym++4WlkqxyyYJUlKyiso4rZXpzFizEJ6t6rDPwf0pHGtzLhjSZIkSeXX6rkw8VH4dERitXKtFvCdXyVmK9c8KO50kkqBBbMkScDC1Vu4/skJTF2ygWuPbsNPTmhPWqqrKCRJkqS9VpgPM19LjMGY996O1cq9BsPB34GU1JgDSipNFsySpEpvVE4uP352MgF44NJsju/kWaolSZKkvbZmHkx4FCaNgM0roVZzOPZX0GMg1GwSdzpJ+4kFsySp0iooKuZPb8zggQ/n061ZLe65uCfN61aLO5YkSZJUfhQVwIzXErOV570HIQUOSa5Wbnucq5WlSsCCWZJUKS1bv5Ubn/yUCZ+vZVC/lvzqtI5kpHnwK0mSJO2RNfN3zFbevAJqNoNjfgE9B7laWapkLJglSZXOB7NW8oOnJ7GtoIi7B/TgjEM9AJYkSZK+UVEBzHw9MVt57rvJ1conJVcrf9fVylIlZcEsSao0iooj/vHObP757mwOaZjFvZf05OAGNeKOJUmSJJVtaxckZit/+kRytXJTOObn0GMQ1GoadzpJMbNgliRVCqs2beP7Iz/lozmrObdnM24/qwtVq7jCQpIkSdqlogKY+Z8Sq5UDtDsxsVq53fGuVpb0BQtmSVKFN3b+Gm58ciLrtxbwp3O7cUHv5nFHkiRJksqmtZ8nZys/AZuWQ1YTOPqWxGzlWs3iTiepDLJgliRVWMXFEfd/OI8/j5pJi7rVeOTyPnRqUjPuWJIkSVLZUlQAs95IrFae805itXLb4yH78sSvqdZHkr6ef0JIkiqk9VsK+NGzk3h7+gpO6dqY/zu3G1mZ6XHHkiRJksqOdQth4mMw8XHYlJtcrfzTxGzl2n7rT9KesWCWJFU4kxet44YnJ7J8Qx6/Pb0Tl/VvRQgh7liSJElS/IoKS6xWfjuxrd0J0OtviV9drSxpL/mnhiSpwoiiiMc/+ZzbX51Og6wMnrn2MHq0qBN3LEmSJCl+21crf/oEbFwGWQfBUT+Bnpe6WlnSt2LBLEmqEDZtK+TnL0zhlclLObZ9A/56QXfqVK8SdyxJkiQpPkWFMHtUYrXy7LcS29p+F079C7Q70dXKkkqFf5JIksq9GbkbuP6JiSxYvZmfnNieIUcfTEqKIzEkSZJUSa1bBJ8+npitvHEp1GgMR/04uVq5RdzpJFUwFsySpHLt2fGL+PVLU8nKTOfJq/vRr029uCNJkiRJB15RIcx+Mzlb+S2IImh7HJzyZzjkJFcrS9pv/NNFklQu5RUU8ZuXpvLM+MUc1qYe/xjQnYZZmXHHkiRJkg6s9YsTK5UnPpZcrdwIjvhhYrVynZZxp5NUCVgwS5LKnfmrNjPkiQnMyN3Ijce25ebjDyHVkRiSJEmqLIqLEjOVJzycWLUcRXDwd+CUPyVXK6fHnVBSJWLBLEkqV177bBm3PP8Z6amBhy/vzbHtG8YdSZIkSTow1i9JzlZ+DDYsSa5Wvjm5WrlV3OkkVVIWzJKkciG/sJg/vj6dR0YvoEeL2txzcU+a1K4adyxJkiRp/yougjlvw/iHYfYoiIoTq5VPuhPan+xqZUmxs2CWJJV5i9du4YYnP2XyonVccXhrfnZyB6qkpcQdS5IkSdp/1i+BT59IrlZeDNUbwuE/SKxWrts67nSS9AULZklSmfbujOXc/PRkiosjhg7sycldD4o7kiRJkrR/FBfBnHcSs5VnvZFYrdzmWDjpj9D+FFcrSyqTLJglSWVSYVExf31rFve+N5eOB9Vk6MCetKpfPe5YkqSyZPS/gAgad4VGXaF6vbgTSdK+2bB0x2rl9YugegM4/PvJ1cpt4k4nSbtlwSxJKnNWbMjjpqc+Zcz8NQzo05xbT+9MZnpq3LEkSWXN1Odh6cQd17MOgkZdEoVz4y6J0rnewZDi3yGSyqDiIpj7bmK28qw3ICqCNsfACbcnViunVYk7oSTtEQtmSVKZMnruKr731CQ2byvkrxccyjk9m8UdSZJUVl3zX9i8CnKnJC7Lp0LuVJj3XyguTOyTVhUadkyWzl0TBXSjzpBZM97skiqvDctKrFZemFit3P8m6HWZq5UllUsWzJKkMqG4OOKe/87hb2/PonX96jx5dV8OaZQVdyxJUllXvT4cfGzisl3hNlg5s0TpPAWmvwwTH92xT51WO1Y7N+qSWPFcuyWEcMBfgqRKoLgI5v43MVt55n8Sq5VbHw0n/B7an+pqZUnlmgWzJCl2azbnc/PTk3h/1krO7N6EP57dleoZ/hUlSdpHaRlwULfEZbsoSsw4zZ0Cy6ckVjovnwozXgOixD4ZtRKrmxt32VE6N+wE6VVjeRmSKoCNufDp4zAhuVq5Wn3ofyP0vCwxwkeSKgA/vUuSYjXh87Xc9OREVm3K5/azujCwbwuCq8ckSaUtBKjVNHFpf9KO7fmbYfm0L5fOn46Ags3J+6VAvXYlSuduiZ9rNHK1s6RdKy6GecnZyl+sVj4Kjv8ddDg18Y9gklSBWDBLkmIRRREPfbSAO16fzkG1M3l+SH+6NqsVdyxJUmVTpTo07524bFdcDGvn75jpnDsFFo1NnFRwu2r1v1o61z8EUtMP/GuQVDZsXJ5YrTzxUVi3EKrVg8NugF6DXa0sqUKzYJYkHXAb8gr46bOf8UZOLsd3asRd5x1KrWp+IJcklREpKYkyqN7B0OnMHdu3roXlOcmVzskTC459AIq2JW5PrQIN2icK5+0jNhp1gWp143kdkva/4uLEiUW3z1YuLoRWR8Jxt0LH012tLKlSsGCWJB1QOUvXc/2IiSxeu5VfntKRq45s7UgMSVL5ULUOtDoicdmuqBBWz/5y6Tz7LZg0Ysc+NZt9ea5zo65Qt02iyJZUvkRR4h+b1n0Oc9+FCY8mfq5WD/oNgZ6DoX7buFNK0gFlwSxJOiCiKGLkuEXc+nIOdatV4elr+pHdyhVdkqRyLjUNGnZMXDh/x/ZNKxJlc+6UHaM2Zr+VmMUKkF4dGnX6cuncqDNk1IjlZUgqIW9DYsTFus8Tv679/Ms/52/csW+rI+G437haWVKlZsEsSdrvtuQX8qsXp/LCp0s4sl19/n5hd+rV8ABcklSB1WgIbY9LXLYryIOVM75cOue8kPhqPQAB6rZOls5dE5dGXaBWM08oKJWm/M2wblGJ0njBlwvlrWu/vH96dajTEmq3gJaHJ36t0zLxj0J128TyEiSpLLFgliTtV3NWbGTIExOZs3ITN3/3EG78TltSU/yQLEmqhNIzoUn3xGW7KIL1i5IjNqbuKJ+nv7xjn8zaO1Y6by+dG3RIPJ6kryrcVqJALrkKOVkib1755f1TM3aUxk17Qe2WOwrl2q0Sc9T9Rx5J+loWzJKk/ealSUv4+QtTqJqeyuNX9OWIdvXjjiRJUtkSQrLEagEdTtmxfdtGWD4tOdc5WT5PfAwKtiTvlwr1D0mudO6yY9VzjYbxvA7pQCoqgPWLdzHGInl947Iv75+SnvgmQJ2W0P7kRIFcskSu3tCZ6JL0LVgwS5JKXV5BEbe9Oo0RYxbSu1Ud/jmgJ41rucpKkg60EMJJwD+AVGB4FEV37nR7SN5+CrAFGBxF0cTkbd8HrgYC8EAURX8/gNGVkQUt+iYu2xUXwZr5O0rn3Cnw+Ucw5Zkd+1RvWKJ0Tv5ar11iVrRUXhQXJUrikqVxyRJ5w2KIinfsH1ISJ9Os3QIO/k6yQG6xo0DOOghSUuN7PZJUwXmUIUkqVQtXb+H6JycwdckGrj26DT8+oT3pqa4IkaQDLYSQCtwDHA8sBsaFEF6Oomhaid1OBtolL32BoUDfEEIXEuVyHyAfeCOE8FoURbMP5GvQTlJSoX7bxKXz2Tu2b1mzY6Zz7pREAf3JUCjKT9yempE4CWHJ0rlRF6haO5aXIVFcDJtXlCiQF3y5QF6/GIoLStwhJEri2i2g5WHJVf8lSuSaTSE1Pa5XI0mVngWzJKnUjMrJ5cfPTk4sdbs0m+M7NYo7kiRVZn2AOVEUzQMIIYwEzgRKFsxnAo9FURQBn4QQaocQDgI6Ap9EUbQled/3gbOBPx3IF/BN8guLSU8NhMo+G7VaXWh9VOKyXVEBrJqVLJ0/SxTQM9+AT5/YsU+tFiXGayRHbNRu5agAfXtRBFtWJ1Ye72oV8vpFUJj35ftUb5AojZv0gM5nlSiRW0Lt5pDmCaIlqayyYJYkfWsFRcX86Y0ZPPDhfLo1q8U9F/eked1qcceSpMquKbCoxPXFJFYpf9M+TYGpwB9CCPWArSRGaIzf1ZOEEK4BrgFo0aJFqQTfE4VFxVw/YiJNamdy6+mdPYHszlLToVHnxOXQCxPbogg2LU+sct5+MsHcqTDrjR3jBqrUSN5ve+ncLbH6uUr1+F6Lyqat63YaXbHTGIuCzV/ev2qdRGncsCO0P6lEeZycQV7FY0dJKq8smCVJ38qy9Vu56clPGf/5Wgb1a8mvTutIRpoz7iSpDNhV4xrtyT5RFE0PIfwf8BawCZgMFO7qSaIouh+4HyA7O3vnx99vUkKgTYPq3P/BPFZt2sZfL+hOZrp//+xWCJDVOHFpd/yO7QVbYcW0HScTzJ0KU56F8Q9uvyPUO3jHiQQbd038XLNJ4jFVMW3btOsT6K37HNYuhG3rv7x/lazEuIq6baDNMTvmH28vkTNrxvIyJEn7nwWzJGmffTBrJT94ehLbCoq4e0APzji0SdyRJEk7LAaal7jeDFi6p/tEUfQg8CBACOGPyX3LjJSUwC9O6UjDrAxuf206azaP5f5Ls6mZ6RzWvZZeFZr2Sly2i6JEkfhF6TwFln4K0/69Y5+qdZKlc7cdozYadIC0Kgf8JWgfFGyFdYtKlMYlSuS1n8PWNV/eP63qjtK4eb+vFshV6/gPDpJUSVkwS5L2WlFxxD/emc0/353NIQ2zuPeSnhzcoEbcsSRJXzYOaBdCaA0sAS4CLt5pn5eBG5PzmfsC66MoWgYQQmgYRdGKEEIL4BzgsAMXfc9ddWQbGmRl8ONnJ3PBsI959Io+NKqZGXes8i8EqNMqcel42o7teRtgec6O0jl3Cox/CAq3Jm5PSUuUzNtHbGxf9Vy9fhyvonIrzE/MOt7lKuSFiXEpJaVW2TGu4qDuO06gt32URfX6FsiSpF2yYJYk7ZVVm7bxg5GT+N+cVZzbsxm3n9WFqlX8SrIklTVRFBWGEG4ERgGpwENRFOWEEK5L3j4MeJ3EfOU5wBbg8hIP8XxyBnMBcEMURWsP6AvYC2d2b0rd6lW47vEJnHPvaB67so//8Lm/ZNaEloclLtsVF8HqubA8WTjnToX578NnI3fsk3XQV0vnem0hxWOIfVZUCBuWfPUEetuvb1jKl6bihFSo1SxRGrc7PnFCx5Ilco1GnuBRkrRPQuKE0d+wUwgnAf8gcWA6PIqiO3e6/SfAwOTVNBJnnW4QRdFO36nZITs7Oxo/fpfnCZEklVFj56/hxicnsn5rAbed2YXzs5sRXMkiaSchhAlRFGXHnUMHXtzH+FMWr2fww2MpjiIevrwP3ZvXji2LgM2rv1w6L58KK2dAcXKcd1rVxAnfGneBRttnO3d2Vu92xcWwKXcXJ9BLXtYvgaioxB0C1Gz61dEV269nNYFU15hJkvbN7o7xv7FgDiGkArOA40nMXRsHDIiiaNrX7H86cHMURd/Z3ePGffApSdpzURRx3wfz+POomTSvU5V7B/aiUxM//EnaNQvmyqssHOMvWLWZSx8ay8qN27j3kp4c275hrHm0k8J8WDWzROmc/LXkvN/aLXecSLBx10QBXbtlxRvPEEWweeWXS+OSq5DXL4Ki/C/fp0bjL5fGJUvkms2cfy1J2m92d4y/J/982QeYE0XRvOSDjQTOBHZZMAMDgKf2JagkqexZv6WAHz07ibenr+CUro35v3O7keUJlCRJZVSr+tV5bshhXP7wOK5+dDz/d243zu3VLO5Y2i6tSrI07rpjWxQlxjlsn+u8fGqidJ7xGl+MeMiomVjdXLJ0btgpcYLCsiqKYOvaEifQ23mMxcIds6u3q1YvURof1C0x+3r7/OM6LRPjLcry65UkVVp7UjA3BRaVuL6YxAlAviKEUA04Cbjxa26/BrgGoEWLFnsVVJJ04H22eB3Xj5jI8g153Hp6Jwb3b+VIDElSmdcwK5OR1/Tjuicm8KNnJ7Ny0zauPaqNf4eVVSFAraaJyyEn7tievxlWTC9ROk+ByU/BuAeS90tJzHEuudq5URfIanzgVjvnrd/1CfS2X8/f+OX9M2slCuP67ZJzkEusQq7dAjKcHS5JKn/2pGDe1d/MXzdX43Tgo6+bvRxF0f3A/ZD4+tweJZQkHXBRFPHEJ59z26vTaZCVwTPXHkaPFnXijiVJ0h7LykznocG9+fGzn3Hnf2awYsM2fnVqR1JSLJnLjSrVoVl24rJdcTGsW7BjpnPuFFg0DqY+v2OfavW+Wjo3aA+p+/ANrPzNO1Yblxxlsb1Azlv35f3Tq+84aV6rI3b8vL1Arlp7H94ISZLKtj0pmBcDzUtcbwYs/Zp9L8LxGJJUrm3aVsjPX5jCK5OXcmz7Bvz1gu7Uqe48P0lS+ZORlso/LuxO/RpVeOij+azctI27zu9GRlpq3NG0r1JSoG6bxKXTGTu2b10Hy3OSpfNniQJ67ANQtC1xe2qVRMncKDleY3v5nF4tMeu4ZGlcchXyllVffv60zB2rjpv13mkWckuoVrfizYqWJOkb7EnBPA5oF0JoDSwhUSJfvPNOIYRawNHAJaWaUJJ0wMzI3cD1IyayYNVmfnJie4YcfbArvSRJ5VpKSuA3p3WiYVYm//fGDNZuzmfYoF7UyNiTj0IqN6rWhlaHJy7bFRXC6jnJERvJkwnOfQcmP/n1j5OSDrWbJ0rjDqcmT6DXascq5BoNLZAlSdrJNx5VRVFUGEK4ERgFpAIPRVGUE0K4Lnn7sOSuZwNvRlG0eb+llSTtN89NWMyv/j2FrMx0nry6H/3a1Is7kiRJpSKEwJBjDqZBVga3PP8ZF93/MQ8P7kODrIy4o2l/Sk2Dhh0SF87fsX3Tih1znQvyvrwKOasxpLjCXZKkvRGiKJ5RyNnZ2dH48eNjeW5J0g55BUXc+lIOT49fRL82dbl7QA8aZmXGHUtSORZCmBBFUfY376mKpjwc4/935gquf2IiDbIyeOyKPrSqXz3uSJIkSWXe7o7xUw50GElS2TF/1WbOuucjnh6/iBuPbcsTV/a1XJYkVWjHtm/Ik1f3ZWNeAecNG82UxevjjiRJklSuWTBLUiX1+pRlnP7P/5G7IY+HL+/Nj09sT1qqfy1Ikiq+Hi3q8NyQ/mSkpXLR/R/z4eyVcUeSJEkqt2wSJKmSyS8s5rcv53D9iIm0a1SD1753JMe2bxh3LEmSDqiDG9Tghev707xuNa54ZBwvTVoSdyRJkqRyyYJZkiqRxWu3cP59H/PI6AVccXhrnr7mMJrWrhp3LEmSYtGoZiZPX3sYPVvU4fsjJzH8w3lxR5IkSSp30uIOIEk6MN6dsZwfPjOZoqKIoQN7cnLXg+KOJElS7GpVTefRK/pw89OTuP216azcuI1bTupASkqIO5okSVK54ApmSargCouK+dMbM7jikfEcVKsqr9x0hOWyJEklZKan8q+LezKoX0vu+2AeP352MgVFxXHHkiRJKhdcwSxJFdiKDXnc9NSnjJm/hgF9mnPr6Z3JTE+NO5YkSWVOakrg92d2plHNDO56cxarNuczdGBPqmf4kUmSJGl3XMEsSRXU6LmrOOXu//HZ4vX89YJDueOcbpbLkiTtRgiBG7/TjjvP6cr/Zq/k4gc+YfWmbXHHkiRJKtMsmCWpgikujvjXu7O5ZPgYalVN46UbD+ecns3ijiVJUrlxUZ8W3Dcomxm5Gzlv2McsWrMl7kiSJElllgWzJFUgazbnc/kj47jrzVmcfmgTXr7xCA5plBV3LEmSyp3jOzVixFV9WbM5n3OGjmba0g1xR5IkSSqTLJglqYKY8PlaTrv7Qz6eu5rbz+rC3y/s7txISZK+hexWdXnuusNISwlceN/HjJ67Ku5IkiRJZY4FsySVc1EU8eD/5nPhfR+Tmhp4fkh/LunXkhBC3NEkSSr32jXK4oXr+3NQ7UwGPzSO1z5bFnckSZKkMsWCWZLKsQ15BVw/YiK3vTqNYzs05NUbj6Rrs1pxx5IkqUI5qFZVnr22P4c2r8WNT03k0dEL4o4kSZJUZvjdaUkqp3KWrueGERNZtHYrvzylI1cd2dpVy5Ik7Se1qqXz+JV9uempT7n15RxWbMzjxye09+9eSZJU6bmCWZLKmSiKeGrsQs6+dzR5BcU8fU0/rj6qjR9wJUnazzLTUxk6sCcD+jTnnv/O5ZbnP6OwqDjuWJIkSbFyBbMklSNb8gv51YtTeeHTJRzZrj5/v7A79WpkxB1LkqRKIy01hT+e3ZUGWZnc/c5sVm/K518X96RqldS4o0mSJMXCFcySVE7MWbGRs+75iBcnLeHm7x7CI5f3sVyWJCkGIQR+ePwh3H5WF96duYKBwz9h7eb8uGNJkiTFwoJZksqBlyYt4Yx/fcTqTfk8fkVfvv/ddqSmOBJDkqQ4XdKvJUMH9mTq0g2cN2w0S9ZtjTuSJEnSAWfBLEllWF5BEb98cQrfHzmJzk1q8tr3juSIdvXjjiVJkpJO6nIQj1/RhxUbt3HOvR8xI3dD3JEkSZIOKAtmSSqjFq7ewnnDRjNizEKuPaoNT17dj8a1MuOOJUmSdtK3TT2eve4wAM4f9jFj5q2OOZEkSdKBY8EsSWXQqJxcTv3nhyxcvYUHLs3m56d0JD3VP7IlSSqrOjSuyfND+tMgK4NBD43ljam5cUeSJEk6IGwrJKkMKSgq5g+vTePaxyfQql51XvvekRzfqVHcsSRJ0h5oVqcaz1/Xn85NanL9iAmMGPN53JEkSZL2OwtmSSojlq3fyoD7P+GBD+czqF9LnhtyGM3rVos7liRJ2gt1qldhxFV9OaZ9Q3754lT+9tYsoiiKO5YkSdJ+kxZ3AEkSfDh7Jd8fOYm8giLuHtCDMw5tEnckSZK0j6pVSeO+Qb34xQtT+Mc7s1mxcRu3n9WF1JQQdzRJkqRSZ8EsSTEqKo64+53Z3P3ubA5pmMU9A3vStmGNuGNJkqRvKT01hT+d140GWRnc+95cVm/axt0DepCZnhp3NEmSpFLliAxJismqTdu47KGx/OOd2ZzToxn/vuFwy2VJkiqQEAI/PakDvz29E29NX86gB8ewfktB3LEkSZJKlQWzJMVg7Pw1nHr3h4xbsIY/nduNu87vRtUqrmiSJKkiGnx4a/45oAeTF63ngvs+Ztn6rXFHkiRJKjUWzJJ0AEVRxLD35zLggU+omp7Ki9cfzgW9mxOCMxklSarITuvWhEcu782SdVs5997RzFmxMe5IkiRJpcKCWZIOkPVbCrj6sfHc+Z8ZnNi5Ea/cdASdmtSMO5YkSTpA+retz9PX9qOgOOLcoR8z4fM1cUeSJEn61iyYJekA+GzxOk7954e8P2slt57eiXsu7klWZnrcsSRJ0gHWuUktXhjSnzrV0hk4fAxvT1sedyRJkqRvxYJZkvazlyYt4byhH1NcHPHMtYdx+eGtHYkhSVIl1rxuNZ4b0p9DGmVx7RMTeHrcwrgjSZIk7TMLZknaj96atpwfPjOZHi1q89r3jqRHizpxR5IkSWVA/RoZPHV1Pw5vW59bnp/Cv96dTRRFcceSJEnaaxbMkrSfjJ67ihuenEiXprV4cHBv6lSvEnckSZJUhlTPSGP4pdmc3aMpd705i1tfzqGo2JJZkiSVL2lxB5CkimjSonVc/eh4WtWrxiODe1Mjwz9uJUnSV1VJS+Ev5x9Kg6wM7v9gHqs2beOvF3QnMz017miSJEl7xMZDkkrZrOUbGfzwWOrWqMLjV/Z15bIkSdqtlJTAL07pSMOsDG5/bTprNo/l/kuzqekJgSVJUjngiAxJKkWL1mxh0INjSE9N4Ykr+9KoZmbckSRJUjlx1ZFt+PuF3Rm/YC0XDPuY5Rvy4o4kSZL0jSyYJamUrNiQx8DhY8grKOaJK/vSsl71uCNJkqRy5qweTXlocG8WrtnCOfeOZu7KTXFHkiRJ2i0LZkkqBeu25DPowbGs2rSNRy7vTfvGWXFHkiRJ5dRRhzRg5DX9yCso4ryho5m0aF3ckSRJkr6WBbMkfUubtxUy+OFxzF+1mQcuzaZHizpxR5IkSeVct2a1eX5If7Iy0xlw/yf8d+aKuCNJkiTtkgWzJH0LeQVFXPP4eKYsWc8/L+7B4W3rxx1JkiRVEK3qV+e5IYfRpkF1rn50PM9PWBx3JEmSpK+wYJakfVRYVMz3nvqUj+as5k/nduPEzo3jjiRJkiqYhlmZjLymH33b1OVHz05m2PtziaIo7liSJElfsGCWpH1QXBzx0+c/481py/nt6Z04t1ezuCNJkqQKKisznYcG9+a0bgdx539mcNur0ykutmSWJEllQ1rcASSpvImiiN+/Oo0XJi7h5u8ewuDDW8cdSZIkVXAZaancfVEPGmRl8NBH81m5aRt3nd+NjLTUuKNJkqRKzoJZkvbS39+ezSOjF3DF4a353nFt444jSZIqiZSUwG9O60TDrEz+740ZrN2cz7BBvaiR4cc6SZIUH0dkSNJeePB/8/nHO7M5v1czfnVqR0IIcUeSJEmVSAiBIccczF3nH8rH81Zz0f0fs3LjtrhjSZKkSsyCWZL20DPjF3Hbq9M4qXNj7jinKykplsuSpLIthHBSCGFmCGFOCOFnu7g9hBDuTt7+WQihZ4nbbg4h5IQQpoYQngohZB7Y9Nqd83o1Y/hl2cxdsZlzh45mwarNcUeSJEmVlAWzJO2BN6Yu42fPf8aR7erzjwHdSUv1j09JUtkWQkgF7gFOBjoBA0IInXba7WSgXfJyDTA0ed+mwPeA7CiKugCpwEUHKLr20LHtG/Lk1X3ZmFfAecNGM2Xx+rgjSZKkSsiGRJK+wf9mr+J7T03i0Oa1GXZJL0+mI0kqL/oAc6IomhdFUT4wEjhzp33OBB6LEj4BaocQDkrelgZUDSGkAdWApQcquPZcjxZ1eG5IfzLSUrno/o/5cPbKuCNJkqRKxoJZknZj4sK1XPP4eNo0qM4jg/tQ3ZPoSJLKj6bAohLXFye3feM+URQtAe4CFgLLgPVRFL25qycJIVwTQhgfQhi/cqXlZhwOblCDF67vT/O61bjikXG8NGlJ3JEkSVIlYsEsSV9j+rINDH5oLA2zMnjsyj7UqpYedyRJkvbGrk4WEO3JPiGEOiRWN7cGmgDVQwiX7OpJoii6P4qi7CiKshs0aPCtAmvfNaqZydPXHkbPFnX4/shJDP9wXtyRJElSJWHBLEm7sGDVZgY9OJZqVdJ4/Mq+NMzyvEaSpHJnMdC8xPVmfHXMxdft811gfhRFK6MoKgBeAPrvx6wqBbWqpvPoFX04uUtjbn9tOne8Pp3i4p3/TUGSJKl0WTBL0k5y1+cxcPgYioqLeeKqPjSvWy3uSJIk7YtxQLsQQusQQhUSJ+l7ead9XgYuDQn9SIzCWEZiNEa/EEK1EEIAjgOmH8jw2jeZ6an86+KeDOrXkvs+mMePn51MQVFx3LEkSVIF5jBRSSphzeZ8LnlwDOu3FvDk1X1p2zAr7kiSJO2TKIoKQwg3AqOAVOChKIpyQgjXJW8fBrwOnALMAbYAlydvGxNCeA6YCBQCnwL3H/hXoX2RmhL4/ZmdaZiVwV/emsWqzfkMHdjTc0lIkqT9wiMMSUramFfA4IfHsmjNFh69og/dmtWOO5IkSd9KFEWvkyiRS24bVuLnCLjha+57K3Drfg2o/SaEwE3HtaNBVga/eHEKFz/wCQ8N7k29GhlxR5MkSRWMIzIkCcgrKOKqR8czbekG7h3Yk35t6sUdSZIk6Vu7qE8L7huUzYzcjZw37GMWrdkSdyRJklTBWDBLqvQKioq5YcRExi5Yw18uOJTjOjaKO5IkSVKpOb5TI0Zc1Zc1m/M5Z+hopi3dEHckSZJUgVgwS6rUiosjfvzsZN6ZsYLfn9mFM7s3jTuSJElSqctuVZfnrjuMtJTAhfd9zOi5q+KOJEmSKggLZkmVVhRF3PpyDi9NWspPTmzPoH4t444kSZK037RrlMUL1/enca1MBj80jtc+WxZ3JEmSVAFYMEuqtO56cyaPf/I51x7VhuuPOTjuOJIkSfvdQbWq8ux1h9GtWS1ufGoij45eEHckSZJUzlkwS6qU7v9gLvf8dy4D+jTnZyd3IIQQdyRJkqQDona1KjxxVV+O69CIW1/O4c+jZhBFUdyxJElSOWXBLKnSGTl2IX98fQandjuI28/qarksSZIqncz0VIZd0pMBfZpzz3/ncsvzn1FYVBx3LEmSVA6lxR1Akg6k1z5bxs9fnMLRhzTgbxd0JzXFclmSJFVOaakp/PHsrjTIyuTud2azelM+/7q4J1WrpMYdTZIklSOuYJZUabw3cwU/ePpTslvWYdglvaiS5h+BkiSpcgsh8MPjD+H2s7rw7swVDBz+CWs358cdS5IklSO2K5IqhXEL1nDdExNo1zCL4Zf1dmWOJElSCZf0a8nQgT2ZunQD5w0bzZJ1W+OOJEmSygkLZkkVXs7S9VzxyDia1KrKY1f2oVbV9LgjSZIklTkndTmIx67ow4qN2zjn3o+Ykbsh7kiSJKkcsGCWVKHNW7mJSx8cS1ZGGo9f1Zf6NTLijiRJklRm9WtTj2evOwyA84d9zJh5q2NOJEmSyjoLZkkV1pJ1W7lk+BgAnriqL01rV405kSRJUtnXoXFNnh/SnwZZGQx6aCxvTM2NO5IkSSrDLJglVUirNm1j0PAxbMwr5NEr+tCmQY24I0mSJJUbzepU47nr+tPpoJpcP2ICI8Z8HnckSZJURlkwS6pwNuQVcNlDY1m6fisPXd6bLk1rxR1JkiSp3KlbvQpPXt2Xow9pwC9fnMrf3ppFFEVxx5IkSWWMBbOkCmVrfhFXPjKOWcs3MuySXvRuVTfuSJIkSeVWtSpp3H9pNuf1asY/3pnNL16cSlGxJbMkSdohLe4AklRa8guLGTJiAuM/X8s/B/TgmPYN444kSZJU7qWnpvDn87rRMCuDe9+by+pN27h7QA8y01PjjiZJksoAVzBLqhCKiiNufmYS781cyR/P7spp3ZrEHUmSJKnCCCHw05M68NvTO/HW9OUMenAM67cUxB1LkiSVARbMksq9KIr41b+n8Npny/j5yR0Y0KdF3JEkSZIqpMGHt+afA3owedF6zr9vNMvWb407kiRJipkFs6Ry7843ZvDU2EVcf8zBXHv0wXHHkSRJqtBO69aERy7vzdJ1eZx772jmrNgYdyRJkhQjC2ZJ5dq9783hvvfncUm/FvzkxPZxx5EkSaoU+retz8hr+pFfFHHu0I+Z8PmauCNJkqSYWDBLKree+ORz/vTGTM7s3oTfn9GFEELckSRJkiqNLk1r8cKQ/tSpls7A4WN4e9ryuCNJkqQYWDBLKpdemrSEX780leM6NOSu8w8lJcVyWZIk6UBrUa8azw3pzyGNsrj2iQk8PW5h3JEkSdIBZsEsqdx5d8ZyfvTMZPq0qss9A3uSnuofZZIkSXGpXyODp67ux+Ft63PL81P417uziaIo7liSJOkAsZWRVK58Mm81Q56YSMeDajL8smwy01PjjiRJklTpVc9IY/il2Zzdoyl3vTmLW1/OoajYklmSpMogLe4AkrSnpixez1WPjqd53Wo8ekUfsjLT444kSZKkpCppKfzl/EOpX6MKD3w4n1WbtvHXC7q7IECSpArOgllSuTBnxUYufWgMtaqm8/iVfahbvUrckSRJkrSTlJTAL0/tRMOsTP7w+nTWbB7L/ZdmU9OFAZIkVViOyJBU5i1as4VLho8lNSWFEVf15aBaVeOOJEmSpN24+qg2/P3C7oxfsJYLhn3M8g15cUeSJEn7iQWzpDJtxcY8Bj04hi35hTx+ZR9a1a8edyRJkiTtgbN6NOWhwb1ZuGYL59w7mrkrN8UdSZIk7QcWzJLKrPVbCrj0wbEs37CNhy/vQ8eDasYdSZIkSXvhqEMaMPKafuQVFHHe0NFMWrQu7kiSJKmUWTBLKpO25Bdy+SNjmbdyM/df2oteLevEHUmSJEn7oFuz2jw3pD81MtMYcP8n/HfmirgjSZKkUmTBLKnM2VZYxLWPT2DSonXcPaA7R7ZrEHckSZIkfQut61fn+SH9aV2/Olc/Op7nJyyOO5IkSSolFsySypTComJ+MHISH85exZ3nduOkLgfFHUmSJEmloGFWJk9f248+revyo2cnM+z9uURRFHcsSZL0LVkwSyoziosjfv7CFP4zNZdfn9aJC7Kbxx1JkiRJpSgrM52HL+/Nad0O4s7/zOC2V6dTXGzJLElSeZYWdwBJAoiiiD+8Pp1nJyzme8e148ojWscdSZIkSftBRloqd1/UgwZZGTz00XxWbtrGXed3IyMtNe5okiRpH1gwSyoT/vnuHB7833wG92/Fzd9tF3ccSZIk7UcpKYHfnNaJhlmZ/N8bM1i7OZ9hg3pRI8OPqJIklTeOyJAUu0c+ms9f35rFOT2b8pvTOhFCiDuSJEmS9rMQAkOOOZg/n9eNj+et5qL7P2blxm1xx5IkSXvJgllSrF6YuJjfvjKNEzo14k/ndiMlxXJZkiSpMjk/uznDL81m7orNnDt0NAtWbY47kiRJ2gsWzJJi82ZOLj957jP6H1yPuwf0IC3VP5IkSZIqo2M7NOTJq/uyMa+A84aNZsri9XFHkiRJe8g2R1IsRs9ZxY1PfkqXprW4/9JsMtM9qYskSVJl1qNFHZ4b0p+MtFQuuv9jPpy9Mu5IkiRpD1gwSzrgJi1ax1WPjadV/Wo8enlvT+YiSZIkAA5uUIMXru9P87rVuOKRcbw0aUnckSRJ0jewYJZ0QM3M3cjgh8dSv0YGj1/Zl9rVqsQdSZIkSWVIo5qZPH3tYfRoUYfvj5zE8A/nxR1JkiTthgWzpANm4eotDHpwDFVSUxhxVV8a1cyMO5IkSZLKoFpV03nsij6c1Lkxt782nTten05xcRR3LEmStAsWzJIOiOUb8hj44CfkFxXzxFV9aV63WtyRJEmSVIZlpqdyz8CeXNKvBfd9MI8fPzuZgqLiuGNJkqSdOPhU0n63dnM+gx4cw5pN+Yy4uh+HNMqKO5IkSZLKgdSUwG1ndqFRViZ/eWsWqzbnM3RgT6p7Dg9JksqMPVrBHEI4KYQwM4QwJ4Tws6/Z55gQwqQQQk4I4f3SjSmpvNq0rZDBj4xjweotPHBZNt2b1447kiRJksqREAI3HdeOO8/pyv9mr+TiBz5h9aZtcceSJElJ31gwhxBSgXuAk4FOwIAQQqed9qkN3AucEUVRZ+D80o8qqbzJKyjimsfGM3XJeu65uCf9D64fdyRJkiSVUxf1acF9g7KZkbuR84Z9zKI1W+KOJEmS2LMVzH2AOVEUzYuiKB8YCZy50z4XAy9EUbQQIIqiFaUbU1J5U1hUzE1Pfcrouau56/xuHN+pUdyRJEmSVM4d36kRI67qy5rN+ZwzdDQ5S9fHHUmSpEpvTwrmpsCiEtcXJ7eVdAhQJ4TwXghhQgjh0l09UAjhmhDC+BDC+JUrV+5bYkllXnFxxE+f+4y3pi3nd2d05uwezeKOJEmSpAoiu1Vdnr3uMNJSAhfd9wmj566KO5IkSZXanhTMYRfbop2upwG9gFOBE4FfhxAO+cqdouj+KIqyoyjKbtCgwV6HlVT2RVHE71+dxgufLuFHxx/CZf1bxR1JkiRJFcwhjbJ4fkh/GtfKZPBD43jts2VxR5IkqdLak4J5MdC8xPVmwNJd7PNGFEWboyhaBXwAHFo6ESWVJ397ezaPjF7AVUe05sbvtI07jiRJkiqoJrWr8ux1h9GtWS1ufGoij45eEHckSZIqpT0pmMcB7UIIrUMIVYCLgJd32ucl4MgQQloIoRrQF5heulEllXXDP5zH3e/M5oLsZvzy1I6EsKsvQEiSJEmlo3a1KjxxVV+O69CIW1/O4c+jZhBFO3/hVpIk7U/fWDBHUVQI3AiMIlEaPxNFUU4I4boQwnXJfaYDbwCfAWOB4VEUTd1/sSWVNc+MW8Ttr03n5C6NueOcbpbLkiSVASGEk0IIM0MIc0IIP9vF7SGEcHfy9s9CCD2T29uHECaVuGwIIfzggL8AaQ9kpqcy7JKeDOjTnHv+O5dbnv+MwqLiuGNJklRppO3JTlEUvQ68vtO2YTtd/zPw59KLJqm8+M+UZfzshc84sl19/n5Rd1JTLJclSYpbCCEVuAc4nsRIu3EhhJejKJpWYreTgXbJS19gKNA3iqKZQPcSj7MEePHApZf2TlpqCn88uysNamRw97tzWL0pn39d3JOqVVLjjiZJUoW3JyMyJOlrfTh7Jd8fOYkeLepw36BeZKR5EC9JUhnRB5gTRdG8KIrygZHAmTvtcybwWJTwCVA7hHDQTvscB8yNoujz/R9Z2nchBH54QntuO6sL785cwcDhn7B2c37csSRJqvAsmCXtswmfr+WaxybQpkF1HrqsN9Wq7NGXIiRJ0oHRFFhU4vri5La93eci4Kmve5IQwjUhhPEhhPErV678FnGl0jGoX0uGDuzJ1KUbOG/YaJas2xp3JEmSKjQLZkn7ZPqyDVz+8Fga1czg8Sv7UqtaetyRJEnSl+1qZtXOZz/b7T7Jk3yfATz7dU8SRdH9URRlR1GU3aBBg30KKpW2k7ocxGNX9GHFxm2cc+9HzMjdEHckSZIqLAtmSXtt/qrNDHpwLNWqpPHEVX1pkJURdyRJkvRVi4HmJa43A5bu5T4nAxOjKFq+XxJK+1G/NvV49rrDiCI4f9jHjJm3Ou5IkiRVSBbMkvbKsvVbuWT4GIqjiCeu6kOzOtXijiRJknZtHNAuhNA6uRL5IuDlnfZ5Gbg0JPQD1kdRtKzE7QPYzXgMqazr0LgmL1zfnwZZGQx6aCxvTM2NO5IkSRWOBbOkPbZ60zYuGT6G9VsLePTyPrRtmBV3JEmS9DWiKCoEbgRGAdOBZ6IoygkhXBdCuC652+vAPGAO8ABw/fb7hxCqAccDLxzQ4FIpa1anGs9d159OB9Xk+hETGDHG81VKklSaPCOXpD2yMa+AwQ+PY/HarTx2RR+6NqsVdyRJkvQNoih6nUSJXHLbsBI/R8ANX3PfLUC9/RpQOkDqVq/Ck1f35YYRE/nli1NZsWEbP/huO0LY1RhySZK0N1zBLOkb5RUUceWj45m+bANDL+lJ3zZ+1pQkSVL5Uq1KGvdfms15vZrxj3dm84sXp1JUvPN5LyVJ0t5yBbOk3SooKub6ERMZt2ANf7+wO9/p0CjuSJIkSdI+SU9N4c/ndaNhVgb3vjeXVZu2cfdFPahaJTXuaJIklVuuYJb0tYqLI3787GTenbGC28/qwpndm8YdSZIkSfpWQgj89KQO/Pb0Trw9fTkDHviE1Zu2xR1LkqRyy4JZ0i5FUcRvXp7KS5OW8tOT2jOwb8u4I0mSJEmlZvDhrRk6sBfTl23gnKGjmb9qc9yRJEkqlyyYJe3Sn0fN5IlPFnLt0W24/pi2cceRJEmSSt1JXRrz5NX92LC1gHOHjmbC52vjjiRJUrljwSzpK+57fy73vjeXAX1a8LOTOsQdR5IkSdpverWswwvXH05WZhoXP/AJb0zNjTuSJEnligWzpC95auxC7vjPDE7rdhC3n9WFEELckSRJkqT9qnX96rwwpD8dD6rJkBETePij+XFHkiSp3LBglvSFVyYv5RcvTuGY9g346wXdSU2xXJYkSVLlUK9GBk9d3Y/vdmzE716Zxu2vTqO4OIo7liRJZZ4FsyQA/jtzBTc/PYneLesydGAvqqT5x4MkSZIql6pVUhl2SS8uO6wlw/83nxufmkheQVHcsSRJKtNskCQxdv4ahjwxgfaNsxg+OJuqVVLjjiRJkiTFIjUl8NszOvPLUzry+pRcLhk+hrWb8+OOJUlSmWXBLFVyU5es58pHxtGkdlUeu6IPNTPT444kSZIkxSqEwNVHteFfF/fgs8XrOXfYaBau3hJ3LEmSyiQLZqkSm7tyE5c9NJaaVdN54sq+1KuREXckSZIkqcw4rVsTnriqL6s35XPO0I+YvGhd3JEkSSpzLJilSmrJuq0MGj6GEODxK/vQpHbVuCNJkiRJZU6f1nV5fkh/MtNTuej+T3h72vK4I0mSVKZYMEuV0KpN2xg0fAwbtxXy6BV9aNOgRtyRJEmSpDKrbcMavHB9f9o2rME1j4/n8U8+jzuSJEllhgWzVMms31rApQ+OZen6rTw8uDedm9SKO5IkSZJU5jXMymTkNf04pn1Dfv3vqdz5nxkUF0dxx5IkKXYWzFIlsjW/iCsfGcfsFRu5b1A22a3qxh1JkiRJKjeqZ6Rx/6BeXNy3BcPen8sPnp7EtsKiuGNJkhSrtLgDSDow8guLue6JCUxcuJZ/DujJ0Yc0iDuSJEmSVO6kpabwh7O60KxOVf70xkyWb8jj/kHZ1KqWHnc0SZJi4QpmqRIoKo64+elJvD9rJX88uyundjso7kiSJElSuRVC4Ppj2vL3C7szceFazhs2msVrt8QdS5KkWFgwSxVcFEX88sUpvDZlGb88pSMX9WkRdyRJkiSpQjirR1MevaIPuRvyOPve0Uxdsj7uSJIkHXAWzFIFFkURd/5nBiPHLeLGY9ty9VFt4o4kSZIkVSj9D67P80P6k54SuOC+j3lv5oq4I0mSdEBZMEsV2L3vzeW+D+Zx6WEt+dEJh8QdR5IkSaqQDmmUxYs3HE6retW58tHxPD1uYdyRJEk6YCyYpQrq8U8+58+jZnJW9yb89vTOhBDijiRJkiRVWI1qZvLMdYdxeNv63PL8FP765kyiKIo7liRJ+50Fs1QBvTRpCb95aSrf7diQP59/KCkplsuSJEnS/lYjI40HL8vmguxm3P3uHH70zGTyC4vjjiVJ0n6VFncASaXrnenL+eEzk+nbui7/urgn6an+O5IkSZJ0oKSnpvB/53ajae1q/O3tWSzfmMfQS3pRMzM97miSJO0XNk9SBfLx3NVcP2IinZvUZPhlvclMT407kiRJklTphBD4/nfbcdf5hzJm3houGPYxy9ZvjTuWJEn7hQWzVEF8tngdVz06jhZ1q/HI5X2okeEXFCRJkqQ4nderGQ9f3pvFa7dy9j2jmb5sQ9yRJEkqdRbMUgUwe/lGLntoLHWqV+HxK/tSt3qVuCNJkiRJAo5s14Bnrj2MiIjzh33M/2avijuSJEmlyoJZKucWrdnCoAfHkpaawoir+tK4VmbckSRJkiSV0KlJTV68/nCa1q7K4IfH8tyExXFHkiSp1FgwS+XYig15XPLgGLYWFPH4lX1oWa963JEkSZIk7UKT2lV5dshh9G1Tlx8/O5m735lNFEVxx5Ik6VuzYJbKqXVb8rn0obGs3LiNhy/vTYfGNeOOJEmSJGk3amam8/DgPpzToyl/fWsWP3t+CgVFxXHHkiTpW/EsYFI5tHlbIZc/Mo55Kzfz0ODe9GxRJ+5IkiRJkvZAlbQU/nLBoTStU5V/vjuH3A153DOwpyfpliSVW65glsqZbYVFXPfEBCYvWsfdA3pwRLv6cUeSJEmStBdCCPzohPbccU5X/jdnFRcM+5jlG/LijiVJ0j6xYJbKkcKiYr7/1CQ+nL2KP513KCd1aRx3JEmSJEn7aECfFgy/LJsFqzdzzr2jmbV8Y9yRJEnaaxbMUjlRXBzxsxem8EZOLr85rRPn9WoWdyRJkiRJ39Kx7RvyzLWHkV9UzLlDR/Px3NVxR5Ikaa9YMEvlQBRF3P7adJ6bsJjvH9eOK45oHXckSZIkSaWkS9NavDCkP41qZnLZQ2N5adKSuCNJkrTHLJilcuDud+bw0EfzufzwVvzgu+3ijiNJkiSplDWvW43nr+tPjxa1+f7ISdz73hyiKIo7liRJ38iCWSrjHv5oPn97exbn9WrGr0/tRAgh7kiSJEmS9oNa1dJ57Mo+nH5oE/70xkx+9e+pFBYVxx1LkqTdSos7gKSv9/yExfzulWmc2LkRd57TlZQUy2VJkiSpIstIS+UfF3anae2qDHt/Lrnr8/jnxT2oVsWP75KksskVzFIZNSonl58+/xmHt63HPy7qQVqq/7tKkiRJlUFKSuBnJ3fgtjM789+ZK7jo/k9YuXFb3LEkSdolGyupDPpozipuevJTujatxf2DsslMT407kiRJkqQDbNBhrbhvUDazlm/knKEfMXflprgjSZL0FRbMUhnz6cK1XP3YeFrXr84jl/emeoZfhZMkSZIqq+M7NWLkNYexZVsR5w4dzbgFa+KOJEnSl1gwS2XIzNyNDH54HA2yMnj8yj7UrlYl7kiSJEmSYta9eW1evP5w6larwsDhY3jts2VxR5Ik6QsWzFIZ8fnqzVzy4Bgy01N44sq+NKyZGXckSZIkSWVEi3rVeH5If7o2rcUNT05k+IfziKIo7liSJFkwS2XB8g15XPLgGAqLinniyr40r1st7kiSJEmSypg61asw4qq+nNylMbe/Np3fvTKNomJLZklSvCyYpZit3ZzPJcPHsGZTPo9c3od2jbLijiRJkiSpjMpMT+Wei3ty5RGteWT0Aq4fMYGt+UVxx5IkVWIWzFKMNm0rZPDDY/l8zRaGX9abQ5vXjjuSJEmSpDIuJSXw69M68ZvTOvHmtOVcPPwTVm/aFncsSVIlZcEsxSSvoIirHx3P1KUbuPfinhx2cL24I0mSJEkqR644ojVDB/Zk2tINnDt0NAtWbY47kiSpErJglmJQUFTMjU9+yifzV/OX8w/lu50axR1JkiRJUjl0UpeDePLqvqzfWsA5Q0czceHauCNJkioZC2bpACsujvjpc5/x9vTl/P6MzpzVo2nckSRJkiSVY71a1uWF6w8nKzONAfd/whtTc+OOJEmqRCyYpQMoiiJ+90oOL366hB+fcAiDDmsVdyRJkiRJFUDr+tV5fkh/OhxUkyEjJvDIR/PjjiRJqiQsmKUD6G9vzeLRjz/n6iNbc8OxbeOOI0mSJKkCqV8jg5FX9+O7HRvx21em8YfXplFcHMUdS5JUwVkwSwfI8A/ncfe7c7gwuzm/OKUjIYS4I0mSJEmqYKpWSWXYJb249LCWPPDhfG566lPyCorijiVJqsAsmKUD4Jlxi7j9temc2vUg/nhOV8tlSZJ0QIQQTgohzAwhzAkh/GwXt4cQwt3J2z8LIfQscVvtEMJzIYQZIYTpIYTDDmx6SfsqNSXwuzM684tTOvDalGUMenAM67bkxx1LklRBWTBL+9nrU5bxsxc+46hDGvC3C7uTmmK5LEmS9r8QQipwD3Ay0AkYEELotNNuJwPtkpdrgKElbvsH8EYURR2AQ4Hp+z20pFITQuCaow7mnwN6MHnRes4ZOppFa7bEHUuSVAFZMEv70fuzVvL9kZ/Ss0Udhl3Skypp/i8nSZIOmD7AnCiK5kVRlA+MBM7caZ8zgceihE+A2iGEg0IINYGjgAcBoijKj6Jo3QHMLqmUnH5oE564qi+rN+Vz9r0f8dnidXFHkiRVMLZd0n4y4fM1XPf4BNo2zOLBwb2pViUt7kiSJKlyaQosKnF9cXLbnuzTBlgJPBxC+DSEMDyEUH1XTxJCuCaEMD6EMH7lypWll15SqenTui7PDzmMzPRULrzvE96ZvjzuSJKkCsSCWdoPpi3dwOCHx9G4ViaPXdGHWlXT444kSZIqn13N5Yr2cJ80oCcwNIqiHsBm4CsznAGiKLo/iqLsKIqyGzRo8G3yStqP2jbM4oXr+9O2YQ2ufmw8I8Z8HnckSVIFYcEslbL5qzZz6UNjqJGRxhNX9aVBVkbckSRJUuW0GGhe4nozYOke7rMYWBxF0Zjk9udIFM6SyrGGWZmMvKYfRx/SgF++OJX/e2MGxcU7/7uTJEl7x4JZKkVL123lkuFjKI7g8Sv70rR21bgjSZKkymsc0C6E0DqEUAW4CHh5p31eBi4NCf2A9VEULYuiKBdYFEJon9zvOGDaAUsuab+pnpHGA5dmM6BPC4a+N5ebn5nEtsKiuGNJksoxh8JKpWT1pm1c8uAYNmwt4Klr+tG2YY24I0mSpEosiqLCEMKNwCggFXgoiqKcEMJ1yduHAa8DpwBzgC3A5SUe4iZgRLKcnrfTbZLKsbTUFP54dhea1anKn0fNZPmGPO4blO1oP0nSPrFglkrBhrwCLnt4LEvWbuXxK/vSpWmtuCNJkiQRRdHrJErkktuGlfg5Am74mvtOArL3Zz5J8QkhcMOxbWlSO5OfPvcZ5w0dzSNX9PFbmJKkveaIDOlbyiso4qpHxzNj2UaGXdKLPq3rxh1JkiRJkvbI2T2a8egVfcjdkMfZ93zE1CXr444kSSpnLJilb6GgqJjrR0xk3II1/O3C7hzboWHckSRJkiRpr/Q/uD7PXdeftJTAhfd9zPuzVsYdSZJUjlgwS/uoqDjih89M5t0ZK/jDWV05/dAmcUeSJEmSpH3SvnEWL1x/OC3qVeeKR8bx9LiFcUeSJJUTFszSPoiiiF+/NJVXJi/lZyd34OK+LeKOJEmSJEnfSuNamTxzbT/6H1yPW56fwl/fnEliVLskSV/PglnaB38aNZMnxyxkyDEHc93RB8cdR5IkSZJKRVZmOg8N7s35vZpx97tz+PGzn5FfWBx3LElSGZYWdwCpvBn2/lyGvjeXgX1b8NMT28cdR5IkSZJKVXpqCn86rxtN61Tl72/PZvmGPIZe0pOszPS4o0mSyiBXMEt74ckxC7nzPzM4/dAm/P7MLoQQ4o4kSZIkSaUuhMAPvnsIfz6vG5/MW835wz5m2fqtcceSJJVBFszSHnp58lJ++e8pfKdDQ/56waGkplguS5IkSarYzs9uzkODe7N47VbOvmc0M3I3xB1JklTGWDBLe+C9mSv44dOT6N2qLvcO7El6qv/rSJIkSaocjjqkAc9cexgREecP/ZiP5qyKO5IkqQyxJZO+wYzcDdwwYiLtG2cx/LJsMtNT444kSZIkSQdUpyY1efH6w2lSuyqXPTSW5ycsjjuSJKmMsGCWdmPVpm1c+ch4amSm8eBlvanpSS0kSZIkVVJNalflmesOo0/ruvzo2cn8853ZRFEUdyxJUswsmKWvkVdQxDWPjWf15m0Mv7Q3jWtlxh1JkiRJkmJVq2o6j1zeh7N7NOUvb83i5y9MoaCoOO5YkqQYpcUdQCqLoijiZ89/xsSF67h3YE+6NqsVdyRJkiRJKhOqpKXw1wsOpWntqvzrv3NYtj6Pewb2pEaGFYMkVUauYJZ24Z7/zuHfk5by4xMO4ZSuB8UdR5IkSZLKlBACPz6xPX88uyv/m7OKC+/7mBUb8uKOJUmKgQWztJPXpyzjrjdncXaPptxwbNu440iSJElSmXVx3xYMvzSb+as2c/a9o5m9fGPckSRJB5gFs1TCZ4vX8cNnJtGrZR3uOKcrIYS4I0mSJElSmXZsh4Y8fc1hbCss5tyho/lk3uq4I0mSDiALZilp2fqtXPXoeOpVz+C+Qb3ITE+NO5IkSZIklQtdm9Xixev70yArg0sfHMtLk5bEHUmSdIBYMEvAlvxCrnp0PJu3FfLg4Gzq18iIO5IkSZIklSvN61bjhSGH071Fbb4/chJD35tLFEVxx5Ik7WcWzKr0iosjfvj0ZKYv28A/L+5Bh8Y1444kSZIkSeVSrWrpPH5lH07rdhD/98YMfv3SVAqLiuOOJUnaj9LiDiDF7a43Z/JGTi6/OrUj3+nQKO44kiRJklSuZaSlcvdFPWhapyr3vT+P3PV53D2gB9WqWEFIUkXkCmZVas9PWMy9781lQJ8WXHlE67jjSJIkSVKFkJIS+PnJHfn9mZ15d8YKBtz/CSs3bos7liRpP7BgVqU1bsEafv7CFPofXI/fn9mZEELckSRJkiSpQrn0sFbcNyibmcs3cs7Qj5i7clPckSRJpcyCWZXSojVbuPbxCTStU5V7B/YkPdX/FSRJkiRpfzi+UyOeurofW7YVce7Q0YxfsCbuSJKkUmSrpkpnY14BVz46jqLiiAcvy6Z2tSpxR5IkSZKkCq1Hizq8cH1/6lSrwsXDx/D6lGVxR5IklRILZlUqhUXF3PTUp8xbuZmhA3vSpkGNuCNJkiRJUqXQsl51nh/Sny5NanLDkxMZ/uG8uCNJkkqBBbMqlT+8Pp33Zq7k92d2oX/b+nHHkSRJkqRKpW71Kjx5dT9O7NSY21+bzu9eyaGoOIo7liTpW7BgVqUxYsznPPzRAq44vDUX920RdxxJkiRJqpQy01O5Z2BPrji8NQ9/tIDrR0wgr6Ao7liSpH1kwaxK4aM5q/jNSzkc274Bvzy1Y9xxJEmSJKlSS00J/Ob0Tvz6tE68OW05Ax74hDWb8+OOJUnaBxbMqvDmrtzEkCcm0LZBDe4e0IPUlBB3JEmSJEkScOURrbn34p5MW7qBc+79iAWrNscdSZK0lyyYVaGt25LPVY+OJz01heGXZZOVmR53JEmSJElSCSd3PYgnr+7L+q0FnDN0NJ8uXBt3JEnSXtijgjmEcFIIYWYIYU4I4We7uP2YEML6EMKk5OU3pR9V2jsFRcUMeWIiS9Zu5b5BvWhet1rckSRJkiRJu9CrZV2eH9KfGhlpDHjgE97MyY07kiRpD31jwRxCSAXuAU4GOgEDQgiddrHrh1EUdU9efl/KOaW9EkURv3lpKh/PW83/ndeV7FZ1444kSZIkSdqNNg1q8ML1/WnfuCbXPjGBR0cviDuSJGkP7MkK5j7AnCiK5kVRlA+MBM7cv7Gkb+fB/83nqbGLuOHYgzm7R7O440iSJEmS9kD9Ghk8dXVfjuvQiFtfzuGPr0+nuDiKO5YkaTf2pGBuCiwqcX1xctvODgshTA4h/CeE0HlXDxRCuCaEMD6EMH7lypX7EFf6Zu/OWM4fXp/OyV0a86Pj28cdR5IkSZK0F6pVSeO+Qb249LCW3P/BPG4a+Sl5BUVxx5IkfY09KZjDLrbt/M+HE4GWURQdCvwT+PeuHiiKovujKMqOoii7QYMGexVU2hMzcjdw05Of0rlJTf5ywaGkpOzqP19JkiRJUlmWmhL43Rmd+fnJHXjts2UMenAM67bkxx1LkrQLe1IwLwaal7jeDFhacofo/9u78/iYr/2P46+TySYSkc2WILFvQYggtKguiku5SnWjqi3VRZffvV1u96vtvVc3t0UVRRddrqouaqu19tj3ImKnmtgjZPn+/piIIEYQ+SaZ9/PxyCMz8z0z85njVI93Ts6xrGOWZZ3Ivj0V8DLGhBZYlSL58OeJ0zw4LgF/X09G398MP29Pu0sSERERERGRq2SM4ZE21RnWO4Y1u4/SfcQidqek2l2WiIhcID8B83KgpjEmyhjjDdwF/JC7gTGmgjHGZN+Oy37d5IIuVuRS0tIzeXhCAsknTzP6/mZUCPS1uyQREREREREpAF0aVeKzB+P48/hpug1fxNo9R+wuSUREcrlswGxZVgbwGDAd2AR8Y1nWBmPMAGPMgOxmPYD1xpg1wDDgLsuytAu/FArLsnhu0lpW7jrCuz0bEx0RaHdJIiIiIiIiUoCaVwvhu0fj8fH0oNfHS5i9+aDdJYmISLb8rGDGsqyplmXVsiyrumVZQ7IfG2lZ1sjs2x9allXfsqxGlmW1sCxr0fUsWiS3j+Zs4/vV+3j21lp0jK5odzkiIiIiIiJyHdQoF8DkQfFUL1ea/uMT+HLpLrtLEhER8hkwixRVU9ftZ+iM3+kWE86gdjXsLkdERERERESuo3IBvnz9cEva1Arjhcnr+Pe0zWRl6ReoRUTspIBZiq21e47w9DeraVo1iLe6R5O9DbiIiIiIiIiUYKV9PPnk/lh6x1Vm+NztPP3Nas5kZNldloiI2/K0uwCRq7H/6Cn6j08gpLQPH9/XFF8vh90liYiIiIiISCHxdHjwZrdoIoL8+M/0LRw8dpqR9zUlsJSX3aWJiLgdrWCWYif1TAb9xydw8nQGY/rGEurvY3dJIiIiIiIiUsiMMQxqV4P3ejUiYWcKd45cxN4jp+wuS0TE7ShglmIlK8vi6a/XsGn/Mf57dwx1KpSxuyQRERERERGxUbeYCMY/EMf+I2l0+2ghG/YdtbskERG3ooBZipWhM7YwbcMBXuhYl5vqlLe7HBERERERESkC4muE8r+B8Tg8DD1HLmbe74fsLklExG0oYJZiY9KKPQyfu53ecVV4sHWU3eWIiIiIiIhIEVK7QgCTH21F5WA/+o1bzjfLd9tdkoiIW1DALMXC8qQUnv9uHfHVQ3i9a32MMXaXJCIiIiIiIkVMhUBfvh3QkvjqIfxt0lrenfk7lmXZXZaISImmgFmKvN0pqTzy2QrCg0ox/J4meDk0bEVERERERCRvAb5ejO3bjDubRjDs16383//Wkp6ZZXdZIiIllqfdBYi4cjwtnQfHLyczy2JMn1jK+nnbXZKIiIiIiIgUcV4OD/7doyHhQaV4f9ZWDh5LY/g9TQjw9bK7NBGREkdLQaXIysjM4vGJq0g8dJIR9zShWpi/3SWJiIiIiIhIMWGMYfDNtfh3j4Ys3p7MnSMXc+Bomt1liYiUOAqYpcgaMnUTc7cc4vWuDYivEWp3OSIiIiIiIlIM9YytzNi+zdidkkq34QvZfOCY3SWJiJQoCpilSPpi6U4+XZhEv1ZR3N28it3liIiIiIiISDF2Y60wvhnQkswsiztHLGbRtj/tLklEpMRQwCxFzsJtf/LylA20qx3Gi53q2l2OiIiIiIiIlAD1KwUyeVArKpb1pc+ny/hu5R67SxIRKREUMEuRsv3QCQZ+voIaYf4M6x2Dw8PYXZKIiIhIsWWM6WCM2WKM2WaMeS6P68YYMyz7+lpjTJNc15KMMeuMMauNMQmFW7mIyPURXrYU3w6IJ7ZqME9/s4YPZ2/Fsiy7yxIRKdYUMEuRcST1DP3HJ+Dl8GB0n1id7isiIiJyDYwxDuAj4HagHtDbGFPvgma3AzWzvx4GRlxwvZ1lWY0ty4q93vWKiBSWwFJejO8Xxx2NKzF0xu+8MHkdGZlZdpclIlJsedpdgAhAemYWAz9fyd7Dp/jyoeZUDvazuyQRERGR4i4O2GZZViKAMeYroCuwMVebrsAEy7l8b4kxpqwxpqJlWfsLv1wRkcLj7enBe70aEx5Uio/mbGf/0TQ+ursJpX0Uk4iIXCmtYBbbWZbFy1PWszgxmX/1iCY2MtjukkRERERKgnBgd677e7Ify28bC5hhjFlhjHn4Um9ijHnYGJNgjEk4dOhQAZQtIlI4jDH83211eLNbNAu2/kmvUYv541ia3WWJiBQ7CpjFdmN+28HEZbsZ1K463WIi7C5HREREpKTI6zCLCzcaddWmlWVZTXBuozHIGHNjXm9iWdYoy7JiLcuKDQsLu/pqRURscnfzKoy+P5btf5yk2/BFbPvjuN0liYgUKwqYxVazNx9kyNRN3N6gAs/cUtvuckRERERKkj1A5Vz3I4B9+W1jWdbZ738Ak3FuuSEiUiK1q1OOrx9pwemMLLoPX8TSxGS7SxIRKTYUMIttNh84xuNfrqJ+pTK807MRHh55LaARERERkau0HKhpjIkyxngDdwE/XNDmB+B+49QCOGpZ1n5jTGljTACAMaY0cCuwvjCLFxEpbA0jyjL50XjCAny4b8wyflhz4c/kREQkLwqYxRZ/njjNg+MS8Pf1ZPT9zfDz1kEKIiIiIgXJsqwM4DFgOrAJ+MayrA3GmAHGmAHZzaYCicA24BPg0ezHywO/GWPWAMuAny3LmlaoH0BExAaVg/2YNDCexpXL8sTEVYyctx3nOagiInIpSvWk0KWlZ/LwhASST57m20fiqRDoa3dJIiIiIiWSZVlTcYbIuR8bmeu2BQzK43mJQKPrXqCISBFU1s+bCQ/G8cy3a3j7l83sPXyKV7vUx6HfuhURyZMCZilUlmXx3KS1rNx1hOH3NCE6ItDukkRERERERETO4+vl4L93xRBRthQfz09k/9FTDOsdo9++FRHJg7bIkEL10ZxtfL96H8/eWouO0RXtLkdEREREREQkTx4ehuc71uX1rvWZvfkPeo9awp8nTttdlohIkaOAWQrNz2v3M3TG73SLCWdQuxp2lyMiIiIiIiJyWfe3jGTkvU3ZcvA43YcvIvHQCbtLEhEpUhQwS6FYu+cIz3y7mqZVg3irezTGaO8qERERERERKR5urV+BiQ+14OTpDLqPWERCUordJYmIFBkKmOW623/0FP3HJxBS2oeP72uKr5fD7pJERERERERErkhMlSC+ezSeID9v7h69lJ/W7rO7JBGRIkEBs1xXqWcy6D8+gZOnMxjTN5ZQfx+7SxIRERERERG5KlVDSjNpYDwNKpXhsS9XMeiLlfxxLM3uskREbKWAWa6brCyLp79ew6b9x/jv3THUqVDG7pJERERERERErklwaW++erglz9xSi5mbDtL+3Xl8vmQnWVmW3aWJiNhCAbNcN0NnbGHahgO80LEuN9Upb3c5IiIiIiIiIgXC29ODx9vXZNqTN9CgUiD/+H49d368mC0HjttdmohIoVPALNfFpBV7GD53O73jqvBg6yi7yxEREREREREpcNXC/PnyoeYMvbMRiYdO0GnYAv49bTNp6Zl2lyYiUmgUMEuBW56UwvPfrSO+egivd62PMcbukkRERERERESuC2MMPZpG8OszbenaOJzhc7dz2/vz+W3rn3aXJiJSKBQwS4HanZLKI5+tIDyoFMPvaYKXQ0NMRERERERESr7g0t6807MRX/Zvjocx3DtmKU99vZo/T5y2uzQRketK6Z8UmONp6Tw4fjmZWRZj+sRS1s/b7pJEREREREREClV8jVB+efIGnripBj+t3cfN787jm+W7sSwdAigiJZMCZikQGZlZPD5xFYmHTjLiniZUC/O3uyQRERERERERW/h6OXj61tpMfeIGapbz52+T1nLXqCVsP3TC7tJERAqcAmYpEEOmbmLulkO83rUB8TVC7S5HRERERERExHY1ywfw9cMteat7NJv2H+P29xfw/qzfOZ2hQwBFpORQwCzX7IulO/l0YRL9WkVxd/MqdpcjIiIiIiIiUmR4eBh6x1Vh1jNtuK1BBd6ftZXbP1jAksRku0sTESkQCpjlmizc9icvT9lAu9phvNiprt3liIiIiIiIiBRJ5QJ8+W/vGMY90Iz0zCzuGrWEv/1vDUdSz9hdmojINVHALFdt+6ETDPx8BTXC/BnWOwaHh7G7JBEREREREZEirW3tcswY3IZH2lRj0sq9tH9nHt+v2qtDAEWk2FLALFflSOoZ+o9PwMvhweg+sQT4etldkoiIiIiIiEixUMrbwfO31+XHx1oTEezH4K9Xc//YZexMPml3aSIiV0wBs1yx9MwsBn6+kr2HT/HxfU2pHOxnd0kiIiIiIiIixU69SmX4bmA8r3etz6pdR7j1vfl8NGcb6ZlZdpcmIpJvCpjliliWxctT1rM4MZl/9YgmNjLY7pJEREREREREii2Hh+H+lpHMeroN7WqX4z/Tt9B52G+s2HnY7tJERPJFAbNckTG/7WDist0MaledbjERdpcjIiIiIiIiUiJUCPRl5H1N+eT+WI6lpdNj5CL+8f06jp5Kt7s0ERGXFDBLvs3efJAhUzdxe4MKPHNLbbvLERERERERESlxbqlXnplPt+GB+Ci+XLqLm9+dx89r9+sQQBEpshQwS75sPnCMx79cRf1KZXinZyM8PIzdJYmIiIiIiIiUSP4+nrz8l3p8P6gV5QJ8GPTlSh4cn8Cew6l2lyYichEFzHJZf544zYPjEvD39WT0/c3w8/a0uyQRERERERGREq9hRFmmDGrFPzrVZfH2ZG55dz6jFySSoUMARaQIUcAsLqWlZ/LwhASST55m9P3NqBDoa3dJIiIiIiIiIm7D0+FB/xuqMfPpG2lZPYR//ryJrh8tZO2eI3aXJiICKGAWFyzL4u+T1rJy1xHe7dmY6IhAu0sSERERERERcUsRQX6M6RPL8HuacOj4ae74aCGv/biBE6cz7C5NRNycAma5pA9nb2PK6n08e2stOkZXtLscEREREREREbdmjKFjdEVmPdOGu5tXYdyiJG55dx4zNx60uzQRcWMKmCVPP6/dzzszf6dbTDiD2tWwuxwRERERERERyVbG14t/3hHN/wbEU8bXi4cmJDDgsxUcOJpmd2ki4oYUMMtF1u45wjPfrqZp1SDe6h6NMcbukkRERERERETkAk2rBvHTE635W4fazNnyBze/O4/xi5LIzLLsLk1E3Iin3QVI0bL/6Cn6j08gpLQPH9/XFF8vh90liYiUXFlZkJHm/Eo/let2GmScyv6eBplnwHiAhyd4OJzfL7yf12PGkX07931P8PC44L7D+Vz9QFFERESk2PFyePBo2xp0iq7IP75fzys/bGDyqr282S2aepXK2F2eiLgBBcySI/VMBv3HJ3DydAaTHo0n1N/H7pJERApPvsLeXKFv7nbppyDjdB7tsh8/r12u1808bfenPt+FobOH4zL3XYXa+WmTj+D7ojZXWsclnp+fNhe9hwJ4ERERKbqqhpRmQr84pqzexxs/beQvH/5G/9ZRPHlzTfy8Ff+IyPWjv2EEgKwsi6e/XsOm/ccY3SeWOhX0U04RsVFeYa+r0DevEPeSoe+Fr5t9LfPM1dfr4QmepcDTB7xKgacvePmee8y/XPZjpXJ993FeP9vOy9d5La92Dm+wsiArE7Iysm9n5LqfmX07+35ej1lnr2VdcD+7zXn3M/J4LI82l6oj4zRknXRRRz7qsrIKbjwVGHNutfg1Bd25Q/irCNwved9FXdVvgtKhdnegiIiIXGfGGO6ICadt7TDemrqZj+cn8vO6/fzzjga0rV3O7vJEpIRSwCwADJ2xhWkbDvCPTnW5qU55u8sRkaIkK6vgV+5eMvQtwLA3d8CbO8T1DbwgCC6VK9j1zX/om7udQ/87LXCWdUGofRVB90Uhdn6C7rzeIz8BfH7ryB3CX8kPAi68n3Fl/dn/VwXMIiIibqSsnzf/6tGQ7k3CeWHyOvp+upy/NKrES53rUi7A1+7yRKSE0b+IhUkr9jB87nZ6x1XhwdZRdpcjIq6cF/Ze68rdy60Ivg5hr5fv+eGsb+AFQXABhL4Ke0sGY7L/LD0BbdmUp6ys/AfwZSrZXa2IiIjYoHm1EKY+eQMj5yby0ZxtzNvyB8/dXpe7mlXGw0Pbf4lIwdC/wt3c8qQUnv9uHfHVQ3i9a32M9pcUKTgZp+HIbji8A47vL5htHa4p7PXKY/VtrnA2d9hbEKGvp6/CXpHrycMD8ACHl92ViIiISBHm4+ngyZtr0rlRRV6cvI4XJq9j8qo9vNktmprlA+wuT0RKAP3L343tTknlkc9WEB5UiuH3NMHL4WF3SSLFi2VBajIcTsr+2gEpSefuH9sLWHk/Nyfs9c07nC1V9gr26L1E6Js7HFbYKyIiIiLi1qqH+TPxoRZ8u2IPb07dRMdhCxjQpjqD2tXA18thd3kiUowpbXBTx9PSeXD8cjKzLMb0iaWsn7fdJYkUTRln4Gj2KuSUHbnC5J3O72eOn98+oCIERULUDc7vQVHO72Uqglfpc6GvhyZwIiIiIiJSuIwx9IytTPs65Rjy8yb+O3sbP63dz5A7GhBfQ+c1iMjVUcDshjIys3h84ioSD51kQr84qoX5212SiH0sC04dzg6PcwfISedWIVtZ59p7+mYHx5EQ2frc7aBICKrqDI9FRERERESKsBB/H97t1ZjuTSJ48ft13D16Kd2bhPOPTvUILq0FaCJyZRQwu6EhUzcxd8sh3uwWrZ9QinvIWYWcdG4ri9wrkU8fO7+9f3nnyuOqrc4PkIOjnNe0V7mIiIiIiJQArWuGMn3wjfx39lY+npfInM1/8GKnevy1SbjOaBKRfFPA7Ga+WLqTTxcm0a9VFHc3r2J3OSIF4+wq5IvC4+yvo3vOX4Xs8DkXGleJPxceB0VC2SrgXbrQP4KIiIiIiIgdfL0c/N9tdejSKJwXJq/j2W/X8N3KPQzpFk1UqP5tJCKXp4DZjSzc9icvT9lAu9phvNiprt3liFyZzPQLViFnf6XsyF6FfPT89qXLOQPjyi2gUdT5K5H9K4CHDrUUERERERE5q3aFAL59pCUTl+/i7V82c9v783m8XQ0eaVMdb0/9+0lELk0Bs5vYfugEAz9fQY0wf4b1jsHhoV91kSIoZxVy0gUH6iVlr0LOPNfW4Q1lqzpXHldpkStAjnLuhaxVyCIiIiIiIlfEw8NwT/Oq3FK3PK/9tJF3Zv7OlDX7eKt7NM0ig+0uT0SKKAXMbuBI6hn6j0/Ay+HB6D6xBPh62V2SuKvMDDi25+Lw+OzWFmkXrkIOy16FHAcNe2aHx5HOr4CKWoUsIiIiIiJyHZQr48tHdzehR5M/+Mf367lz5GJ6x1XmuQ51CfRTpiAi51PAXMKlZ2Yx8POV7D18ii8fak7lYD+7S5KS7tSRvMPjw0lwZPf5q5A9vJyrjYMiIaLZ+dtYBFUFn4DCrl5ERERERESytatTjplP38j7s7Yy5rcdzNx4kJc616NLo0o6BFBEcihgLsEsy+LlKetZnJjMe70aEatfZ5GCkJkBx/Ze+kC9U4fPb+8X4lx5HB4LDXqcC5CDo7JXITsK+QOIiIiIiIhIfvl5e/JCx7p0bVyJ579bx5NfrWbSyr0MuaOBFrGJCKCAuUQb89sOJi7bzaB21ekWE2F3OVKcpB3LOzxO2eE8aC8r41xbDy8oW8UZGldqci48Dop07pHsW8aGDyAiIiIiIiIFqX6lQCY/2ooJi5MYOn0Lt7w3j8E31+LB1lF4ObR9oYg7U8BcQs3efJAhUzdxe4MKPHNLbbvLkaImKzPXKuSkiw/VO5VyfvtSwdkBcgw06H7+VhZlwrUKWURERERExA04PAwPtIqiQ4MKvDJlA2//spnvV+3lre7RxFQJsrs8EbGJAuYSaPOBYzz+5SrqVyrDOz0b4eGhfZHcUtoxOLIz7wP1juyCrPRzbT08IbCyc+VxpTtyBchRzr2QfQNt+AAiIiIiIiJSFFUMLMWo+2OZvuEAr0zZQPcRi7ivRVX+77baBPjqEEARd6OAuYQ5dPw0D45LwN/Xk9H3N8PPW3/EJVZWJhzbl/dheoeTIDX5/PalgpyhccWGUK9LdngceW4VskNjRURERERERPLvtvoViK8ewjszfmf84iSmbzjAa13qc1v9CjoEUMSNKFEqQdLSM3nkswSST57m20fiqRDoa3dJcq1OH4fDO/M+UO/ILsg8c66tcUDZys7AuG6X87exCIqEUmULu3oREREREREp4QJ8vXi1S326xYTz3HfrGPD5Sm6uW57Xu9anUtlSdpcnIoVAAXMJYVkWf5+0lpW7jjD8niZER2hLg2IhKwuO77/0gXqpf57f3jfQufK4fAOo0/lceBwcBWUitApZREREREREbNGocll+fKwVYxfu4L2ZW7n53Xk8c2tt+sZH4tDWnSIlmtKoEuLD2duYsnofz95ai47RFe0uR3I7fcK5F3Jeh+kd2XnxKuTACGdoXKfTufA4ZxWyDk0QERERERGRosnT4cHDN1bn9gYVeWnKet74aWPOIYANwrUQTqSkUsBcAvy8dj/vzPydbjHhDGpXw+5y3E9WFpw4cHF4fPbr5B/nt/cp4wyLy9WFOh3P38YisDI4dCCCiIiIFAxjTAfgA8ABjLYs6+0Lrpvs6x2BVKCvZVkrc113AAnAXsuyOhda4SIiUqxVDvbj077N+Hndfl79YSNdPvyNfq2ieOqWWpT2URQlUtLov+pibu2eIzzz7WqaVg3ire7R2kT/esnMgD9/vzg8PrzDuUdy5ulzbY2Hc7uK4Eio3SFXgBx1bhWy/pxERETkOssOhz8CbgH2AMuNMT9YlrUxV7PbgZrZX82BEdnfz3oS2ASUKZSiRUSkxDDG0LlhJW6oGca/pm1m9G87+GX9AV7vWp/2dcvbXZ6IFCAFzMXY/qOn6D8+gZDSPnx8X1N8vRx2l1TyZGXBxsnw6xvOMPks7wBngBxWG2rddi48PrsK2dPbpoJFREREcsQB2yzLSgQwxnwFdAVyB8xdgQmWZVnAEmNMWWNMRcuy9htjIoBOwBDg6UKuXURESojAUl682S2a7jHhPP/dOh4cn0DH6Aq88pf6lC/ja3d5IlIAFDAXU6lnMug/PoGTpzOY9Gg8of4+dpdU8myfA7Negf1roFx96Docwuo4Q2S/YK1CFhERkaIuHNid6/4ezl+dfKk24cB+4H3gb0CAqzcxxjwMPAxQpUqVaypYRERKrtjIYH5+4gY+WZDIB79uZcHvf/K32+twT1wVPHQIoEix5mF3AXLlsrIsnv56DZv2H+O/d8dQp4J+Y7FA7VsNE+6Az+6A1BTo9jEMWAAx90BEUygdonBZREREioO8JixWftoYYzoDf1iWteJyb2JZ1ijLsmIty4oNCwu7mjpFRMRNeHt6MKhdDWYMvpGGlQN56fv1/HXkIjYfOGZ3aSJyDRQwF0NDZ2xh2oYDvNipHjfV0b5FBSYlEf7XD0a1ca5avu1NeCwBGt0FHtp+RERERIqdPUDlXPcjgH35bNMK6GKMSQK+Am4yxnx+/UoVERF3Ehlams8fbM67PRuxMzmVzsN+41/TNpOWnml3aSJyFRQwFzOTVuxh+Nzt9I6rQr9WkXaXUzKc+AN+fhY+bAZbfoEbnoUnV0PLQeCl/aBERESk2FoO1DTGRBljvIG7gB8uaPMDcL9xagEctSxrv2VZz1uWFWFZVmT282ZblnVvoVYvIiIlmjGG7k0i+PXpNnSLCWfE3O3c+t58Fmw9ZHdpInKFtAdzMbI8KYXnv1tHfPUQXu9aH6NtGq5N2jFY/CEs+hAy0qBpH2jzdwioYHdlIiIiItfMsqwMY8xjwHTAAYy1LGuDMWZA9vWRwFSgI7ANSAUesKteERFxT0GlvfnPnY3o3iSCFyev474xy+jauBIvda6n86ZEignjPDC68MXGxloJCQm2vHdxtDslla4fLSSwlBeTH42nrJ+33SUVXxmnIeFTmP9vSE2GenfATS9BaA27KxMRESkRjDErLMuKtbsOKXya44uIyLVIS89k+NztjJi7DT9vT17oWIeesZW1wE6kCHA1x9cWGcXA8bR0Hhy/nMwsizF9YhUuX62sLFj7rXMrjGl/h3L14KHZ0HO8wmURERERERERm/l6OXj6llr88uQN1C4fwN8nraPXqCVs++OE3aWJiAsKmIu4jMwsHp+4isRDJxlxTxOqhfnbXVLxY1mwbRaMuhG+6w++ZeDeSdDnRwhvand1IiIiIiIiIpJLjXIBfPVwC/7112i2HDjO7R/M592Zv+sQQJEiSnswF3FDpm5i7pZDvNktmvgaoXaXU/zsXQEzX4GkBVC2KnQfDQ3+Ch762YqIiIiIiIhIUeXhYejVrArt65bnjZ82MuzXrfy0Zh9DukXTsnqI3eWJSC5K2YqwL5bu5NOFSfRrFcXdzavYXU7x8uc2+OZ++OQm+GMT3P5veCwBGt6pcFlERERERESkmAj19+GDu2KY0C+OjCyL3p8s4f++XcPhk2fsLk1EsmkFcxG1cNufvDxlA+1qh/Fip7p2l1N8HD8A8/4FK8aDpy+0eQ7iHwOfALsrExEREREREZGrdGOtMKYPvpFhs7fyyfxEft38By91rssdjcN1CKCIzRQwF0HbD51g4OcrqBHmz7DeMTg89BflZaUdhYXDYMlwyDwDsf2gzd/Av5zdlYmIiIiIiIhIASjl7eDvHerQtXElnv9uHU99vYZJK/byzzsaEBla2u7yRNyWAuYi5kjqGfqPT8DL4cHoPrEE+HrZXVLRlnEalo+G+UPhVIpzf+V2L0JIdbsrExEREREREZHroE6FMkwaEM8XS3fy72lbuO39+TzRviYP3VANb09tiylS2BQwFyHpmVkM/Hwlew+f4suHmlM52M/ukoqurExY+w3MeROO7oJq7eDmV6FSY7srExEREREREZHrzMPDcF/LSG6tX4HXftzAf6ZvYcrqvbzVPZqmVYPtLk/ErejHOkWEZVm8PGU9ixOT+VePaGIj9ZdhniwLfp8OI2+A7weAXzDc9z3c/73CZRERERERERE3U76ML8PvacqYPrGcPJ3JX0cs5sXJ6zh6Kt3u0kTchlYwFxFjftvBxGW7GdSuOt1iIuwup2javRxmvQI7F0JQFPQYC/W6gYd+TiIiIiIiIiLiztrXLU+LaiG8O/N3Pl24gxkbD/LKX+rRKbqiDgEUuc6UzBUBszcfZMjUTdzeoALP3FLb7nKKnkO/w1f3wJib4c+t0HEoPLbcud+ywmURERERERERAUr7ePJS53r88FhrKpTx5bEvV9Fv3HJ2p6TaXZpIiaZ0zmabDxzj8S9XUb9SGd7p2QgPD/1ULcexffDD4zC8OSTOdR7e98QqiHsIHDr8UEREREREREQu1iA8kMmPxvNS53os3ZHCre/NZ9T87WRkZtldmkiJpC0ybHTo+GkeHJeAv68no+9vhp+3/jgAOHUYfnsflo50HuYX9wjc+CyUDrW7MhEREREREREpBjwdHjzYOooODSrwypT1vDl1M9+v2sdb3aNpVLms3eWJlChKNG2Slp7JI58lkHzyNN8+Ek+FQF+7S7JfehosGwUL3oG0o9CwJ7R7AYIi7a5MRERERERERIqh8LKl+OT+WKZvOMArP2yg2/CF3N8ykmdvq42/j2IxkYKg/5JsYFkWf5+0lpW7jjD8niZERwTaXZK9sjJhzUSY8yYc2ws1bob2r0DFhnZXJiIiIiIiIiLFnDGGDg0qEl8jlKHTtzB+cRLT1h/gta71ua1+BbvLEyn28rUHszGmgzFmizFmmzHmORftmhljMo0xPQquxJLnw9nbmLJ6H8/eWouO0RXtLsc+lgWbp8KIVjBlEARUgD4/wr2TFC6LiIiIiIiISIEq4+vF610bMGlgPGX9vHjksxU8PCGB/UdP2V2aSLF22YDZGOMAPgJuB+oBvY0x9S7R7l/A9IIusiT5ee1+3pn5O91iwhnUrobd5dhn1xIY2wG+6g1Z6dBzAvT/FaJutLsyERERERERESnBmlQJ4sfHW/Pc7XWYv/UQt7w7n3ELd5CZZdldmkixlJ8VzHHANsuyEi3LOgN8BXTNo93jwCTgjwKsr0RZu+cIz3y7mqZVg3irezTGGLtLKnx/bIKJvWHsbXA4CTq/D48ugXpdwR37Q0REREREREQKnZfDgwFtqjNjcBuaVA3i1R830n3EIjbsO2p3aSLFTn4C5nBgd677e7Ify2GMCQe6ASNdvZAx5mFjTIIxJuHQoUNXWmuxtv/oKfqPTyCktA8f39cUXy+H3SUVrqN74PtBMCIekn6Dm16CJ1ZC7APg8LK7OhERERERERFxQ1VC/Bj/QDM+uKsxew+n0uXDhbw5dROpZzLsLk2k2MjPIX95LSu98HcG3gf+bllWpqtVuZZljQJGAcTGxrrN7x2knsmg//gETp7OYNKj8YT6+9hdUuFJTYHf3oWlowALWjwKNzwDfsF2VyYiIiIiIiIigjGGro3DaVMrjH9N28yo+YlMXbefN+5oQLva5ewuT6TIy0/AvAeonOt+BLDvgjaxwFfZ4XIo0NEYk2FZ1vcFUWRxlpVl8fTXa9i0/xij+8RSp0IZu0sqHGdSYelI+O19OH0MGvWGds9D2Sp2VyYiIiIiIiIicpGyft681b0h3WIieGHyOh74dDmdG1bk5b/Uo1yAr93liRRZ+QmYlwM1jTFRwF7gLuDu3A0sy4o6e9sYMw74SeGy09AZW5i24QAvda7HTXXK213O9ZeZAas/h7lvw/H9UKsDtH8Zyte3uzIRERERERERkcuKiwrm5yda8/G8RD6cs415vx/iudvr0LtZFTw8dH6UyIUuGzBblpVhjHkMmA44gLGWZW0wxgzIvu5y32V3NmnFHobP3U7vuCr0axVpdznXl2XB5p9g1muQvBUi4qDHWKgab3dlIiIiIiIiIiJXxMfTwRPta9K5YUVenLyeFyevZ/LKvbzZPZpa5QPsLk+kSDGWZc9WyLGxsVZCQoIt710YlielcM8nS4mNDGJ8vzi8HPk5T7GYSloIs16BPcshtBa0fwXqdAIX+3GLiIhIyWWMWWFZVqzddUjhK+lzfBERcU+WZTFp5V6G/LyRE6czeOTG6jx2Uw18vRx2lyZSaFzN8fOzRYZcod0pqTzy2QrCg0ox/J4mJTdcPrAefn0Nts6AgErQ5b/Q6G5waFiJiIiIiIiISMlgjKFH0whuqlOOIT9v4sM52/hp7T6GdIumVY1Qu8sTsV0JTT7tczwtnQfHLyczy2JMn1jK+nnbXVLBO7ILJg+Aka1h91K4+TV4YiU0uV/hsoiIiIiIiIiUSMGlvXmnZyO+7N8cgHtGL+Xpr1eTfOK0zZWJ2EtpYAHKyMzi8YmrSDx0kgn94qgW5m93SQXrZDIseAeWfwIYaPUEtH4KSgXZXZmIiIiIiIiISKGIrxHKtME38tGcbYyct505W/7ghY516dE0AqPtQsUNKWAuQEOmbmLulkO82S2a+JL0KxJnTsKS4bBwGJw5AY3vhrbPQ2CE3ZWJiIiIiIiIiBQ6Xy8Hz9xamy6NKvH8d+v4v/+tZdLKPbzZLbrkLTgUuQxtkVFAvli6k08XJtGvVRR3N69idzkFIzMdlo+BYTEw+58QeQMMXAxdP1K4LCIiIiIiIiJur2b5AL55pCVvdY9m475jdHh/AR/M2srpjEy7SxMpNFrBXAAWbvuTl6dsoF3tMF7sVNfucq6dZcHG7+HXNyBlO1RpCT0/gyrN7a5MRERERERERKRI8fAw9I6rQvu65Xjjp028N+t3flizl7e6NyQuKtju8kSuO61gvkbbD51g4OcrqBHmz7DeMTg8ivleOzvmwyc3wbd9weENvb+CB35RuCwiIiIiIiIi4kK5AF/+2zuGcQ8043RGFj0/Xszf/7eWI6ln7C5N5LrSCuZrcCT1DP3HJ+Dl8GB0n1gCfL3sLunq7V8Ls16F7b9CmQi4YwQ07AUeDrsrExEREREREREpNtrWLseMp27kg1+3MnrBDn7dfJCXOtejS6NKOgRQSiStYL5K6ZlZDPx8JXsPn+Lj+5pSOdjP7pKuTsoOmNQfPr4B9q2EW/8Jj69wHuSncFlERERERERE5Ir5eXvy/O11+fGx1oQH+fHkV6u5f+wydiWn2l2aSIHTCuarYFkWL09Zz+LEZN7r1YjYyGK4n87JP2H+f5yH+Hl4QuunodWTUKqs3ZWJiIiIiIiIiJQI9SqV4buB8Xy+ZCf/mb6FW9+fx5Pta9H/hii8HFr3KSWDAuarMOa3HUxctptB7arTLSbC7nKuzOkTsPgjWDQM0k9Bk/ugzXNQpqLdlYmIiIiIiIiIlDgOD0Of+Ehuq1+BV35Yz7+mbebr5btoV6ccLaqF0DwqmLJ+3naXKXLVFDBfoV83HWTI1E3c3qACz9xS2+5y8i/jDKwcD/P+BScPQd0u0P5lCK1pd2UiIiIiIiIiIiVehUBfPr4vlpkbDzJu0Q4mLtvFpwuTMAbqVihDi2ohtKgWTPOoEAL9ivE5X+J2FDBfgU37j/HExFXUr1SGd3o2wsOjGGzMnpUFG76D2f+Ewzugamvo/RVExNpdmYiIiIiIiIiI27mlXnluqVee0xmZrN1zlMXbk1mSmMwXS3cyduEOjIF6Fc8GziHERQUTWEqBsxRdCpjz6dDx0/Qfn4C/ryej72+Gn3cx6Lrts2HWq7B/DZRvAPf8D2rcDDqxVERERERERETEVj6eDppFBtMsMpgn2tfkdEYma3afC5w/W7KTMb85A+f6lcrQIio7cK4WTBlfBc5SdBSDlNR+aemZPPJZAsknT/PtI/FUCPS1uyTX9q1yBsuJcyGwCnQbBdF3goc2jxcRERERERERKYp8PB3ERQUTFxXMk9QkLT2TNbuPsDjRGThPWLKT0b/twMNA/UqBtKgWTMvqIcRGKnAWeylgvgzLsvj7pLWs3HWE4fc0IToi0O6SLi15u3MrjA3fQalg6PA2xPYDTx+7KxMRERERERERkSvg6+WgebUQmlcLAZwLIFftOsKS7MB5/KKdfLLAGTg3CA+kRbUQWlYLITYyiAAFzlKIFDBfxoeztzFl9T6evbUWHaMr2l1O3o4fhPn/hhXjwOENN/4N4h8H3zJ2VyYiIiIiIiIiIgXA18tBy+ohtKx+LnBeueswSxJTWJKYzLiFSYyan4iHgejwQFpUd26p0SwyGH8fRYBy/Wh0ufDz2v28M/N3usWEM6hdDbvLuVjaMVj0X1j8EWSehiZ9oM3fIaC83ZWJiIiIiIiIiMh15OvlIL56KPHVQwE4dSaTVbsOZ69wTmHsbzv4eF4iDg/jDJyrhdCiWjCxCpylgGk0XcLaPUd45tvVNK0axFvdozFF6WC8jNOQ8Klz1XJqMtTvBje9BCHV7a5MRERERERERERsUMrbQXyNUOJrnAucV2YHzou3JzPmt0RGztuOw8PQMOJs4BxCbNUgSitwlmug0ZOH/UdP0X98AiGlffj4vqb4ejnsLskpKwvW/w9mvwFHdkHUjXDzaxDexO7KRERERERERESkCCnl7aBVjVBaZQfOqWcyWLnTuYfz4sRkPpmfyIi52/G8MHCODMLPW5Gh5J9GywVSz2TQf3wCJ09nMOnReEL9i8ABeZYF236FWa/CwXVQoSHc+z5UvwmK0spqEREREREREREpkvy8PWldM5TWNc8Fzit2HmbxduehgaPmJzI8O3BuVLksLaoF07JaKE2rBlHKu4gsvpQiSQFzLllZFk9/vYZN+48xuk8sdSoUgUPy9qyAWa9A0gIIioS/joH63cHDw+7KRERERERERESkmPLz9uSGmmHcUDMMgJOnM0jYeXYP52RGzkvkoznb8XIYGkWUpUU15wGDTaoocJbzKWDOZeiMLUzbcICXOtfjpjo2H5T35zaY/TpsnAJ+oXD7f6BpX/D0trcuEREREREREREpcUr7eNKmVhhtajkD5xOnM0hISmFJYgpLEpMZMW87H87ZhpfD0LhyduBcLYQmVYOKzvayYgsFzNkmrdjD8Lnb6R1XhX6tIu0r5PgBmPs2rJwAXqWg7fPQchD4BNhXk4iIiIiIiIiIuBV/H0/a1i5H29rlAGfgvDwpJXuFcwofzdnGf2dvw9vh4Qycq4fQolowTaoocHY3CpiB5UkpPP/dOuKrh/B61/oYO/Y1TjsKCz+AxcMhKwOa9Ycb/w/8wwq/FhERERERERERkVz8fTxpV7sc7bID5+Np6SQkndtS48PZWxn2K3h7ehCTvcK5RbUQYqqUVeBcwrl9wLw7JZVHPltBeFApht/TBC9HIe9tnJ4Gy0fDgqFw6jBE3wntXoTgqMKtQ0REREREREREJJ8CfL1oV6cc7eo4A+djaek5W2os3p7Mf2dv5YNft+Lt6UGTKucHzj6eCpxLErcOmI+npfPg+OVkZlmM6RNLWb9C3N84KxPWfg1z3oSju6H6TXDzq1CxUeHVICIiIiIiIiIiUgDK+HpxU53yOeeaHT11NnBOZnFiMh/8upX3Z23Fx9ODJlWCsgPnYBorcC723DZgzsjM4vGJq0g8dJIJ/eKoFuZfOG9sWbB1Bsx6Ff7YCJVioOuHUK1t4by/iIiIiIiIiIjIdRZYyov2dcvTvu65wHn5jhQWZ2+p8f6vv2PNAh9PD5pWdQbOLauH0DAiUIFzMeO2AfOQqZuYu+UQb3aLJr5GaOG86e5lMPMV2LUIgqvBneOg3h1gx57PIiIiIiIiIiIihSSwlBc31yvPzfWyA+fUdJYlObfTWJKYzHuzfufdmeDrlR04R50NnMvi7VnIW9rKFXHLgPmLpTv5dGES/VpFcXfzKtf/DQ9tgV9fh80/Qely0OkdaNIHHF7X/71FRERExG0ZYzoAHwAOYLRlWW9fcN1kX+8IpAJ9LctaaYzxBeYDPjj/zfA/y7JeKdTiRUREpEQL9PPilnrluSU7cD6SeoalO1KyDw1M4Z2Zv0N24BxbNZgW1YJpWT2E6HAFzkWN2wXMC7f9yctTNtCudhgvdqp7fd/s6F6Y+xas/gK8SkO7f0CLgeBTSNtxiIiIiIjbMsY4gI+AW4A9wHJjzA+WZW3M1ex2oGb2V3NgRPb308BNlmWdMMZ4Ab8ZY36xLGtJoX4IERERcRtl/by5rX4FbqtfAYDDJ3MHzskMnfE7AKW8HMRGBuUcGtgwIhAvhwJnO7lVwLz90AkGfr6CGmH+DOsdg8PjOm1Nceow/PY+LB0JVhY0HwA3PAulQ67P+4mIiIiIXCwO2GZZViKAMeYroCuQO2DuCkywLMsClhhjyhpjKlqWtR84kd3GK/vLKrzSRURExN0FlfamQ4MKdGjgDJxTTp5h2Q7n6uYlicn8Z/oWAPy8HTStGkTL6s7AOTpcgXNhc5uA+UjqGfqPT8DL4cHoPrEE+F6H7SnST8GyUbDgXUg7Cg17QbsXIKhqwb+XiIiIiIhr4cDuXPf34FydfLk24cD+7BXQK4AawEeWZS3N602MMQ8DDwNUqVII28+JiIiIWwou7U2HBhXp0KAiAMknTrMs15Ya/57mDJxLezuIjQzOXuEcTHR4IJ4KnK8rtwiYLcvisS9XsffwKb58qDmVg/0K9g0yM2DNROd2GMf2Qs1bof0rUKFBwb6PiIiIiEj+5fXreheuQr5kG8uyMoHGxpiywGRjTAPLstZf1NiyRgGjAGJjY7XKWURERApFiL8Pt0dX5PZoZ+D8Z67AefH2ZP41bTPgDJybRQXnbKnRoFIZBc4FzC0CZmMM/VpHcmdsBLGRwQX3wpYFW6Y6D/A7tBnCY6H7KIhsXXDvISIiUkSkp6ezZ88e0tLS7C5FighfX18iIiLw8tLBxUXUHqByrvsRwL4rbWNZ1hFjzFygA3BRwCwiIiJSFIT6+9AxuiIdswPnQ8edgfPixD9ZkpjC2784A2d/H0+aZe/h3LJ6CPUqKnC+Vm4RMAPcVKd8wb7gzsUw6xXYvRRCakDPz6DuX8Bcp32dRUREbLZnzx4CAgKIjIzE6P93bs+yLJKTk9mzZw9RUVF2lyN5Ww7UNMZEAXuBu4C7L2jzA/BY9v7MzYGjlmXtN8aEAenZ4XIp4GbgX4VYu4iIiMg1CQvwoVPDinRqeC5wXrrDubp5SWIyc7YcAiDAxzN7hXMwLauFUq9Smet3blsJ5TYBc4E5uNG5Yvn3X8C/AvzlA2h8LzjUlSIiUrKlpaUpXJYcxhhCQkI4dOiQ3aXIJViWlWGMeQyYDjiAsZZlbTDGDMi+PhKYCnQEtgGpwAPZT68IjM/eh9kD+MayrJ8K+zOIiIiIFJSwAB86N6xE54aVAPjjeFrOgYFLEpOZvfkPwBk4x0UF5xwaWLeiAufLUSqaX0d2O/dYXjMRvAOceyw3HwDeBbyfs4iISBGmcFly03go+izLmoozRM792Mhcty1gUB7PWwvEXPcCRURERGxSLsCXLo0q0aVRduB8LI3F2QcGLk1M5tezgbOvJ81z7eGswPliCpgvJzUFfnsXlo5y3m85CFo/DX4FuJeziIiIiIiIiIiI2KZcGV+6Ng6na+NwAA4cTWPpjuTsFc4pzNrkDJzL+HoSFxWSvcI5mLoVyuDh5oGzAuZLOZMKS0fCb+/DmePQqDe0fR7KVr7sU0VERKTgJScn0759ewAOHDiAw+EgLCwMgGXLluHt7X3J5yYkJDBhwgSGDRvm8j3i4+NZtGhRgdX85JNP8r///Y/du3fj4aGDQ0REREREiosKgecHzvuPnmJpri01Zm06CEBgKa/zVjjXqRDgdoGzAuYLZWbA6s9h7ttwfD/Uuh3avwzl69ldmYiIiFsLCQlh9erVALz66qv4+/vz7LPP5lzPyMjA0zPvqU1sbCyxsbGXfY+CDJezsrKYPHkylStXZv78+bRt27bAXju3zMxMHA7HdXltERERERFxqhhYijtiwrkjxhk47ztyyrnCeXsKixOTmbHRGTiX9Ts/cK5dvuQHzgqYz7Is2PSj8wC/5K1QuTn0+BSqtrS7MhERkSLntR83sHHfsQJ9zXqVyvDKX+pf0XP69u1LcHAwq1atokmTJvTq1YvBgwdz6tQpSpUqxaeffkrt2rWZO3cuQ4cO5aeffuLVV19l165dJCYmsmvXLgYPHswTTzwBgL+/PydOnGDu3Lm8+uqrhIaGsn79epo2bcrnn3+OMYapU6fy9NNPExoaSpMmTUhMTOSnny4++2zOnDk0aNCAXr16MXHixJyA+eDBgwwYMIDExEQARowYQXx8PBMmTGDo0KEYY2jYsCGfffYZffv2pXPnzvTo0eOi+l577TUqVqzI6tWr2bhxI3fccQe7d+8mLS2NJ598kocffhiAadOm8cILL5CZmUloaCgzZ86kdu3aLFq0iLCwMLKysqhVqxZLliwhNDT0av/4RERERETcSqWypegWE0G3mAgA9h45xdLs1c2LE5OZvsEZOAf5edE8yrmdRsvqodQs51/iAmcFzABJv8HMV2BvAoTVgbu+hNodQQfXiIiIFHm///47s2bNwuFwcOzYMebPn4+npyezZs3ihRdeYNKkSRc9Z/PmzcyZM4fjx49Tu3ZtBg4ciJeX13ltVq1axYYNG6hUqRKtWrVi4cKFxMbG8sgjjzB//nyioqLo3bv3JeuaOHEivXv3pmvXrrzwwgukp6fj5eXFE088QZs2bZg8eTKZmZmcOHGCDRs2MGTIEBYuXEhoaCgpKSmX/dzLli1j/fr1REVFATB27FiCg4M5deoUzZo1469//StZWVk89NBDOfWmpKTg4eHBvffeyxdffMHgwYOZNWsWjRo1UrgsIiIiInINwsuWonuTCLo3cQbOew6nsjQxJfvgwGSmbTgAQHBp75wVzi2rh1CznH+xPzzbvQPmA+vh19dg6wwoEw5dPnTutexw724RERG5nCtdaXw93XnnnTlbRBw9epQ+ffqwdetWjDGkp6fn+ZxOnTrh4+ODj48P5cqV4+DBg0RERJzXJi4uLuexxo0bk5SUhL+/P9WqVcsJdXv37s2oUaMuev0zZ84wdepU3nvvPQICAmjevDkzZsygU6dOzJ49mwkTJgDgcDgIDAxkwoQJ9OjRIyfkDQ6+/GHCcXFxOXUADBs2jMmTJwOwe/dutm7dyqFDh7jxxhtz2p193X79+tG1a1cGDx7M2LFjeeCBBy77fiIiIiIikn8RQX5ENPXjr02d/6bYnZKac2DgksRkflnvDJxDSnvTvFowLbO31KhRDANn90xSD++EOW/C2q/Btwzc8jrEPQxepeyuTERERK5Q6dKlc26/9NJLtGvXjsmTJ5OUlHTJfY99fHxybjscDjIyMvLVxrKsfNU0bdo0jh49SnR0NACpqan4+fnRqVOnPNtblpXnJNLT05OsrKycNmfOnMm5lvtzz507l1mzZrF48WL8/Pxo27YtaWlpl3zdypUrU758eWbPns3SpUv54osv8vW5RERERETk6lQO9qNysB93xlYGnIHz2dXNS7YnM3WdM3AO9fd2bqlRPYSW1YKpHlb0A2f3CphPJsOCobB8NBgPaPUktB4MpYLsrkxEREQKwNGjRwkPdx66MW7cuAJ//Tp16pCYmEhSUhKRkZF8/fXXebabOHEio0ePztlC4+TJk0RFRZGamkr79u0ZMWIEgwcPJjMzk5MnT9K+fXu6devGU089RUhICCkpKQQHBxMZGcmKFSvo2bMnU6ZMueSK7KNHjxIUFISfnx+bN29myZIlALRs2ZJBgwaxY8eOnC0yzq5i7t+/P/feey/33XefDgkUERERESlkZwPnnrGVsSyL3Smnslc4O/dw/nndfgBC/X1oUe3coYHVw0oXucDZfQLmBe86v9JPQsy90OY5CAy3uyoREREpQH/729/o06cP7777LjfddFOBv36pUqUYPnw4HTp0IDQ0lLi4uIvapKamMn36dD7++OOcx0qXLk3r1q358ccf+eCDD3j44YcZM2YMDoeDESNG0LJlS1588UXatGmDw+EgJiaGcePG8dBDD9G1a1fi4uJo3779eauWc+vQoQMjR46kYcOG1K5dmxYtWgAQFhbGqFGj6N69O1lZWZQrV46ZM2cC0KVLFx544AFtjyEiIiIiYjNjDFVC/KgS4kfPZs7AeVeuLTUWb0/mp7XOwDkswIcW1UJ47vY6hJctGrsxmPz+qmdBi42NtRISEgrvDac8BqcOQ/uXIax24b2viIhICbFp0ybq1q1rdxm2O3HiBP7+/liWxaBBg6hZsyZPPfWU3WVdsYSEBJ566ikWLFhwTa+T17gwxqywLCv2ml5YiqVCn+OLiIiIuAHLstiZnJqzwnnZjhR+GXwjgaW8Lv/kAuJqju8+K5g7v6/D+0REROSaffLJJ4wfP54zZ84QExPDI488YndJV+ztt99mxIgR2ntZRERERKQYMMYQGVqayNDS3BVXxe5yLuI+K5hFRETkmmgFs+RFK5glN83xRUREREomV3N8j8IuRkRERERERERERERKBgXMIiIiIiIiIiIiInJVFDCLiIiIiIiIiIiIyFVRwCwiIiIiIiIiIiIiV0UBs4iIiBQLbdu2Zfr06ec99v777/Poo4+6fM7ZA8c6duzIkSNHLmrz6quvMnToUJfv/f3337Nx48ac+y+//DKzZs26gupde/LJJwkPDycrK6vAXlNERERERKQwKGAWERGRYqF379589dVX5z321Vdf0bt373w9f+rUqZQtW/aq3vvCgPn111/n5ptvvqrXulBWVhaTJ0+mcuXKzJ8/v0BeMy+ZmZnX7bVFRERERMR9edpdgIiIiBRDvzwHB9YV7GtWiIbb377k5R49evCPf/yD06dP4+PjQ1JSEvv27aN169YMHDiQ5cuXc+rUKXr06MFrr7120fMjIyNJSEggNDSUIUOGMGHCBCpXrkxYWBhNmzYF4JNPPmHUqFGcOXOGGjVq8Nlnn7F69Wp++OEH5s2bxz//+U8mTZrEG2+8QefOnenRowe//vorzz77LBkZGTRr1owRI0bg4+NDZGQkffr04ccffyQ9PZ1vv/2WOnXqXFTXnDlzaNCgAb169WLixIm0bdsWgIMHDzJgwAASExMBGDFiBPHx8UyYMIGhQ4dijKFhw4Z89tln9O3bN6ceAH9/f06cOMHcuXN57bXXqFixIqtXr2bjxo3ccccd7N69m7S0NJ588kkefvhhAKZNm8YLL7xAZmYmoaGhzJw5k9q1a7No0SLCwsLIysqiVq1aLFmyhNDQ0Gv6oxYRERERkZJDK5hFRESkWAgJCSEuLo5p06YBztXLvXr1whjDkCFDSEhIYO3atcybN4+1a9de8nVWrFjBV199xapVq/juu+9Yvnx5zrXu3buzfPly1qxZQ926dRkzZgzx8fF06dKF//znP6xevZrq1avntE9LS6Nv3758/fXXrFu3joyMDEaMGJFzPTQ0lJUrVzJw4MBLbsMxceJEevfuTbdu3fjpp59IT08H4IknnqBNmzasWbOGlStXUr9+fTZs2MCQIUOYPXs2a9as4YMPPrhsvy1btowhQ4bkrMAeO3YsK1asICEhgWHDhpGcnMyhQ4d46KGHmDRpEmvWrOHbb7/Fw8ODe++9ly+++AKAWbNm0ahRI4XLIiIiIiJyHq1gFhERkSvnYqXx9XR2m4yuXbvy1VdfMXbsWAC++eYbRo0aRUZGBvv372fjxo00bNgwz9dYsGAB3bp1w8/PD4AuXbrkXFu/fj3/+Mc/OHLkCCdOnOC2225zWc+WLVuIioqiVq1aAPTp04ePPvqIwYMHA87AGqBp06Z89913Fz3/zJkzTJ06lffee4+AgACaN2/OjBkz6NSpE7Nnz2bChAkAOBwOAgMDmTBhAj169MgJeYODgy/bZ3FxcURFReXcHzZsGJMnTwZg9+7dbN26lUOHDnHjjTfmtDv7uv369aNr164MHjyYsWPH8sADD1z2/URERERExL0oYBYREZFi44477uDpp59m5cqVnDp1iiZNmrBjxw6GDh3K8uXLCQoKom/fvqSlpbl8HWNMno/37duX77//nkaNGjFu3Djmzp3r8nUsy3J53cfHB3AGxBkZGRddnzZtGkePHiU6OhqA1NRU/Pz86NSp0yXfL6/aPT09cw4ItCyLM2fO5FwrXbp0zu25c+cya9YsFi9ejJ+fH23btiUtLe2Sr1u5cmXKly/P7NmzWbp0ac5qZhERERERkbO0RYaIiIgUG/7+/rRt25Z+/frlHO537NgxSpcuTWBgIAcPHuSXX35x+Ro33ngjkydP5tSpUxw/fpwff/wx59rx48epWLEi6enp54WpAQEBHD9+/KLXqlOnDklJSWzbtg2Azz77jDZt2uT780ycOJHRo0eTlJREUlISO3bsYMaMGaSmptK+ffuc7TYyMzM5duwY7du355tvviE5ORmAlJQUwLm/9IoVKwCYMmVKzjYbFzp69ChBQUH4+fmxefNmlixZAkDLli2ZN28eO3bsOO91Afr378+9995Lz549cTgc+f5sIiIiIiLiHhQwi4iISLHSu3dv1qxZw1133QVAo0aNiImJoX79+vTr149WrVq5fH6TJk3o1asXjRs35q9//Ss33HBDzrU33niD5s2bc8stt5x3IN9dd93Ff/7zH2JiYti+fXvO476+vnz66afceeedREdH4+HhwYABA/L1OVJTU5k+ffp5q5VLly5N69at+fHHH/nggw+YM2cO0dHRNG3alA0bNlC/fn1efPFF2rRpQ6NGjXj66acBeOihh5g3bx5xcXEsXbr0vFXLuXXo0IGMjAwaNmzISy+9RIsWLQAICwtj1KhRdO/enUaNGtGrV6+c53Tp0oUTJ05oewwREREREcmTudyvdl4vsbGxVkJCgi3vLSIiIldu06ZN1K1b1+4ypJAlJCTw1FNPsWDBgjyv5zUujDErLMuKLYz6pGjRHF9ERESkZHI1x9cezCIiIiKSp7fffpsRI0Zo72UREREREbkkbZEhIiIiInl67rnn2LlzJ61bt7a7FBERERERKaIUMIuIiEi+2bW1lhRNGg8iIiIiIqKAWURERPLF19eX5ORkhYoCOMPl5ORkfH197S5FRERERERspD2YRUREJF8iIiLYs2cPhw4dsrsUKSJ8fX2JiIiwuwwREREREbGRAmYRERHJFy8vL6KiouwuQ0RERERERIoQbZEhIiIiIiIiIiIiIldFAbOIiIiIiIiIiIiIXBUFzCIiIiIiIiIiIiJyVYxdJ8EbYw4BOwv5bUOBPwv5PYsT9Y9r6h/X1D+uqX9cU/+4pv5xTf3jmh39U9WyrLBCfk8pAjTHL5LUP66pf1xT/7im/nFN/eOa+sc19Y9rRWqOb1vAbAdjTIJlWbF211FUqX9cU/+4pv5xTf3jmvrHNfWPa+of19Q/UtJpjLum/nFN/eOa+sc19Y9r6h/X1D+uqX9cK2r9oy0yREREREREREREROSqKGAWERERERERERERkavibgHzKLsLKOLUP66pf1xT/7im/nFN/eOa+sc19Y9r6h8p6TTGXVP/uKb+cU3945r6xzX1j2vqH9fUP64Vqf5xqz2YRURERERERERERKTguNsKZhEREREREREREREpIAqYRUREREREREREROSqlMiA2RjTwRizxRizzRjzXB7XjTFmWPb1tcaYJnbUaZd89E9bY8xRY8zq7K+X7ajTDsaYscaYP4wx6y9x3d3HzuX6x23HDoAxprIxZo4xZpMxZoMx5sk82rjtGMpn/7jtGDLG+Bpjlhlj1mT3z2t5tHHn8ZOf/nHb8XOWMcZhjFlljPkpj2tuO36kZNAc3zXN8S9Nc3zXNMd3TXN81zTHd01zfNc0x8+f4jDH97TjTa8nY4wD+Ai4BdgDLDfG/GBZ1sZczW4HamZ/NQdGZH8v8fLZPwALLMvqXOgF2m8c8CEw4RLX3XbsZBuH6/4B9x07ABnAM5ZlrTTGBAArjDEz9fdPjvz0D7jvGDoN3GRZ1gljjBfwmzHmF8uyluRq487jJz/9A+47fs56EtgElMnjmjuPHynmNMd3TXP8yxqH5viujENzfFc0x3dNc3zXNMd3TXP8/Cnyc/ySuII5DthmWVaiZVlngK+Arhe06QpMsJyWAGWNMRULu1Cb5Kd/3JZlWfOBFBdN3Hns5Kd/3JplWfsty1qZffs4zv8BhF/QzG3HUD77x21lj4kT2Xe9sr8uPInXncdPfvrHrRljIoBOwOhLNHHb8SMlgub4rmmO74Lm+K5pju+a5viuaY7vmub4rmmOf3nFZY5fEgPmcGB3rvt7uPgvt/y0Kany+9lbZv+Kwi/GmPqFU1qx4M5jJ780dgBjTCQQAyy94JLGEC77B9x4DGX/6tNq4A9gpmVZGj+55KN/wI3HD/A+8Dcg6xLX3Xr8SLGnOb5rmuNfG3ceO/mlsYPm+JejOX7eNMd3TXP8y3qfYjDHL4kBs8njsQt/+pGfNiVVfj77SqCqZVmNgP8C31/voooRdx47+aGxAxhj/IFJwGDLso5deDmPp7jVGLpM/7j1GLIsK9OyrMZABBBnjGlwQRO3Hj/56B+3HT/GmM7AH5ZlrXDVLI/H3Gb8SLGnOb5rmuNfG3ceO/mhsYPm+JejOf6laY7vmub4l1ac5vglMWDeA1TOdT8C2HcVbUqqy352y7KOnf0VBcuypgJexpjQwiuxSHPnsXNZGjuQvW/UJOALy7K+y6OJW4+hy/WPxpCTZVlHgLlAhwsuufX4OetS/ePm46cV0MUYk4TzV+NvMsZ8fkEbjR8pzjTHd01z/GvjzmPnsjR2NMe/HM3x80dzfNc0x89TsZnjl8SAeTlQ0xgTZYzxBu4CfrigzQ/A/dknLbYAjlqWtb+wC7XJZfvHGFPBGGOyb8fhHCfJhV5p0eTOY+ey3H3sZH/2McAmy7LevUQztx1D+ekfdx5DxpgwY0zZ7NulgJuBzRc0c+fxc9n+cefxY1nW85ZlRViWFYnz/+2zLcu694Jmbjt+pETQHN81zfGvjTuPncty97GjOb5rmuO7pjm+a5rju1ac5viehf2G15tlWRnGmMeA6YADGGtZ1gZjzIDs6yOBqUBHYBuQCjxgV72FLZ/90wMYaIzJAE4Bd1mW5Ra/nmGMmQi0BUKNMXuAV3BuMu/2Ywfy1T9uO3aytQLuA9YZ5x5SAC8AVUBjiPz1jzuPoYrAeGOMA+ek6RvLsn7S/79y5Kd/3Hn85EnjR0oKzfFd0xzfNc3xXdMc/7I0x3dNc3zXNMd3TXP8q1AUx49x8z8TEREREREREREREblKJXGLDBEREREREREREREpBAqYRUREREREREREROSqKGAWERERERERERERkauigFlERERERERERERErooCZhERERERERERERG5KgqYRUREREREREREROSqKGAWERERERERERERkavy/wS52baxERtfAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1800x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_model_performance(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6243aff1",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c5456e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to evaluate the model on the test set\n",
    "def evaluate_model(test_dataset):\n",
    "    test_res = trainer.evaluate(eval_dataset=test_dataset)\n",
    "    print(test_res)\n",
    "         \n",
    "    return trainer.predict(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f7bb617c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09969250857830048, 'eval_f1': 0.5540516473731077, 'eval_recall': 0.4915468478432612, 'eval_precision': 0.6347684146092634, 'eval_roc_auc': 0.739627585581213, 'eval_accuracy': 0.44094343099318223, 'eval_runtime': 7.3912, 'eval_samples_per_second': 734.255, 'eval_steps_per_second': 46.001, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "y_test = evaluate_model(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c172305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate each emotion label metrics on test set\n",
    "def calc_label_metrics(label, y_targets, y_preds, threshold):\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"accuracy\": metrics.accuracy_score(y_targets, y_preds),\n",
    "        \"precision\": metrics.precision_score(y_targets, y_preds, zero_division=0),\n",
    "        \"recall\": metrics.recall_score(y_targets, y_preds, zero_division=0),\n",
    "        \"f1\": metrics.f1_score(y_targets, y_preds, zero_division=0),\n",
    "        \"mcc\": metrics.matthews_corrcoef(y_targets, y_preds),\n",
    "        \"support\": y_targets.sum(),\n",
    "        \"threshold\": threshold,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "78c37071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate overall metric on test set\n",
    "def calc_test_metrics(y_test, test_dataset, target_cols):\n",
    "    threshold = 0.5\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(y_test.predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= 0.5)] = 1\n",
    "    \n",
    "    # finally, compute metrics\n",
    "    y_true = df_test[target_cols].values\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'recall' : recall_micro,\n",
    "               'precision': precision_micro,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    metrics_df = pd.DataFrame.from_dict(metrics, orient='index', columns=['Value'])\n",
    "\n",
    "    display(metrics_df)\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for label_index, label in enumerate(target_cols):\n",
    "        y_targets, y_preds = y_true[:, label_index], y_pred[:, label_index]\n",
    "        results.append(calc_label_metrics(label, y_targets, y_preds, threshold))\n",
    "\n",
    "    per_label_results = pd.DataFrame(results, index=target_cols)\n",
    "    display(per_label_results.drop(columns=[\"label\"]).round(3))\n",
    "    \n",
    "    return y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "794fec4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.554052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.491547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.634768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc</th>\n",
       "      <td>0.739628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.440943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "f1         0.554052\n",
       "recall     0.491547\n",
       "precision  0.634768\n",
       "roc_auc    0.739628\n",
       "accuracy   0.440943"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>mcc</th>\n",
       "      <th>support</th>\n",
       "      <th>threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.935</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.612</td>\n",
       "      <td>504</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.980</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.798</td>\n",
       "      <td>264</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.966</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.438</td>\n",
       "      <td>198</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.938</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.281</td>\n",
       "      <td>320</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.938</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.369</td>\n",
       "      <td>351</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.973</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.312</td>\n",
       "      <td>135</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.971</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.398</td>\n",
       "      <td>153</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.946</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.362</td>\n",
       "      <td>284</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.986</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.423</td>\n",
       "      <td>83</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.968</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.221</td>\n",
       "      <td>151</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.948</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.343</td>\n",
       "      <td>267</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.981</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.495</td>\n",
       "      <td>123</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.994</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.476</td>\n",
       "      <td>37</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.981</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.379</td>\n",
       "      <td>103</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.989</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.659</td>\n",
       "      <td>78</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.988</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.900</td>\n",
       "      <td>352</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.999</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.974</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.524</td>\n",
       "      <td>161</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.979</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.740</td>\n",
       "      <td>238</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.995</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.299</td>\n",
       "      <td>23</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.974</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.557</td>\n",
       "      <td>186</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.998</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.446</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.969</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.255</td>\n",
       "      <td>145</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.998</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.992</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.626</td>\n",
       "      <td>56</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.978</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.574</td>\n",
       "      <td>156</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.978</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.500</td>\n",
       "      <td>141</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.760</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.437</td>\n",
       "      <td>1787</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                accuracy  precision  recall     f1    mcc  support  threshold\n",
       "admiration         0.935      0.652   0.643  0.647  0.612      504        0.5\n",
       "amusement          0.980      0.759   0.860  0.806  0.798      264        0.5\n",
       "anger              0.966      0.552   0.374  0.446  0.438      198        0.5\n",
       "annoyance          0.938      0.450   0.212  0.289  0.281      320        0.5\n",
       "approval           0.938      0.536   0.296  0.382  0.369      351        0.5\n",
       "caring             0.973      0.420   0.252  0.315  0.312      135        0.5\n",
       "confusion          0.971      0.474   0.359  0.409  0.398      153        0.5\n",
       "curiosity          0.946      0.476   0.317  0.381  0.362      284        0.5\n",
       "desire             0.986      0.610   0.301  0.403  0.423       83        0.5\n",
       "disappointment     0.968      0.348   0.159  0.218  0.221      151        0.5\n",
       "disapproval        0.948      0.455   0.300  0.361  0.343      267        0.5\n",
       "disgust            0.981      0.625   0.407  0.493  0.495      123        0.5\n",
       "embarrassment      0.994      0.706   0.324  0.444  0.476       37        0.5\n",
       "excitement         0.981      0.485   0.311  0.379  0.379      103        0.5\n",
       "fear               0.989      0.615   0.718  0.663  0.659       78        0.5\n",
       "gratitude          0.988      0.944   0.869  0.905  0.900      352        0.5\n",
       "grief              0.999      0.000   0.000  0.000  0.000        6        0.5\n",
       "joy                0.974      0.561   0.516  0.537  0.524      161        0.5\n",
       "love               0.979      0.803   0.702  0.749  0.740      238        0.5\n",
       "nervousness        0.995      0.417   0.217  0.286  0.299       23        0.5\n",
       "optimism           0.974      0.672   0.484  0.563  0.557      186        0.5\n",
       "pride              0.998      0.800   0.250  0.381  0.446       16        0.5\n",
       "realization        0.969      0.362   0.200  0.258  0.255      145        0.5\n",
       "relief             0.998      0.000   0.000  0.000  0.000       11        0.5\n",
       "remorse            0.992      0.585   0.679  0.628  0.626       56        0.5\n",
       "sadness            0.978      0.651   0.526  0.582  0.574      156        0.5\n",
       "surprise           0.978      0.604   0.433  0.504  0.500      141        0.5\n",
       "neutral            0.760      0.662   0.555  0.604  0.437     1787        0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "targets, outputs = calc_test_metrics(y_test, test_dataset, target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2f1631d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results DataFrame:\n",
      "         Actual                Predicted\n",
      "0     [sadness]                   [love]\n",
      "1  [admiration]             [admiration]\n",
      "2  [excitement]               [optimism]\n",
      "3   [gratitude]              [gratitude]\n",
      "4     [neutral]                [neutral]\n",
      "5   [gratitude]              [gratitude]\n",
      "6   [gratitude]              [gratitude]\n",
      "7   [gratitude]  [admiration, gratitude]\n",
      "8     [remorse]                [sadness]\n",
      "9     [sadness]                [sadness]\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to store actual labels and predicted labels\n",
    "final_df = pd.DataFrame({\n",
    "    'Actual': [list(np.where(targets[i])[0]) for i in range(len(targets))],\n",
    "    'Predicted': [list(np.where(outputs[i])[0]) for i in range(len(outputs))]\n",
    "})\n",
    "\n",
    "# Map label indices to label names in the 'Actual' column\n",
    "final_df['Actual'] = final_df['Actual'].apply(lambda indices: [target_cols[idx] for idx in indices])\n",
    "\n",
    "# Map label indices to label names in the 'Predicted' column\n",
    "final_df['Predicted'] = final_df['Predicted'].apply(lambda indices: [target_cols[idx] for idx in indices])\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(\"Results DataFrame:\")\n",
    "print(final_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "301ef201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the label DataFrame with the original DataFramev\n",
    "val_df_terms = df_test['clean_text']\n",
    "result_df = pd.concat([val_df_terms, final_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a335fd5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am really sorry about your situation frown s...</td>\n",
       "      <td>[sadness]</td>\n",
       "      <td>[love]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it is wonderful because it is awful at not with</td>\n",
       "      <td>[admiration]</td>\n",
       "      <td>[admiration]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kings fan here good luck to you guys will be a...</td>\n",
       "      <td>[excitement]</td>\n",
       "      <td>[optimism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i did not know that thank you for teaching me ...</td>\n",
       "      <td>[gratitude]</td>\n",
       "      <td>[gratitude]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>they got bored from haunting earth for thousan...</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5422</th>\n",
       "      <td>thanks i was diagnosed with bp 1 after the hos...</td>\n",
       "      <td>[gratitude]</td>\n",
       "      <td>[gratitude]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5423</th>\n",
       "      <td>well that makes sense</td>\n",
       "      <td>[approval]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5424</th>\n",
       "      <td>daddy issues name</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5425</th>\n",
       "      <td>so glad i discovered that subreddit a couple m...</td>\n",
       "      <td>[admiration]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5426</th>\n",
       "      <td>had to watch elmo in grouchland one time too m...</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5427 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_text        Actual  \\\n",
       "0     i am really sorry about your situation frown s...     [sadness]   \n",
       "1       it is wonderful because it is awful at not with  [admiration]   \n",
       "2     kings fan here good luck to you guys will be a...  [excitement]   \n",
       "3     i did not know that thank you for teaching me ...   [gratitude]   \n",
       "4     they got bored from haunting earth for thousan...     [neutral]   \n",
       "...                                                 ...           ...   \n",
       "5422  thanks i was diagnosed with bp 1 after the hos...   [gratitude]   \n",
       "5423                              well that makes sense    [approval]   \n",
       "5424                                  daddy issues name     [neutral]   \n",
       "5425  so glad i discovered that subreddit a couple m...  [admiration]   \n",
       "5426  had to watch elmo in grouchland one time too m...     [neutral]   \n",
       "\n",
       "         Predicted  \n",
       "0           [love]  \n",
       "1     [admiration]  \n",
       "2       [optimism]  \n",
       "3      [gratitude]  \n",
       "4        [neutral]  \n",
       "...            ...  \n",
       "5422   [gratitude]  \n",
       "5423     [neutral]  \n",
       "5424     [neutral]  \n",
       "5425            []  \n",
       "5426     [neutral]  \n",
       "\n",
       "[5427 rows x 3 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d681bb",
   "metadata": {},
   "source": [
    "## 4. Save the output, tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5113b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('output_distilbert_m5.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b8b778c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer\n",
    "tokenizer.save_pretrained('./distilbert_M5_transformer/')\n",
    "\n",
    "# Save model\n",
    "trainer.save_model('./distilbert_M5_transformer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba9b95b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
