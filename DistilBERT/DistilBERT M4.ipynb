{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25beadb5",
   "metadata": {},
   "source": [
    "# DistilBERT M4 (with BERT Augmented dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a1275c",
   "metadata": {},
   "source": [
    "Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019c1d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn ,cuda\n",
    "from torch.utils.data import DataLoader,Dataset,RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "import nltk.corpus\n",
    "from sklearn import metrics\n",
    "from scipy.special import softmax\n",
    "\n",
    "import transformers\n",
    "from transformers import  AutoTokenizer, AutoModel\n",
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer,TrainerCallback, EarlyStoppingCallback\n",
    "import glob\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from transformers import EvalPrediction   \n",
    "import copy\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from typing import Optional\n",
    "from torch import FloatTensor\n",
    "from torch.nn import BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621fff88",
   "metadata": {},
   "source": [
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9fc1af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_path = 'data/clean/'\n",
    "file_pattern = folder_path + '*.csv'\n",
    "csv_files = glob.glob(file_pattern)\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    if 'train' in csv_file:\n",
    "        df_train = pd.read_csv(csv_file)\n",
    "    elif 'val' in csv_file:\n",
    "        df_val = pd.read_csv(csv_file)\n",
    "    else:\n",
    "        df_test = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74ee6884",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the augmented dataset in the train dataframe\n",
    "df_train = pd.read_csv('data/clean/augmented/augmented_insert_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0b05fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>disappointment</th>\n",
       "      <th>...</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>my favourite food is anything the i did requir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>and now surely if tonight he does strip off hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>but why the damn fuck is this bayless isoing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>to forcibly make her parents feel threatened</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>as dirty southern bill wankers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   admiration  amusement  anger  annoyance  approval  caring  confusion  \\\n",
       "0           0          0      0          0         0       0          0   \n",
       "1           0          0      0          0         0       0          0   \n",
       "2           0          0      1          0         0       0          0   \n",
       "3           0          0      0          0         0       0          0   \n",
       "4           0          0      0          1         0       0          0   \n",
       "\n",
       "   curiosity  desire  disappointment  ...  nervousness  optimism  pride  \\\n",
       "0          0       0               0  ...            0         0      0   \n",
       "1          0       0               0  ...            0         0      0   \n",
       "2          0       0               0  ...            0         0      0   \n",
       "3          0       0               0  ...            0         0      0   \n",
       "4          0       0               0  ...            0         0      0   \n",
       "\n",
       "   realization  relief  remorse  sadness  surprise  neutral  \\\n",
       "0            0       0        0        0         0        1   \n",
       "1            0       0        0        0         0        1   \n",
       "2            0       0        0        0         0        0   \n",
       "3            0       0        0        0         0        0   \n",
       "4            0       0        0        0         0        0   \n",
       "\n",
       "                                          clean_text  \n",
       "0  my favourite food is anything the i did requir...  \n",
       "1  and now surely if tonight he does strip off hi...  \n",
       "2       but why the damn fuck is this bayless isoing  \n",
       "3       to forcibly make her parents feel threatened  \n",
       "4                     as dirty southern bill wankers  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b8b739",
   "metadata": {},
   "source": [
    "## 2. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46926bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections of config\n",
    "\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 200\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 0.2\n",
    "MODEL_NAME = 'distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ac2a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3882724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bf973e",
   "metadata": {},
   "source": [
    "#### Checking the max token length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d47c9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = df_train['clean_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a834a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  156.0\n"
     ]
    }
   ],
   "source": [
    "max_len = np.zeros(len(text_data))\n",
    "for i in range(len(text_data)):\n",
    "    input_ids = tokenizer.encode(text_data[i], add_special_tokens=True)\n",
    "    max_len[i] = len(input_ids)\n",
    "print('Max length: ', max_len.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aaa441b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dbab4d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9283e79b",
   "metadata": {},
   "source": [
    "#### Storing all 28 labels into variable target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4af580be",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [col for col in df_train.columns if col not in ['clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43261378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b7c54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a0abef",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42c58afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb3d783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the training set\n",
    "train_encodings = tokenizer(list(df_train['clean_text']), padding=True, truncation=True, return_tensors='pt')\n",
    "train_dataset = Dataset.from_dict({'input_ids': train_encodings['input_ids'],\n",
    "                                   'attention_mask': train_encodings['attention_mask'],\n",
    "                                   'labels': torch.tensor(df_train[target_cols].values, dtype=torch.float32)})\n",
    "\n",
    "# Tokenize the validation set\n",
    "val_encodings = tokenizer(list(df_val['clean_text']), padding=True, truncation=True, return_tensors='pt')\n",
    "valid_dataset = Dataset.from_dict({'input_ids': val_encodings['input_ids'],\n",
    "                                 'attention_mask': val_encodings['attention_mask'],\n",
    "                                 'labels': torch.tensor(df_val[target_cols].values, dtype=torch.float32)})\n",
    "\n",
    "\n",
    "test_encodings = tokenizer(list(df_test['clean_text']), padding=True, truncation=True, return_tensors='pt')\n",
    "test_dataset = Dataset.from_dict({'input_ids': test_encodings['input_ids'],\n",
    "                                 'attention_mask': test_encodings['attention_mask'],\n",
    "                                 'labels': torch.tensor(df_test[target_cols].values, dtype=torch.float32)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000eb841",
   "metadata": {},
   "source": [
    "### Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02efe9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Model\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(target_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e91901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom callback to get train and validation info during training\n",
    "class CustomCallback(TrainerCallback):\n",
    "    \n",
    "    def __init__(self, trainer) -> None:\n",
    "        super().__init__()\n",
    "        self._trainer = trainer\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if control.should_evaluate:\n",
    "            control_copy = copy.deepcopy(control)\n",
    "            self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\")\n",
    "            return control_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1980801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='data/output/dis/copy/',\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=VALID_BATCH_SIZE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    logging_dir=\"data/output/dis/copy/logs\",\n",
    "    learning_rate=float(LEARNING_RATE),\n",
    "    weight_decay=0.2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0eae176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom function to calculate the metrics for multi label classification\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'recall' : recall_micro,\n",
    "               'precision': precision_micro,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60338308",
   "metadata": {},
   "source": [
    "#### Compute class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5cdb39da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "num_classes = len(target_cols)\n",
    "class_counts = np.sum(train_dataset['labels'], axis=0)\n",
    "class_weights = 1.0 / class_counts\n",
    "\n",
    "# Normalize weights\n",
    "class_weights /= class_weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82eeccf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00434095, 0.00770108, 0.01144105, 0.00725835, 0.00610007,\n",
       "       0.01649321, 0.01310535, 0.00818262, 0.02796899, 0.0141389 ,\n",
       "       0.00886653, 0.02260797, 0.05936463, 0.02101773, 0.03013129,\n",
       "       0.00673483, 0.23283272, 0.01234719, 0.0085945 , 0.10998846,\n",
       "       0.01133973, 0.16151459, 0.01615146, 0.11717725, 0.03289563,\n",
       "       0.01353066, 0.01691332, 0.00126095])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6f99c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom trainer to incorporate class weights\n",
    "class MultiLabelTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights: Optional[FloatTensor] = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            class_weights = class_weights.to(self.args.device)\n",
    "            logging.info(f\"Using multi-label classification with class weights\", class_weights)\n",
    "        self.loss_fct = BCEWithLogitsLoss(weight=class_weights)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        try:\n",
    "            loss = self.loss_fct(outputs.logits.view(-1, model.num_labels), labels.view(-1,model.num_labels))\n",
    "        except AttributeError:  # DataParallel\n",
    "            loss = self.loss_fct(outputs.logits.view(-1, model.module.num_labels), labels.view(-1, model.num_labels))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b3287773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# create Trainer instance\n",
    "trainer = MultiLabelTrainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.add_callback(CustomCallback(trainer)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1e1c6498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/aishah/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13565' max='13565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13565/13565 50:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.097100</td>\n",
       "      <td>0.085436</td>\n",
       "      <td>0.524748</td>\n",
       "      <td>0.396912</td>\n",
       "      <td>0.774054</td>\n",
       "      <td>0.695914</td>\n",
       "      <td>0.384791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.097100</td>\n",
       "      <td>0.089917</td>\n",
       "      <td>0.514570</td>\n",
       "      <td>0.397179</td>\n",
       "      <td>0.730470</td>\n",
       "      <td>0.695377</td>\n",
       "      <td>0.385735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.084700</td>\n",
       "      <td>0.068246</td>\n",
       "      <td>0.658745</td>\n",
       "      <td>0.557684</td>\n",
       "      <td>0.804540</td>\n",
       "      <td>0.775869</td>\n",
       "      <td>0.528267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.084700</td>\n",
       "      <td>0.087049</td>\n",
       "      <td>0.563354</td>\n",
       "      <td>0.478056</td>\n",
       "      <td>0.685701</td>\n",
       "      <td>0.734226</td>\n",
       "      <td>0.450977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.068600</td>\n",
       "      <td>0.048466</td>\n",
       "      <td>0.781820</td>\n",
       "      <td>0.700550</td>\n",
       "      <td>0.884420</td>\n",
       "      <td>0.848266</td>\n",
       "      <td>0.666144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.068600</td>\n",
       "      <td>0.091469</td>\n",
       "      <td>0.563337</td>\n",
       "      <td>0.491066</td>\n",
       "      <td>0.660552</td>\n",
       "      <td>0.740002</td>\n",
       "      <td>0.454110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.034197</td>\n",
       "      <td>0.854525</td>\n",
       "      <td>0.791084</td>\n",
       "      <td>0.929028</td>\n",
       "      <td>0.894216</td>\n",
       "      <td>0.758639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.103259</td>\n",
       "      <td>0.557627</td>\n",
       "      <td>0.499373</td>\n",
       "      <td>0.631266</td>\n",
       "      <td>0.743293</td>\n",
       "      <td>0.453926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.028587</td>\n",
       "      <td>0.884272</td>\n",
       "      <td>0.835548</td>\n",
       "      <td>0.939031</td>\n",
       "      <td>0.916584</td>\n",
       "      <td>0.800981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.110839</td>\n",
       "      <td>0.557198</td>\n",
       "      <td>0.511129</td>\n",
       "      <td>0.612394</td>\n",
       "      <td>0.748474</td>\n",
       "      <td>0.451714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13565, training_loss=0.07190804746904103, metrics={'train_runtime': 3016.481, 'train_samples_per_second': 71.951, 'train_steps_per_second': 4.497, 'total_flos': 8764048050900480.0, 'train_loss': 0.07190804746904103, 'epoch': 5.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9dffa6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1635</td>\n",
       "      <td>4.815702e-05</td>\n",
       "      <td>0.18</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1145</td>\n",
       "      <td>4.631404e-05</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1040</td>\n",
       "      <td>4.447107e-05</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0996</td>\n",
       "      <td>4.262809e-05</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0971</td>\n",
       "      <td>4.078511e-05</td>\n",
       "      <td>0.92</td>\n",
       "      <td>2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2713</td>\n",
       "      <td>0.085436</td>\n",
       "      <td>0.524748</td>\n",
       "      <td>0.396912</td>\n",
       "      <td>0.774054</td>\n",
       "      <td>0.695914</td>\n",
       "      <td>0.384791</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089917</td>\n",
       "      <td>0.514570</td>\n",
       "      <td>0.397179</td>\n",
       "      <td>0.730470</td>\n",
       "      <td>0.695377</td>\n",
       "      <td>0.385735</td>\n",
       "      <td>11.2995</td>\n",
       "      <td>480.198</td>\n",
       "      <td>30.090</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0918</td>\n",
       "      <td>3.894213e-05</td>\n",
       "      <td>1.11</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0858</td>\n",
       "      <td>3.709915e-05</td>\n",
       "      <td>1.29</td>\n",
       "      <td>3500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0858</td>\n",
       "      <td>3.525617e-05</td>\n",
       "      <td>1.47</td>\n",
       "      <td>4000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0853</td>\n",
       "      <td>3.341320e-05</td>\n",
       "      <td>1.66</td>\n",
       "      <td>4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0847</td>\n",
       "      <td>3.157022e-05</td>\n",
       "      <td>1.84</td>\n",
       "      <td>5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5426</td>\n",
       "      <td>0.068246</td>\n",
       "      <td>0.658745</td>\n",
       "      <td>0.557684</td>\n",
       "      <td>0.804540</td>\n",
       "      <td>0.775869</td>\n",
       "      <td>0.528267</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087049</td>\n",
       "      <td>0.563354</td>\n",
       "      <td>0.478056</td>\n",
       "      <td>0.685701</td>\n",
       "      <td>0.734226</td>\n",
       "      <td>0.450977</td>\n",
       "      <td>11.2559</td>\n",
       "      <td>482.059</td>\n",
       "      <td>30.206</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0817</td>\n",
       "      <td>2.972724e-05</td>\n",
       "      <td>2.03</td>\n",
       "      <td>5500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0706</td>\n",
       "      <td>2.788426e-05</td>\n",
       "      <td>2.21</td>\n",
       "      <td>6000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0699</td>\n",
       "      <td>2.604128e-05</td>\n",
       "      <td>2.40</td>\n",
       "      <td>6500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0677</td>\n",
       "      <td>2.419830e-05</td>\n",
       "      <td>2.58</td>\n",
       "      <td>7000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0689</td>\n",
       "      <td>2.235533e-05</td>\n",
       "      <td>2.76</td>\n",
       "      <td>7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0686</td>\n",
       "      <td>2.051235e-05</td>\n",
       "      <td>2.95</td>\n",
       "      <td>8000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.048466</td>\n",
       "      <td>0.781820</td>\n",
       "      <td>0.700550</td>\n",
       "      <td>0.884420</td>\n",
       "      <td>0.848266</td>\n",
       "      <td>0.666144</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>8139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091469</td>\n",
       "      <td>0.563337</td>\n",
       "      <td>0.491066</td>\n",
       "      <td>0.660552</td>\n",
       "      <td>0.740002</td>\n",
       "      <td>0.454110</td>\n",
       "      <td>11.8388</td>\n",
       "      <td>458.324</td>\n",
       "      <td>28.719</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0562</td>\n",
       "      <td>1.866937e-05</td>\n",
       "      <td>3.13</td>\n",
       "      <td>8500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0522</td>\n",
       "      <td>1.682639e-05</td>\n",
       "      <td>3.32</td>\n",
       "      <td>9000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0518</td>\n",
       "      <td>1.498341e-05</td>\n",
       "      <td>3.50</td>\n",
       "      <td>9500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0507</td>\n",
       "      <td>1.314043e-05</td>\n",
       "      <td>3.69</td>\n",
       "      <td>10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0513</td>\n",
       "      <td>1.129746e-05</td>\n",
       "      <td>3.87</td>\n",
       "      <td>10500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.00</td>\n",
       "      <td>10852</td>\n",
       "      <td>0.034197</td>\n",
       "      <td>0.854525</td>\n",
       "      <td>0.791084</td>\n",
       "      <td>0.929028</td>\n",
       "      <td>0.894216</td>\n",
       "      <td>0.758639</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.00</td>\n",
       "      <td>10852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103259</td>\n",
       "      <td>0.557627</td>\n",
       "      <td>0.499373</td>\n",
       "      <td>0.631266</td>\n",
       "      <td>0.743293</td>\n",
       "      <td>0.453926</td>\n",
       "      <td>11.6749</td>\n",
       "      <td>464.759</td>\n",
       "      <td>29.122</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0489</td>\n",
       "      <td>9.454478e-06</td>\n",
       "      <td>4.05</td>\n",
       "      <td>11000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0384</td>\n",
       "      <td>7.611500e-06</td>\n",
       "      <td>4.24</td>\n",
       "      <td>11500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0395</td>\n",
       "      <td>5.768522e-06</td>\n",
       "      <td>4.42</td>\n",
       "      <td>12000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0396</td>\n",
       "      <td>3.925544e-06</td>\n",
       "      <td>4.61</td>\n",
       "      <td>12500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0391</td>\n",
       "      <td>2.082565e-06</td>\n",
       "      <td>4.79</td>\n",
       "      <td>13000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0389</td>\n",
       "      <td>2.395872e-07</td>\n",
       "      <td>4.98</td>\n",
       "      <td>13500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.028587</td>\n",
       "      <td>0.884272</td>\n",
       "      <td>0.835548</td>\n",
       "      <td>0.939031</td>\n",
       "      <td>0.916584</td>\n",
       "      <td>0.800981</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110839</td>\n",
       "      <td>0.557198</td>\n",
       "      <td>0.511129</td>\n",
       "      <td>0.612394</td>\n",
       "      <td>0.748474</td>\n",
       "      <td>0.451714</td>\n",
       "      <td>11.4053</td>\n",
       "      <td>475.745</td>\n",
       "      <td>29.811</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.071908</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.764048e+15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      loss  learning_rate  epoch   step  train_loss  train_f1  train_recall  \\\n",
       "0   0.1635   4.815702e-05   0.18    500         NaN       NaN           NaN   \n",
       "1   0.1145   4.631404e-05   0.37   1000         NaN       NaN           NaN   \n",
       "2   0.1040   4.447107e-05   0.55   1500         NaN       NaN           NaN   \n",
       "3   0.0996   4.262809e-05   0.74   2000         NaN       NaN           NaN   \n",
       "4   0.0971   4.078511e-05   0.92   2500         NaN       NaN           NaN   \n",
       "5      NaN            NaN   1.00   2713    0.085436  0.524748      0.396912   \n",
       "6      NaN            NaN   1.00   2713         NaN       NaN           NaN   \n",
       "7   0.0918   3.894213e-05   1.11   3000         NaN       NaN           NaN   \n",
       "8   0.0858   3.709915e-05   1.29   3500         NaN       NaN           NaN   \n",
       "9   0.0858   3.525617e-05   1.47   4000         NaN       NaN           NaN   \n",
       "10  0.0853   3.341320e-05   1.66   4500         NaN       NaN           NaN   \n",
       "11  0.0847   3.157022e-05   1.84   5000         NaN       NaN           NaN   \n",
       "12     NaN            NaN   2.00   5426    0.068246  0.658745      0.557684   \n",
       "13     NaN            NaN   2.00   5426         NaN       NaN           NaN   \n",
       "14  0.0817   2.972724e-05   2.03   5500         NaN       NaN           NaN   \n",
       "15  0.0706   2.788426e-05   2.21   6000         NaN       NaN           NaN   \n",
       "16  0.0699   2.604128e-05   2.40   6500         NaN       NaN           NaN   \n",
       "17  0.0677   2.419830e-05   2.58   7000         NaN       NaN           NaN   \n",
       "18  0.0689   2.235533e-05   2.76   7500         NaN       NaN           NaN   \n",
       "19  0.0686   2.051235e-05   2.95   8000         NaN       NaN           NaN   \n",
       "20     NaN            NaN   3.00   8139    0.048466  0.781820      0.700550   \n",
       "21     NaN            NaN   3.00   8139         NaN       NaN           NaN   \n",
       "22  0.0562   1.866937e-05   3.13   8500         NaN       NaN           NaN   \n",
       "23  0.0522   1.682639e-05   3.32   9000         NaN       NaN           NaN   \n",
       "24  0.0518   1.498341e-05   3.50   9500         NaN       NaN           NaN   \n",
       "25  0.0507   1.314043e-05   3.69  10000         NaN       NaN           NaN   \n",
       "26  0.0513   1.129746e-05   3.87  10500         NaN       NaN           NaN   \n",
       "27     NaN            NaN   4.00  10852    0.034197  0.854525      0.791084   \n",
       "28     NaN            NaN   4.00  10852         NaN       NaN           NaN   \n",
       "29  0.0489   9.454478e-06   4.05  11000         NaN       NaN           NaN   \n",
       "30  0.0384   7.611500e-06   4.24  11500         NaN       NaN           NaN   \n",
       "31  0.0395   5.768522e-06   4.42  12000         NaN       NaN           NaN   \n",
       "32  0.0396   3.925544e-06   4.61  12500         NaN       NaN           NaN   \n",
       "33  0.0391   2.082565e-06   4.79  13000         NaN       NaN           NaN   \n",
       "34  0.0389   2.395872e-07   4.98  13500         NaN       NaN           NaN   \n",
       "35     NaN            NaN   5.00  13565    0.028587  0.884272      0.835548   \n",
       "36     NaN            NaN   5.00  13565         NaN       NaN           NaN   \n",
       "37     NaN            NaN   5.00  13565    0.071908       NaN           NaN   \n",
       "\n",
       "    train_precision  train_roc_auc  train_accuracy  ...  eval_loss   eval_f1  \\\n",
       "0               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "1               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "2               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "3               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "4               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "5          0.774054       0.695914        0.384791  ...        NaN       NaN   \n",
       "6               NaN            NaN             NaN  ...   0.089917  0.514570   \n",
       "7               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "8               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "9               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "10              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "11              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "12         0.804540       0.775869        0.528267  ...        NaN       NaN   \n",
       "13              NaN            NaN             NaN  ...   0.087049  0.563354   \n",
       "14              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "15              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "16              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "17              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "18              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "19              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "20         0.884420       0.848266        0.666144  ...        NaN       NaN   \n",
       "21              NaN            NaN             NaN  ...   0.091469  0.563337   \n",
       "22              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "23              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "24              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "25              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "26              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "27         0.929028       0.894216        0.758639  ...        NaN       NaN   \n",
       "28              NaN            NaN             NaN  ...   0.103259  0.557627   \n",
       "29              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "30              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "31              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "32              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "33              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "34              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "35         0.939031       0.916584        0.800981  ...        NaN       NaN   \n",
       "36              NaN            NaN             NaN  ...   0.110839  0.557198   \n",
       "37              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "\n",
       "    eval_recall  eval_precision  eval_roc_auc  eval_accuracy  eval_runtime  \\\n",
       "0           NaN             NaN           NaN            NaN           NaN   \n",
       "1           NaN             NaN           NaN            NaN           NaN   \n",
       "2           NaN             NaN           NaN            NaN           NaN   \n",
       "3           NaN             NaN           NaN            NaN           NaN   \n",
       "4           NaN             NaN           NaN            NaN           NaN   \n",
       "5           NaN             NaN           NaN            NaN           NaN   \n",
       "6      0.397179        0.730470      0.695377       0.385735       11.2995   \n",
       "7           NaN             NaN           NaN            NaN           NaN   \n",
       "8           NaN             NaN           NaN            NaN           NaN   \n",
       "9           NaN             NaN           NaN            NaN           NaN   \n",
       "10          NaN             NaN           NaN            NaN           NaN   \n",
       "11          NaN             NaN           NaN            NaN           NaN   \n",
       "12          NaN             NaN           NaN            NaN           NaN   \n",
       "13     0.478056        0.685701      0.734226       0.450977       11.2559   \n",
       "14          NaN             NaN           NaN            NaN           NaN   \n",
       "15          NaN             NaN           NaN            NaN           NaN   \n",
       "16          NaN             NaN           NaN            NaN           NaN   \n",
       "17          NaN             NaN           NaN            NaN           NaN   \n",
       "18          NaN             NaN           NaN            NaN           NaN   \n",
       "19          NaN             NaN           NaN            NaN           NaN   \n",
       "20          NaN             NaN           NaN            NaN           NaN   \n",
       "21     0.491066        0.660552      0.740002       0.454110       11.8388   \n",
       "22          NaN             NaN           NaN            NaN           NaN   \n",
       "23          NaN             NaN           NaN            NaN           NaN   \n",
       "24          NaN             NaN           NaN            NaN           NaN   \n",
       "25          NaN             NaN           NaN            NaN           NaN   \n",
       "26          NaN             NaN           NaN            NaN           NaN   \n",
       "27          NaN             NaN           NaN            NaN           NaN   \n",
       "28     0.499373        0.631266      0.743293       0.453926       11.6749   \n",
       "29          NaN             NaN           NaN            NaN           NaN   \n",
       "30          NaN             NaN           NaN            NaN           NaN   \n",
       "31          NaN             NaN           NaN            NaN           NaN   \n",
       "32          NaN             NaN           NaN            NaN           NaN   \n",
       "33          NaN             NaN           NaN            NaN           NaN   \n",
       "34          NaN             NaN           NaN            NaN           NaN   \n",
       "35          NaN             NaN           NaN            NaN           NaN   \n",
       "36     0.511129        0.612394      0.748474       0.451714       11.4053   \n",
       "37          NaN             NaN           NaN            NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second    total_flos  \n",
       "0                       NaN                    NaN           NaN  \n",
       "1                       NaN                    NaN           NaN  \n",
       "2                       NaN                    NaN           NaN  \n",
       "3                       NaN                    NaN           NaN  \n",
       "4                       NaN                    NaN           NaN  \n",
       "5                       NaN                    NaN           NaN  \n",
       "6                   480.198                 30.090           NaN  \n",
       "7                       NaN                    NaN           NaN  \n",
       "8                       NaN                    NaN           NaN  \n",
       "9                       NaN                    NaN           NaN  \n",
       "10                      NaN                    NaN           NaN  \n",
       "11                      NaN                    NaN           NaN  \n",
       "12                      NaN                    NaN           NaN  \n",
       "13                  482.059                 30.206           NaN  \n",
       "14                      NaN                    NaN           NaN  \n",
       "15                      NaN                    NaN           NaN  \n",
       "16                      NaN                    NaN           NaN  \n",
       "17                      NaN                    NaN           NaN  \n",
       "18                      NaN                    NaN           NaN  \n",
       "19                      NaN                    NaN           NaN  \n",
       "20                      NaN                    NaN           NaN  \n",
       "21                  458.324                 28.719           NaN  \n",
       "22                      NaN                    NaN           NaN  \n",
       "23                      NaN                    NaN           NaN  \n",
       "24                      NaN                    NaN           NaN  \n",
       "25                      NaN                    NaN           NaN  \n",
       "26                      NaN                    NaN           NaN  \n",
       "27                      NaN                    NaN           NaN  \n",
       "28                  464.759                 29.122           NaN  \n",
       "29                      NaN                    NaN           NaN  \n",
       "30                      NaN                    NaN           NaN  \n",
       "31                      NaN                    NaN           NaN  \n",
       "32                      NaN                    NaN           NaN  \n",
       "33                      NaN                    NaN           NaN  \n",
       "34                      NaN                    NaN           NaN  \n",
       "35                      NaN                    NaN           NaN  \n",
       "36                  475.745                 29.811           NaN  \n",
       "37                      NaN                    NaN  8.764048e+15  \n",
       "\n",
       "[38 rows x 23 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view train and validation metrics from training\n",
    "log_history = pd.DataFrame(trainer.state.log_history)\n",
    "log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9edd140f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2713</td>\n",
       "      <td>0.085436</td>\n",
       "      <td>0.524748</td>\n",
       "      <td>0.396912</td>\n",
       "      <td>0.774054</td>\n",
       "      <td>0.695914</td>\n",
       "      <td>0.384791</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5426</td>\n",
       "      <td>0.068246</td>\n",
       "      <td>0.658745</td>\n",
       "      <td>0.557684</td>\n",
       "      <td>0.804540</td>\n",
       "      <td>0.775869</td>\n",
       "      <td>0.528267</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.048466</td>\n",
       "      <td>0.781820</td>\n",
       "      <td>0.700550</td>\n",
       "      <td>0.884420</td>\n",
       "      <td>0.848266</td>\n",
       "      <td>0.666144</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10852</td>\n",
       "      <td>0.034197</td>\n",
       "      <td>0.854525</td>\n",
       "      <td>0.791084</td>\n",
       "      <td>0.929028</td>\n",
       "      <td>0.894216</td>\n",
       "      <td>0.758639</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.028587</td>\n",
       "      <td>0.884272</td>\n",
       "      <td>0.835548</td>\n",
       "      <td>0.939031</td>\n",
       "      <td>0.916584</td>\n",
       "      <td>0.800981</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    loss  learning_rate  epoch   step  train_loss  train_f1  train_recall  \\\n",
       "5    NaN            NaN    1.0   2713    0.085436  0.524748      0.396912   \n",
       "12   NaN            NaN    2.0   5426    0.068246  0.658745      0.557684   \n",
       "20   NaN            NaN    3.0   8139    0.048466  0.781820      0.700550   \n",
       "27   NaN            NaN    4.0  10852    0.034197  0.854525      0.791084   \n",
       "35   NaN            NaN    5.0  13565    0.028587  0.884272      0.835548   \n",
       "\n",
       "    train_precision  train_roc_auc  train_accuracy  ...  eval_loss  eval_f1  \\\n",
       "5          0.774054       0.695914        0.384791  ...        NaN      NaN   \n",
       "12         0.804540       0.775869        0.528267  ...        NaN      NaN   \n",
       "20         0.884420       0.848266        0.666144  ...        NaN      NaN   \n",
       "27         0.929028       0.894216        0.758639  ...        NaN      NaN   \n",
       "35         0.939031       0.916584        0.800981  ...        NaN      NaN   \n",
       "\n",
       "    eval_recall  eval_precision  eval_roc_auc  eval_accuracy  eval_runtime  \\\n",
       "5           NaN             NaN           NaN            NaN           NaN   \n",
       "12          NaN             NaN           NaN            NaN           NaN   \n",
       "20          NaN             NaN           NaN            NaN           NaN   \n",
       "27          NaN             NaN           NaN            NaN           NaN   \n",
       "35          NaN             NaN           NaN            NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  total_flos  \n",
       "5                       NaN                    NaN         NaN  \n",
       "12                      NaN                    NaN         NaN  \n",
       "20                      NaN                    NaN         NaN  \n",
       "27                      NaN                    NaN         NaN  \n",
       "35                      NaN                    NaN         NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store train metrics in dataframe\n",
    "train_history = log_history[log_history['train_f1'].notna()]\n",
    "train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d7c76fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089917</td>\n",
       "      <td>0.514570</td>\n",
       "      <td>0.397179</td>\n",
       "      <td>0.730470</td>\n",
       "      <td>0.695377</td>\n",
       "      <td>0.385735</td>\n",
       "      <td>11.2995</td>\n",
       "      <td>480.198</td>\n",
       "      <td>30.090</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087049</td>\n",
       "      <td>0.563354</td>\n",
       "      <td>0.478056</td>\n",
       "      <td>0.685701</td>\n",
       "      <td>0.734226</td>\n",
       "      <td>0.450977</td>\n",
       "      <td>11.2559</td>\n",
       "      <td>482.059</td>\n",
       "      <td>30.206</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091469</td>\n",
       "      <td>0.563337</td>\n",
       "      <td>0.491066</td>\n",
       "      <td>0.660552</td>\n",
       "      <td>0.740002</td>\n",
       "      <td>0.454110</td>\n",
       "      <td>11.8388</td>\n",
       "      <td>458.324</td>\n",
       "      <td>28.719</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103259</td>\n",
       "      <td>0.557627</td>\n",
       "      <td>0.499373</td>\n",
       "      <td>0.631266</td>\n",
       "      <td>0.743293</td>\n",
       "      <td>0.453926</td>\n",
       "      <td>11.6749</td>\n",
       "      <td>464.759</td>\n",
       "      <td>29.122</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110839</td>\n",
       "      <td>0.557198</td>\n",
       "      <td>0.511129</td>\n",
       "      <td>0.612394</td>\n",
       "      <td>0.748474</td>\n",
       "      <td>0.451714</td>\n",
       "      <td>11.4053</td>\n",
       "      <td>475.745</td>\n",
       "      <td>29.811</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    loss  learning_rate  epoch   step  train_loss  train_f1  train_recall  \\\n",
       "6    NaN            NaN    1.0   2713         NaN       NaN           NaN   \n",
       "13   NaN            NaN    2.0   5426         NaN       NaN           NaN   \n",
       "21   NaN            NaN    3.0   8139         NaN       NaN           NaN   \n",
       "28   NaN            NaN    4.0  10852         NaN       NaN           NaN   \n",
       "36   NaN            NaN    5.0  13565         NaN       NaN           NaN   \n",
       "\n",
       "    train_precision  train_roc_auc  train_accuracy  ...  eval_loss   eval_f1  \\\n",
       "6               NaN            NaN             NaN  ...   0.089917  0.514570   \n",
       "13              NaN            NaN             NaN  ...   0.087049  0.563354   \n",
       "21              NaN            NaN             NaN  ...   0.091469  0.563337   \n",
       "28              NaN            NaN             NaN  ...   0.103259  0.557627   \n",
       "36              NaN            NaN             NaN  ...   0.110839  0.557198   \n",
       "\n",
       "    eval_recall  eval_precision  eval_roc_auc  eval_accuracy  eval_runtime  \\\n",
       "6      0.397179        0.730470      0.695377       0.385735       11.2995   \n",
       "13     0.478056        0.685701      0.734226       0.450977       11.2559   \n",
       "21     0.491066        0.660552      0.740002       0.454110       11.8388   \n",
       "28     0.499373        0.631266      0.743293       0.453926       11.6749   \n",
       "36     0.511129        0.612394      0.748474       0.451714       11.4053   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  total_flos  \n",
       "6                   480.198                 30.090         NaN  \n",
       "13                  482.059                 30.206         NaN  \n",
       "21                  458.324                 28.719         NaN  \n",
       "28                  464.759                 29.122         NaN  \n",
       "36                  475.745                 29.811         NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store validation metrics in dataframe\n",
    "val_history = log_history[log_history['eval_f1'].notna()]\n",
    "val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e1febf2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_x</th>\n",
       "      <th>learning_rate_x</th>\n",
       "      <th>epoch_x</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss_x</th>\n",
       "      <th>train_f1_x</th>\n",
       "      <th>train_recall_x</th>\n",
       "      <th>train_precision_x</th>\n",
       "      <th>train_roc_auc_x</th>\n",
       "      <th>train_accuracy_x</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss_y</th>\n",
       "      <th>eval_f1_y</th>\n",
       "      <th>eval_recall_y</th>\n",
       "      <th>eval_precision_y</th>\n",
       "      <th>eval_roc_auc_y</th>\n",
       "      <th>eval_accuracy_y</th>\n",
       "      <th>eval_runtime_y</th>\n",
       "      <th>eval_samples_per_second_y</th>\n",
       "      <th>eval_steps_per_second_y</th>\n",
       "      <th>total_flos_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2713</td>\n",
       "      <td>0.085436</td>\n",
       "      <td>0.524748</td>\n",
       "      <td>0.396912</td>\n",
       "      <td>0.774054</td>\n",
       "      <td>0.695914</td>\n",
       "      <td>0.384791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089917</td>\n",
       "      <td>0.514570</td>\n",
       "      <td>0.397179</td>\n",
       "      <td>0.730470</td>\n",
       "      <td>0.695377</td>\n",
       "      <td>0.385735</td>\n",
       "      <td>11.2995</td>\n",
       "      <td>480.198</td>\n",
       "      <td>30.090</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5426</td>\n",
       "      <td>0.068246</td>\n",
       "      <td>0.658745</td>\n",
       "      <td>0.557684</td>\n",
       "      <td>0.804540</td>\n",
       "      <td>0.775869</td>\n",
       "      <td>0.528267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087049</td>\n",
       "      <td>0.563354</td>\n",
       "      <td>0.478056</td>\n",
       "      <td>0.685701</td>\n",
       "      <td>0.734226</td>\n",
       "      <td>0.450977</td>\n",
       "      <td>11.2559</td>\n",
       "      <td>482.059</td>\n",
       "      <td>30.206</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.048466</td>\n",
       "      <td>0.781820</td>\n",
       "      <td>0.700550</td>\n",
       "      <td>0.884420</td>\n",
       "      <td>0.848266</td>\n",
       "      <td>0.666144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091469</td>\n",
       "      <td>0.563337</td>\n",
       "      <td>0.491066</td>\n",
       "      <td>0.660552</td>\n",
       "      <td>0.740002</td>\n",
       "      <td>0.454110</td>\n",
       "      <td>11.8388</td>\n",
       "      <td>458.324</td>\n",
       "      <td>28.719</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10852</td>\n",
       "      <td>0.034197</td>\n",
       "      <td>0.854525</td>\n",
       "      <td>0.791084</td>\n",
       "      <td>0.929028</td>\n",
       "      <td>0.894216</td>\n",
       "      <td>0.758639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103259</td>\n",
       "      <td>0.557627</td>\n",
       "      <td>0.499373</td>\n",
       "      <td>0.631266</td>\n",
       "      <td>0.743293</td>\n",
       "      <td>0.453926</td>\n",
       "      <td>11.6749</td>\n",
       "      <td>464.759</td>\n",
       "      <td>29.122</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.028587</td>\n",
       "      <td>0.884272</td>\n",
       "      <td>0.835548</td>\n",
       "      <td>0.939031</td>\n",
       "      <td>0.916584</td>\n",
       "      <td>0.800981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110839</td>\n",
       "      <td>0.557198</td>\n",
       "      <td>0.511129</td>\n",
       "      <td>0.612394</td>\n",
       "      <td>0.748474</td>\n",
       "      <td>0.451714</td>\n",
       "      <td>11.4053</td>\n",
       "      <td>475.745</td>\n",
       "      <td>29.811</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loss_x  learning_rate_x  epoch_x   step  train_loss_x  train_f1_x  \\\n",
       "0     NaN              NaN      1.0   2713      0.085436    0.524748   \n",
       "1     NaN              NaN      2.0   5426      0.068246    0.658745   \n",
       "2     NaN              NaN      3.0   8139      0.048466    0.781820   \n",
       "3     NaN              NaN      4.0  10852      0.034197    0.854525   \n",
       "4     NaN              NaN      5.0  13565      0.028587    0.884272   \n",
       "\n",
       "   train_recall_x  train_precision_x  train_roc_auc_x  train_accuracy_x  ...  \\\n",
       "0        0.396912           0.774054         0.695914          0.384791  ...   \n",
       "1        0.557684           0.804540         0.775869          0.528267  ...   \n",
       "2        0.700550           0.884420         0.848266          0.666144  ...   \n",
       "3        0.791084           0.929028         0.894216          0.758639  ...   \n",
       "4        0.835548           0.939031         0.916584          0.800981  ...   \n",
       "\n",
       "   eval_loss_y  eval_f1_y  eval_recall_y  eval_precision_y  eval_roc_auc_y  \\\n",
       "0     0.089917   0.514570       0.397179          0.730470        0.695377   \n",
       "1     0.087049   0.563354       0.478056          0.685701        0.734226   \n",
       "2     0.091469   0.563337       0.491066          0.660552        0.740002   \n",
       "3     0.103259   0.557627       0.499373          0.631266        0.743293   \n",
       "4     0.110839   0.557198       0.511129          0.612394        0.748474   \n",
       "\n",
       "   eval_accuracy_y  eval_runtime_y  eval_samples_per_second_y  \\\n",
       "0         0.385735         11.2995                    480.198   \n",
       "1         0.450977         11.2559                    482.059   \n",
       "2         0.454110         11.8388                    458.324   \n",
       "3         0.453926         11.6749                    464.759   \n",
       "4         0.451714         11.4053                    475.745   \n",
       "\n",
       "   eval_steps_per_second_y  total_flos_y  \n",
       "0                   30.090           NaN  \n",
       "1                   30.206           NaN  \n",
       "2                   28.719           NaN  \n",
       "3                   29.122           NaN  \n",
       "4                   29.811           NaN  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = pd.merge(train_history, val_history, on='step', how='outer')\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ebe0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting function to check for accuracy with graphs \n",
    "def plot_model_performance(history):\n",
    "\n",
    "    #getting train and validation accuracy\n",
    "    acc = history['train_accuracy_x']\n",
    "    val_acc = history['eval_accuracy_y']\n",
    "\n",
    "    #getting train and validation loss\n",
    "    loss = history['train_loss_x']\n",
    "    val_loss = history['eval_loss_y']\n",
    "\n",
    "    epochs_range = range(5)\n",
    "\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c97c350d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_model_performance(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7edce3",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c5456e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to evaluate the model on the test set\n",
    "def evaluate_model(test_dataset):\n",
    "    test_res = trainer.evaluate(eval_dataset=test_dataset)\n",
    "    print(test_res)\n",
    "         \n",
    "    return trainer.predict(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7bb617c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08623728901147842, 'eval_f1': 0.5638725352770769, 'eval_recall': 0.47669458050244906, 'eval_precision': 0.6900731930466606, 'eval_roc_auc': 0.7336949936304055, 'eval_accuracy': 0.4488667772249862, 'eval_runtime': 7.5913, 'eval_samples_per_second': 714.898, 'eval_steps_per_second': 44.788, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "y_test = evaluate_model(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c172305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate each emotion label metrics on test set\n",
    "def calc_label_metrics(label, y_targets, y_preds, threshold):\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"accuracy\": metrics.accuracy_score(y_targets, y_preds),\n",
    "        \"precision\": metrics.precision_score(y_targets, y_preds, zero_division=0),\n",
    "        \"recall\": metrics.recall_score(y_targets, y_preds, zero_division=0),\n",
    "        \"f1\": metrics.f1_score(y_targets, y_preds, zero_division=0),\n",
    "        \"mcc\": metrics.matthews_corrcoef(y_targets, y_preds),\n",
    "        \"support\": y_targets.sum(),\n",
    "        \"threshold\": threshold,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "78c37071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate overall metric on test set\n",
    "def calc_test_metrics(y_test, test_dataset, target_cols):\n",
    "    threshold = 0.5\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(y_test.predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= 0.5)] = 1\n",
    "    \n",
    "    # finally, compute metrics\n",
    "    y_true = df_test[target_cols].values\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'recall' : recall_micro,\n",
    "               'precision': precision_micro,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    metrics_df = pd.DataFrame.from_dict(metrics, orient='index', columns=['Value'])\n",
    "\n",
    "    display(metrics_df)\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for label_index, label in enumerate(target_cols):\n",
    "        y_targets, y_preds = y_true[:, label_index], y_pred[:, label_index]\n",
    "        results.append(calc_label_metrics(label, y_targets, y_preds, threshold))\n",
    "\n",
    "    per_label_results = pd.DataFrame(results, index=target_cols)\n",
    "    display(per_label_results.drop(columns=[\"label\"]).round(3))\n",
    "    \n",
    "    return y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "794fec4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.563873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.476695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.690073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc</th>\n",
       "      <td>0.733695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.448867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "f1         0.563873\n",
       "recall     0.476695\n",
       "precision  0.690073\n",
       "roc_auc    0.733695\n",
       "accuracy   0.448867"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>mcc</th>\n",
       "      <th>support</th>\n",
       "      <th>threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.935</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.623</td>\n",
       "      <td>504</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.982</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.808</td>\n",
       "      <td>264</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.966</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.475</td>\n",
       "      <td>198</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.943</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.191</td>\n",
       "      <td>320</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.942</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.330</td>\n",
       "      <td>351</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.976</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.328</td>\n",
       "      <td>135</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.973</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.307</td>\n",
       "      <td>153</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.949</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.314</td>\n",
       "      <td>284</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.987</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.475</td>\n",
       "      <td>83</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.974</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.280</td>\n",
       "      <td>151</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.950</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.283</td>\n",
       "      <td>267</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.981</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.445</td>\n",
       "      <td>123</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.994</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.327</td>\n",
       "      <td>37</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.983</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.409</td>\n",
       "      <td>103</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.991</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.652</td>\n",
       "      <td>78</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.989</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.911</td>\n",
       "      <td>352</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.999</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.977</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.480</td>\n",
       "      <td>161</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.982</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.800</td>\n",
       "      <td>238</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.996</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.147</td>\n",
       "      <td>23</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.975</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.566</td>\n",
       "      <td>186</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.997</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.975</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.262</td>\n",
       "      <td>145</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.998</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.993</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.706</td>\n",
       "      <td>56</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.978</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.552</td>\n",
       "      <td>156</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.978</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.471</td>\n",
       "      <td>141</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.776</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.485</td>\n",
       "      <td>1787</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                accuracy  precision  recall     f1    mcc  support  threshold\n",
       "admiration         0.935      0.640   0.679  0.659  0.623      504        0.5\n",
       "amusement          0.982      0.805   0.830  0.817  0.808      264        0.5\n",
       "anger              0.966      0.551   0.439  0.489  0.475      198        0.5\n",
       "annoyance          0.943      0.720   0.056  0.104  0.191      320        0.5\n",
       "approval           0.942      0.701   0.174  0.279  0.330      351        0.5\n",
       "caring             0.976      0.549   0.207  0.301  0.328      135        0.5\n",
       "confusion          0.973      0.610   0.163  0.258  0.307      153        0.5\n",
       "curiosity          0.949      0.541   0.208  0.300  0.314      284        0.5\n",
       "desire             0.987      0.620   0.373  0.466  0.475       83        0.5\n",
       "disappointment     0.974      0.875   0.093  0.168  0.280      151        0.5\n",
       "disapproval        0.950      0.469   0.199  0.279  0.283      267        0.5\n",
       "disgust            0.981      0.739   0.276  0.402  0.445      123        0.5\n",
       "embarrassment      0.994      0.667   0.162  0.261  0.327       37        0.5\n",
       "excitement         0.983      0.636   0.272  0.381  0.409      103        0.5\n",
       "fear               0.991      0.730   0.590  0.652  0.652       78        0.5\n",
       "gratitude          0.989      0.930   0.903  0.916  0.911      352        0.5\n",
       "grief              0.999      0.000   0.000  0.000  0.000        6        0.5\n",
       "joy                0.977      0.769   0.311  0.442  0.480      161        0.5\n",
       "love               0.982      0.778   0.840  0.808  0.800      238        0.5\n",
       "nervousness        0.996      0.500   0.043  0.080  0.147       23        0.5\n",
       "optimism           0.975      0.713   0.468  0.565  0.566      186        0.5\n",
       "pride              0.997      0.000   0.000  0.000  0.000       16        0.5\n",
       "realization        0.975      0.857   0.083  0.151  0.262      145        0.5\n",
       "relief             0.998      0.000   0.000  0.000  0.000       11        0.5\n",
       "remorse            0.993      0.613   0.821  0.702  0.706       56        0.5\n",
       "sadness            0.978      0.667   0.474  0.554  0.552      156        0.5\n",
       "surprise           0.978      0.607   0.383  0.470  0.471      141        0.5\n",
       "neutral            0.776      0.670   0.629  0.649  0.485     1787        0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "targets, outputs = calc_test_metrics(y_test, test_dataset, target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f1631d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results DataFrame:\n",
      "         Actual                Predicted\n",
      "0     [sadness]                   [love]\n",
      "1  [admiration]             [admiration]\n",
      "2  [excitement]               [optimism]\n",
      "3   [gratitude]              [gratitude]\n",
      "4     [neutral]                [neutral]\n",
      "5   [gratitude]              [gratitude]\n",
      "6   [gratitude]              [gratitude]\n",
      "7   [gratitude]  [admiration, gratitude]\n",
      "8     [remorse]       [remorse, sadness]\n",
      "9     [sadness]                [sadness]\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to store actual labels and predicted labels\n",
    "final_df = pd.DataFrame({\n",
    "    'Actual': [list(np.where(targets[i])[0]) for i in range(len(targets))],\n",
    "    'Predicted': [list(np.where(outputs[i])[0]) for i in range(len(outputs))]\n",
    "})\n",
    "\n",
    "# Map label indices to label names in the 'Actual' column\n",
    "final_df['Actual'] = final_df['Actual'].apply(lambda indices: [target_cols[idx] for idx in indices])\n",
    "\n",
    "# Map label indices to label names in the 'Predicted' column\n",
    "final_df['Predicted'] = final_df['Predicted'].apply(lambda indices: [target_cols[idx] for idx in indices])\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(\"Results DataFrame:\")\n",
    "print(final_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "301ef201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the label DataFrame with the original DataFramev\n",
    "val_df_terms = df_test['clean_text']\n",
    "result_df = pd.concat([val_df_terms, final_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a335fd5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am really sorry about your situation frown s...</td>\n",
       "      <td>[sadness]</td>\n",
       "      <td>[love]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it is wonderful because it is awful at not with</td>\n",
       "      <td>[admiration]</td>\n",
       "      <td>[admiration]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kings fan here good luck to you guys will be a...</td>\n",
       "      <td>[excitement]</td>\n",
       "      <td>[optimism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i did not know that thank you for teaching me ...</td>\n",
       "      <td>[gratitude]</td>\n",
       "      <td>[gratitude]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>they got bored from haunting earth for thousan...</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5422</th>\n",
       "      <td>thanks i was diagnosed with bp 1 after the hos...</td>\n",
       "      <td>[gratitude]</td>\n",
       "      <td>[gratitude]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5423</th>\n",
       "      <td>well that makes sense</td>\n",
       "      <td>[approval]</td>\n",
       "      <td>[approval]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5424</th>\n",
       "      <td>daddy issues name</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5425</th>\n",
       "      <td>so glad i discovered that subreddit a couple m...</td>\n",
       "      <td>[admiration]</td>\n",
       "      <td>[admiration, joy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5426</th>\n",
       "      <td>had to watch elmo in grouchland one time too m...</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5427 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_text        Actual  \\\n",
       "0     i am really sorry about your situation frown s...     [sadness]   \n",
       "1       it is wonderful because it is awful at not with  [admiration]   \n",
       "2     kings fan here good luck to you guys will be a...  [excitement]   \n",
       "3     i did not know that thank you for teaching me ...   [gratitude]   \n",
       "4     they got bored from haunting earth for thousan...     [neutral]   \n",
       "...                                                 ...           ...   \n",
       "5422  thanks i was diagnosed with bp 1 after the hos...   [gratitude]   \n",
       "5423                              well that makes sense    [approval]   \n",
       "5424                                  daddy issues name     [neutral]   \n",
       "5425  so glad i discovered that subreddit a couple m...  [admiration]   \n",
       "5426  had to watch elmo in grouchland one time too m...     [neutral]   \n",
       "\n",
       "              Predicted  \n",
       "0                [love]  \n",
       "1          [admiration]  \n",
       "2            [optimism]  \n",
       "3           [gratitude]  \n",
       "4             [neutral]  \n",
       "...                 ...  \n",
       "5422        [gratitude]  \n",
       "5423         [approval]  \n",
       "5424          [neutral]  \n",
       "5425  [admiration, joy]  \n",
       "5426          [neutral]  \n",
       "\n",
       "[5427 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71077a67",
   "metadata": {},
   "source": [
    "## 4. Save the output, tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5113b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('output_distilbert_m4.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b8b778c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer\n",
    "tokenizer.save_pretrained('./distilbert_M4_transformer/')\n",
    "\n",
    "# Save model\n",
    "trainer.save_model('./distilbert_M4_transformer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba9b95b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
