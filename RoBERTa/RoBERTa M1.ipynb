{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a4b07d",
   "metadata": {},
   "source": [
    "# Roberta M1 (with class weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00c3d39",
   "metadata": {},
   "source": [
    "Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019c1d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn ,cuda\n",
    "from torch.utils.data import DataLoader,Dataset,RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "import nltk.corpus\n",
    "from sklearn import metrics\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, AutoTokenizer, BertModel, BertConfig, AutoModel, AdamW\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer,TrainerCallback\n",
    "import glob\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from transformers import EvalPrediction   \n",
    "import copy\n",
    "from typing import Optional\n",
    "from torch import FloatTensor\n",
    "from torch.nn import BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8380697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb30b595",
   "metadata": {},
   "source": [
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9fc1af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_path = 'data/clean/'\n",
    "file_pattern = folder_path + '*.csv'\n",
    "csv_files = glob.glob(file_pattern)\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    if 'train' in csv_file:\n",
    "        df_train = pd.read_csv(csv_file)\n",
    "    elif 'val' in csv_file:\n",
    "        df_val = pd.read_csv(csv_file)\n",
    "    else:\n",
    "        df_test = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d73c9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my favourite food is anything i did not have t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>now if he does off himself everyone will think...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why the fuck is bayless isoing</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to make her feel threatened</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dirty southern wankers</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  admiration  amusement  \\\n",
       "0  my favourite food is anything i did not have t...           0          0   \n",
       "1  now if he does off himself everyone will think...           0          0   \n",
       "2                     why the fuck is bayless isoing           0          0   \n",
       "3                        to make her feel threatened           0          0   \n",
       "4                             dirty southern wankers           0          0   \n",
       "\n",
       "   anger  annoyance  approval  caring  confusion  curiosity  desire  ...  \\\n",
       "0      0          0         0       0          0          0       0  ...   \n",
       "1      0          0         0       0          0          0       0  ...   \n",
       "2      1          0         0       0          0          0       0  ...   \n",
       "3      0          0         0       0          0          0       0  ...   \n",
       "4      0          1         0       0          0          0       0  ...   \n",
       "\n",
       "   love  nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0     0            0         0      0            0       0        0        0   \n",
       "1     0            0         0      0            0       0        0        0   \n",
       "2     0            0         0      0            0       0        0        0   \n",
       "3     0            0         0      0            0       0        0        0   \n",
       "4     0            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  neutral  \n",
       "0         0        1  \n",
       "1         0        1  \n",
       "2         0        0  \n",
       "3         0        0  \n",
       "4         0        0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaa441b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30bbe2a",
   "metadata": {},
   "source": [
    "## 2. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46926bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections of config\n",
    "\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 312\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87038cff",
   "metadata": {},
   "source": [
    "#### Storing all 28 labels into variable target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4af580be",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [col for col in df_train.columns if col not in ['clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43261378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b7c54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ffe4fe",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb3d783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the training set\n",
    "train_encodings = tokenizer(list(df_train['clean_text']), padding=True, truncation=True, return_tensors='pt')\n",
    "train_dataset = Dataset.from_dict({'input_ids': train_encodings['input_ids'],\n",
    "                                   'attention_mask': train_encodings['attention_mask'],\n",
    "                                   'labels': torch.tensor(df_train[target_cols].values, dtype=torch.float32)})\n",
    "\n",
    "# Tokenize the validation set\n",
    "val_encodings = tokenizer(list(df_val['clean_text']), padding=True, truncation=True, return_tensors='pt')\n",
    "valid_dataset = Dataset.from_dict({'input_ids': val_encodings['input_ids'],\n",
    "                                 'attention_mask': val_encodings['attention_mask'],\n",
    "                                 'labels': torch.tensor(df_val[target_cols].values, dtype=torch.float32)})\n",
    "\n",
    "\n",
    "test_encodings = tokenizer(list(df_test['clean_text']), padding=True, truncation=True, return_tensors='pt')\n",
    "test_dataset = Dataset.from_dict({'input_ids': test_encodings['input_ids'],\n",
    "                                 'attention_mask': test_encodings['attention_mask'],\n",
    "                                 'labels': torch.tensor(df_test[target_cols].values, dtype=torch.float32)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ae4840",
   "metadata": {},
   "source": [
    "### Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0442a1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# download model from model hub\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(target_cols))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e91901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom callback to get train and validation info during training\n",
    "class CustomCallback(TrainerCallback):\n",
    "    \n",
    "    def __init__(self, trainer) -> None:\n",
    "        super().__init__()\n",
    "        self._trainer = trainer\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if control.should_evaluate:\n",
    "            control_copy = copy.deepcopy(control)\n",
    "            self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\")\n",
    "            return control_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb5596",
   "metadata": {},
   "source": [
    "#### Compute class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cdb39da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "num_classes = len(target_cols)\n",
    "class_counts = np.sum(train_dataset['labels'], axis=0)\n",
    "class_weights = 1.0 / class_counts\n",
    "\n",
    "# Normalize weights\n",
    "class_weights /= class_weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82eeccf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00434095, 0.00770108, 0.01144105, 0.00725835, 0.00610007,\n",
       "       0.01649321, 0.01310535, 0.00818262, 0.02796899, 0.0141389 ,\n",
       "       0.00886653, 0.02260797, 0.05936463, 0.02101773, 0.03013129,\n",
       "       0.00673483, 0.23283272, 0.01234719, 0.0085945 , 0.10998846,\n",
       "       0.01133973, 0.16151459, 0.01615146, 0.11717725, 0.03289563,\n",
       "       0.01353066, 0.01691332, 0.00126095])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "611e9804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom trainer to incorporate class weights\n",
    "class MultiLabelTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights: Optional[FloatTensor] = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            class_weights = class_weights.to(self.args.device)\n",
    "            logging.info(f\"Using multi-label classification with class weights\", class_weights)\n",
    "        self.loss_fct = BCEWithLogitsLoss(weight=class_weights)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        try:\n",
    "            loss = self.loss_fct(outputs.logits.view(-1, model.num_labels), labels.view(-1,model.num_labels))\n",
    "        except AttributeError:  # DataParallel\n",
    "            loss = self.loss_fct(outputs.logits.view(-1, model.module.num_labels), labels.view(-1, model.num_labels))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1980801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='data/output/',\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=VALID_BATCH_SIZE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    logging_dir=\"data/output/logs\",\n",
    "    learning_rate=float(LEARNING_RATE),\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0eae176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom function to calculate the metrics for multi label classification\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'recall' : recall_micro,\n",
    "               'precision': precision_micro,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3287773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Trainer instance\n",
    "trainer = MultiLabelTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.add_callback(CustomCallback(trainer)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e1c6498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aishah/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13565' max='13565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13565/13565 1:59:59, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.096100</td>\n",
       "      <td>0.086644</td>\n",
       "      <td>0.551453</td>\n",
       "      <td>0.442100</td>\n",
       "      <td>0.732680</td>\n",
       "      <td>0.717511</td>\n",
       "      <td>0.420291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.096100</td>\n",
       "      <td>0.090069</td>\n",
       "      <td>0.536499</td>\n",
       "      <td>0.428527</td>\n",
       "      <td>0.717209</td>\n",
       "      <td>0.710560</td>\n",
       "      <td>0.408035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>0.073704</td>\n",
       "      <td>0.632315</td>\n",
       "      <td>0.539875</td>\n",
       "      <td>0.762950</td>\n",
       "      <td>0.766257</td>\n",
       "      <td>0.502419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>0.085520</td>\n",
       "      <td>0.573798</td>\n",
       "      <td>0.488088</td>\n",
       "      <td>0.696021</td>\n",
       "      <td>0.739372</td>\n",
       "      <td>0.449318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.076800</td>\n",
       "      <td>0.062126</td>\n",
       "      <td>0.703585</td>\n",
       "      <td>0.624988</td>\n",
       "      <td>0.804793</td>\n",
       "      <td>0.809167</td>\n",
       "      <td>0.581483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.076800</td>\n",
       "      <td>0.084707</td>\n",
       "      <td>0.578203</td>\n",
       "      <td>0.506426</td>\n",
       "      <td>0.673686</td>\n",
       "      <td>0.747837</td>\n",
       "      <td>0.459639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.065300</td>\n",
       "      <td>0.051391</td>\n",
       "      <td>0.769872</td>\n",
       "      <td>0.701431</td>\n",
       "      <td>0.853113</td>\n",
       "      <td>0.848065</td>\n",
       "      <td>0.655847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.065300</td>\n",
       "      <td>0.088195</td>\n",
       "      <td>0.573957</td>\n",
       "      <td>0.513323</td>\n",
       "      <td>0.650835</td>\n",
       "      <td>0.750626</td>\n",
       "      <td>0.459823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.054300</td>\n",
       "      <td>0.045781</td>\n",
       "      <td>0.801975</td>\n",
       "      <td>0.742333</td>\n",
       "      <td>0.872037</td>\n",
       "      <td>0.868776</td>\n",
       "      <td>0.697107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.054300</td>\n",
       "      <td>0.091490</td>\n",
       "      <td>0.573618</td>\n",
       "      <td>0.522100</td>\n",
       "      <td>0.636416</td>\n",
       "      <td>0.754513</td>\n",
       "      <td>0.462219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13565, training_loss=0.07920859592056696, metrics={'train_runtime': 7201.134, 'train_samples_per_second': 30.14, 'train_steps_per_second': 1.884, 'total_flos': 2.320457522193408e+16, 'train_loss': 0.07920859592056696, 'epoch': 5.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start the training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9dffa6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1566</td>\n",
       "      <td>4.815702e-05</td>\n",
       "      <td>0.18</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1162</td>\n",
       "      <td>4.631404e-05</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1071</td>\n",
       "      <td>4.447107e-05</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0991</td>\n",
       "      <td>4.262809e-05</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0961</td>\n",
       "      <td>4.078511e-05</td>\n",
       "      <td>0.92</td>\n",
       "      <td>2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2713</td>\n",
       "      <td>0.086644</td>\n",
       "      <td>0.551453</td>\n",
       "      <td>0.442100</td>\n",
       "      <td>0.732680</td>\n",
       "      <td>0.717511</td>\n",
       "      <td>0.420291</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090069</td>\n",
       "      <td>0.536499</td>\n",
       "      <td>0.428527</td>\n",
       "      <td>0.717209</td>\n",
       "      <td>0.710560</td>\n",
       "      <td>0.408035</td>\n",
       "      <td>21.4140</td>\n",
       "      <td>253.386</td>\n",
       "      <td>15.877</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0910</td>\n",
       "      <td>3.894213e-05</td>\n",
       "      <td>1.11</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0885</td>\n",
       "      <td>3.709915e-05</td>\n",
       "      <td>1.29</td>\n",
       "      <td>3500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0872</td>\n",
       "      <td>3.525617e-05</td>\n",
       "      <td>1.47</td>\n",
       "      <td>4000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0848</td>\n",
       "      <td>3.341320e-05</td>\n",
       "      <td>1.66</td>\n",
       "      <td>4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0860</td>\n",
       "      <td>3.157022e-05</td>\n",
       "      <td>1.84</td>\n",
       "      <td>5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5426</td>\n",
       "      <td>0.073704</td>\n",
       "      <td>0.632315</td>\n",
       "      <td>0.539875</td>\n",
       "      <td>0.762950</td>\n",
       "      <td>0.766257</td>\n",
       "      <td>0.502419</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085520</td>\n",
       "      <td>0.573798</td>\n",
       "      <td>0.488088</td>\n",
       "      <td>0.696021</td>\n",
       "      <td>0.739372</td>\n",
       "      <td>0.449318</td>\n",
       "      <td>20.3191</td>\n",
       "      <td>267.039</td>\n",
       "      <td>16.733</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0827</td>\n",
       "      <td>2.972724e-05</td>\n",
       "      <td>2.03</td>\n",
       "      <td>5500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0768</td>\n",
       "      <td>2.788426e-05</td>\n",
       "      <td>2.21</td>\n",
       "      <td>6000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0751</td>\n",
       "      <td>2.604128e-05</td>\n",
       "      <td>2.40</td>\n",
       "      <td>6500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>2.419830e-05</td>\n",
       "      <td>2.58</td>\n",
       "      <td>7000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0743</td>\n",
       "      <td>2.235533e-05</td>\n",
       "      <td>2.76</td>\n",
       "      <td>7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0768</td>\n",
       "      <td>2.051235e-05</td>\n",
       "      <td>2.95</td>\n",
       "      <td>8000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.062126</td>\n",
       "      <td>0.703585</td>\n",
       "      <td>0.624988</td>\n",
       "      <td>0.804793</td>\n",
       "      <td>0.809167</td>\n",
       "      <td>0.581483</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>8139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084707</td>\n",
       "      <td>0.578203</td>\n",
       "      <td>0.506426</td>\n",
       "      <td>0.673686</td>\n",
       "      <td>0.747837</td>\n",
       "      <td>0.459639</td>\n",
       "      <td>21.2268</td>\n",
       "      <td>255.621</td>\n",
       "      <td>16.018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0675</td>\n",
       "      <td>1.866937e-05</td>\n",
       "      <td>3.13</td>\n",
       "      <td>8500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0646</td>\n",
       "      <td>1.682639e-05</td>\n",
       "      <td>3.32</td>\n",
       "      <td>9000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0654</td>\n",
       "      <td>1.498341e-05</td>\n",
       "      <td>3.50</td>\n",
       "      <td>9500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0646</td>\n",
       "      <td>1.314043e-05</td>\n",
       "      <td>3.69</td>\n",
       "      <td>10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0653</td>\n",
       "      <td>1.129746e-05</td>\n",
       "      <td>3.87</td>\n",
       "      <td>10500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.00</td>\n",
       "      <td>10852</td>\n",
       "      <td>0.051391</td>\n",
       "      <td>0.769872</td>\n",
       "      <td>0.701431</td>\n",
       "      <td>0.853113</td>\n",
       "      <td>0.848065</td>\n",
       "      <td>0.655847</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.00</td>\n",
       "      <td>10852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088195</td>\n",
       "      <td>0.573957</td>\n",
       "      <td>0.513323</td>\n",
       "      <td>0.650835</td>\n",
       "      <td>0.750626</td>\n",
       "      <td>0.459823</td>\n",
       "      <td>20.5626</td>\n",
       "      <td>263.877</td>\n",
       "      <td>16.535</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0623</td>\n",
       "      <td>9.454478e-06</td>\n",
       "      <td>4.05</td>\n",
       "      <td>11000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0565</td>\n",
       "      <td>7.611500e-06</td>\n",
       "      <td>4.24</td>\n",
       "      <td>11500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0558</td>\n",
       "      <td>5.768522e-06</td>\n",
       "      <td>4.42</td>\n",
       "      <td>12000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0549</td>\n",
       "      <td>3.925544e-06</td>\n",
       "      <td>4.61</td>\n",
       "      <td>12500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0561</td>\n",
       "      <td>2.082565e-06</td>\n",
       "      <td>4.79</td>\n",
       "      <td>13000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0543</td>\n",
       "      <td>2.395872e-07</td>\n",
       "      <td>4.98</td>\n",
       "      <td>13500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.045781</td>\n",
       "      <td>0.801975</td>\n",
       "      <td>0.742333</td>\n",
       "      <td>0.872037</td>\n",
       "      <td>0.868776</td>\n",
       "      <td>0.697107</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091490</td>\n",
       "      <td>0.573618</td>\n",
       "      <td>0.522100</td>\n",
       "      <td>0.636416</td>\n",
       "      <td>0.754513</td>\n",
       "      <td>0.462219</td>\n",
       "      <td>19.9316</td>\n",
       "      <td>272.231</td>\n",
       "      <td>17.058</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.079209</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.320458e+16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      loss  learning_rate  epoch   step  train_loss  train_f1  train_recall  \\\n",
       "0   0.1566   4.815702e-05   0.18    500         NaN       NaN           NaN   \n",
       "1   0.1162   4.631404e-05   0.37   1000         NaN       NaN           NaN   \n",
       "2   0.1071   4.447107e-05   0.55   1500         NaN       NaN           NaN   \n",
       "3   0.0991   4.262809e-05   0.74   2000         NaN       NaN           NaN   \n",
       "4   0.0961   4.078511e-05   0.92   2500         NaN       NaN           NaN   \n",
       "5      NaN            NaN   1.00   2713    0.086644  0.551453      0.442100   \n",
       "6      NaN            NaN   1.00   2713         NaN       NaN           NaN   \n",
       "7   0.0910   3.894213e-05   1.11   3000         NaN       NaN           NaN   \n",
       "8   0.0885   3.709915e-05   1.29   3500         NaN       NaN           NaN   \n",
       "9   0.0872   3.525617e-05   1.47   4000         NaN       NaN           NaN   \n",
       "10  0.0848   3.341320e-05   1.66   4500         NaN       NaN           NaN   \n",
       "11  0.0860   3.157022e-05   1.84   5000         NaN       NaN           NaN   \n",
       "12     NaN            NaN   2.00   5426    0.073704  0.632315      0.539875   \n",
       "13     NaN            NaN   2.00   5426         NaN       NaN           NaN   \n",
       "14  0.0827   2.972724e-05   2.03   5500         NaN       NaN           NaN   \n",
       "15  0.0768   2.788426e-05   2.21   6000         NaN       NaN           NaN   \n",
       "16  0.0751   2.604128e-05   2.40   6500         NaN       NaN           NaN   \n",
       "17  0.0762   2.419830e-05   2.58   7000         NaN       NaN           NaN   \n",
       "18  0.0743   2.235533e-05   2.76   7500         NaN       NaN           NaN   \n",
       "19  0.0768   2.051235e-05   2.95   8000         NaN       NaN           NaN   \n",
       "20     NaN            NaN   3.00   8139    0.062126  0.703585      0.624988   \n",
       "21     NaN            NaN   3.00   8139         NaN       NaN           NaN   \n",
       "22  0.0675   1.866937e-05   3.13   8500         NaN       NaN           NaN   \n",
       "23  0.0646   1.682639e-05   3.32   9000         NaN       NaN           NaN   \n",
       "24  0.0654   1.498341e-05   3.50   9500         NaN       NaN           NaN   \n",
       "25  0.0646   1.314043e-05   3.69  10000         NaN       NaN           NaN   \n",
       "26  0.0653   1.129746e-05   3.87  10500         NaN       NaN           NaN   \n",
       "27     NaN            NaN   4.00  10852    0.051391  0.769872      0.701431   \n",
       "28     NaN            NaN   4.00  10852         NaN       NaN           NaN   \n",
       "29  0.0623   9.454478e-06   4.05  11000         NaN       NaN           NaN   \n",
       "30  0.0565   7.611500e-06   4.24  11500         NaN       NaN           NaN   \n",
       "31  0.0558   5.768522e-06   4.42  12000         NaN       NaN           NaN   \n",
       "32  0.0549   3.925544e-06   4.61  12500         NaN       NaN           NaN   \n",
       "33  0.0561   2.082565e-06   4.79  13000         NaN       NaN           NaN   \n",
       "34  0.0543   2.395872e-07   4.98  13500         NaN       NaN           NaN   \n",
       "35     NaN            NaN   5.00  13565    0.045781  0.801975      0.742333   \n",
       "36     NaN            NaN   5.00  13565         NaN       NaN           NaN   \n",
       "37     NaN            NaN   5.00  13565    0.079209       NaN           NaN   \n",
       "\n",
       "    train_precision  train_roc_auc  train_accuracy  ...  eval_loss   eval_f1  \\\n",
       "0               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "1               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "2               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "3               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "4               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "5          0.732680       0.717511        0.420291  ...        NaN       NaN   \n",
       "6               NaN            NaN             NaN  ...   0.090069  0.536499   \n",
       "7               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "8               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "9               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "10              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "11              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "12         0.762950       0.766257        0.502419  ...        NaN       NaN   \n",
       "13              NaN            NaN             NaN  ...   0.085520  0.573798   \n",
       "14              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "15              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "16              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "17              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "18              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "19              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "20         0.804793       0.809167        0.581483  ...        NaN       NaN   \n",
       "21              NaN            NaN             NaN  ...   0.084707  0.578203   \n",
       "22              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "23              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "24              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "25              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "26              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "27         0.853113       0.848065        0.655847  ...        NaN       NaN   \n",
       "28              NaN            NaN             NaN  ...   0.088195  0.573957   \n",
       "29              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "30              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "31              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "32              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "33              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "34              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "35         0.872037       0.868776        0.697107  ...        NaN       NaN   \n",
       "36              NaN            NaN             NaN  ...   0.091490  0.573618   \n",
       "37              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "\n",
       "    eval_recall  eval_precision  eval_roc_auc  eval_accuracy  eval_runtime  \\\n",
       "0           NaN             NaN           NaN            NaN           NaN   \n",
       "1           NaN             NaN           NaN            NaN           NaN   \n",
       "2           NaN             NaN           NaN            NaN           NaN   \n",
       "3           NaN             NaN           NaN            NaN           NaN   \n",
       "4           NaN             NaN           NaN            NaN           NaN   \n",
       "5           NaN             NaN           NaN            NaN           NaN   \n",
       "6      0.428527        0.717209      0.710560       0.408035       21.4140   \n",
       "7           NaN             NaN           NaN            NaN           NaN   \n",
       "8           NaN             NaN           NaN            NaN           NaN   \n",
       "9           NaN             NaN           NaN            NaN           NaN   \n",
       "10          NaN             NaN           NaN            NaN           NaN   \n",
       "11          NaN             NaN           NaN            NaN           NaN   \n",
       "12          NaN             NaN           NaN            NaN           NaN   \n",
       "13     0.488088        0.696021      0.739372       0.449318       20.3191   \n",
       "14          NaN             NaN           NaN            NaN           NaN   \n",
       "15          NaN             NaN           NaN            NaN           NaN   \n",
       "16          NaN             NaN           NaN            NaN           NaN   \n",
       "17          NaN             NaN           NaN            NaN           NaN   \n",
       "18          NaN             NaN           NaN            NaN           NaN   \n",
       "19          NaN             NaN           NaN            NaN           NaN   \n",
       "20          NaN             NaN           NaN            NaN           NaN   \n",
       "21     0.506426        0.673686      0.747837       0.459639       21.2268   \n",
       "22          NaN             NaN           NaN            NaN           NaN   \n",
       "23          NaN             NaN           NaN            NaN           NaN   \n",
       "24          NaN             NaN           NaN            NaN           NaN   \n",
       "25          NaN             NaN           NaN            NaN           NaN   \n",
       "26          NaN             NaN           NaN            NaN           NaN   \n",
       "27          NaN             NaN           NaN            NaN           NaN   \n",
       "28     0.513323        0.650835      0.750626       0.459823       20.5626   \n",
       "29          NaN             NaN           NaN            NaN           NaN   \n",
       "30          NaN             NaN           NaN            NaN           NaN   \n",
       "31          NaN             NaN           NaN            NaN           NaN   \n",
       "32          NaN             NaN           NaN            NaN           NaN   \n",
       "33          NaN             NaN           NaN            NaN           NaN   \n",
       "34          NaN             NaN           NaN            NaN           NaN   \n",
       "35          NaN             NaN           NaN            NaN           NaN   \n",
       "36     0.522100        0.636416      0.754513       0.462219       19.9316   \n",
       "37          NaN             NaN           NaN            NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second    total_flos  \n",
       "0                       NaN                    NaN           NaN  \n",
       "1                       NaN                    NaN           NaN  \n",
       "2                       NaN                    NaN           NaN  \n",
       "3                       NaN                    NaN           NaN  \n",
       "4                       NaN                    NaN           NaN  \n",
       "5                       NaN                    NaN           NaN  \n",
       "6                   253.386                 15.877           NaN  \n",
       "7                       NaN                    NaN           NaN  \n",
       "8                       NaN                    NaN           NaN  \n",
       "9                       NaN                    NaN           NaN  \n",
       "10                      NaN                    NaN           NaN  \n",
       "11                      NaN                    NaN           NaN  \n",
       "12                      NaN                    NaN           NaN  \n",
       "13                  267.039                 16.733           NaN  \n",
       "14                      NaN                    NaN           NaN  \n",
       "15                      NaN                    NaN           NaN  \n",
       "16                      NaN                    NaN           NaN  \n",
       "17                      NaN                    NaN           NaN  \n",
       "18                      NaN                    NaN           NaN  \n",
       "19                      NaN                    NaN           NaN  \n",
       "20                      NaN                    NaN           NaN  \n",
       "21                  255.621                 16.018           NaN  \n",
       "22                      NaN                    NaN           NaN  \n",
       "23                      NaN                    NaN           NaN  \n",
       "24                      NaN                    NaN           NaN  \n",
       "25                      NaN                    NaN           NaN  \n",
       "26                      NaN                    NaN           NaN  \n",
       "27                      NaN                    NaN           NaN  \n",
       "28                  263.877                 16.535           NaN  \n",
       "29                      NaN                    NaN           NaN  \n",
       "30                      NaN                    NaN           NaN  \n",
       "31                      NaN                    NaN           NaN  \n",
       "32                      NaN                    NaN           NaN  \n",
       "33                      NaN                    NaN           NaN  \n",
       "34                      NaN                    NaN           NaN  \n",
       "35                      NaN                    NaN           NaN  \n",
       "36                  272.231                 17.058           NaN  \n",
       "37                      NaN                    NaN  2.320458e+16  \n",
       "\n",
       "[38 rows x 23 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view train and validation metrics from training\n",
    "log_history = pd.DataFrame(trainer.state.log_history)\n",
    "log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf0d198a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2713</td>\n",
       "      <td>0.086644</td>\n",
       "      <td>0.551453</td>\n",
       "      <td>0.442100</td>\n",
       "      <td>0.732680</td>\n",
       "      <td>0.717511</td>\n",
       "      <td>0.420291</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5426</td>\n",
       "      <td>0.073704</td>\n",
       "      <td>0.632315</td>\n",
       "      <td>0.539875</td>\n",
       "      <td>0.762950</td>\n",
       "      <td>0.766257</td>\n",
       "      <td>0.502419</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.062126</td>\n",
       "      <td>0.703585</td>\n",
       "      <td>0.624988</td>\n",
       "      <td>0.804793</td>\n",
       "      <td>0.809167</td>\n",
       "      <td>0.581483</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10852</td>\n",
       "      <td>0.051391</td>\n",
       "      <td>0.769872</td>\n",
       "      <td>0.701431</td>\n",
       "      <td>0.853113</td>\n",
       "      <td>0.848065</td>\n",
       "      <td>0.655847</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.045781</td>\n",
       "      <td>0.801975</td>\n",
       "      <td>0.742333</td>\n",
       "      <td>0.872037</td>\n",
       "      <td>0.868776</td>\n",
       "      <td>0.697107</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    loss  learning_rate  epoch   step  train_loss  train_f1  train_recall  \\\n",
       "5    NaN            NaN    1.0   2713    0.086644  0.551453      0.442100   \n",
       "12   NaN            NaN    2.0   5426    0.073704  0.632315      0.539875   \n",
       "20   NaN            NaN    3.0   8139    0.062126  0.703585      0.624988   \n",
       "27   NaN            NaN    4.0  10852    0.051391  0.769872      0.701431   \n",
       "35   NaN            NaN    5.0  13565    0.045781  0.801975      0.742333   \n",
       "\n",
       "    train_precision  train_roc_auc  train_accuracy  ...  eval_loss  eval_f1  \\\n",
       "5          0.732680       0.717511        0.420291  ...        NaN      NaN   \n",
       "12         0.762950       0.766257        0.502419  ...        NaN      NaN   \n",
       "20         0.804793       0.809167        0.581483  ...        NaN      NaN   \n",
       "27         0.853113       0.848065        0.655847  ...        NaN      NaN   \n",
       "35         0.872037       0.868776        0.697107  ...        NaN      NaN   \n",
       "\n",
       "    eval_recall  eval_precision  eval_roc_auc  eval_accuracy  eval_runtime  \\\n",
       "5           NaN             NaN           NaN            NaN           NaN   \n",
       "12          NaN             NaN           NaN            NaN           NaN   \n",
       "20          NaN             NaN           NaN            NaN           NaN   \n",
       "27          NaN             NaN           NaN            NaN           NaN   \n",
       "35          NaN             NaN           NaN            NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  total_flos  \n",
       "5                       NaN                    NaN         NaN  \n",
       "12                      NaN                    NaN         NaN  \n",
       "20                      NaN                    NaN         NaN  \n",
       "27                      NaN                    NaN         NaN  \n",
       "35                      NaN                    NaN         NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store train metrics in dataframe\n",
    "train_history = log_history[log_history['train_f1'].notna()]\n",
    "train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97eea248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090069</td>\n",
       "      <td>0.536499</td>\n",
       "      <td>0.428527</td>\n",
       "      <td>0.717209</td>\n",
       "      <td>0.710560</td>\n",
       "      <td>0.408035</td>\n",
       "      <td>21.4140</td>\n",
       "      <td>253.386</td>\n",
       "      <td>15.877</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085520</td>\n",
       "      <td>0.573798</td>\n",
       "      <td>0.488088</td>\n",
       "      <td>0.696021</td>\n",
       "      <td>0.739372</td>\n",
       "      <td>0.449318</td>\n",
       "      <td>20.3191</td>\n",
       "      <td>267.039</td>\n",
       "      <td>16.733</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084707</td>\n",
       "      <td>0.578203</td>\n",
       "      <td>0.506426</td>\n",
       "      <td>0.673686</td>\n",
       "      <td>0.747837</td>\n",
       "      <td>0.459639</td>\n",
       "      <td>21.2268</td>\n",
       "      <td>255.621</td>\n",
       "      <td>16.018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088195</td>\n",
       "      <td>0.573957</td>\n",
       "      <td>0.513323</td>\n",
       "      <td>0.650835</td>\n",
       "      <td>0.750626</td>\n",
       "      <td>0.459823</td>\n",
       "      <td>20.5626</td>\n",
       "      <td>263.877</td>\n",
       "      <td>16.535</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091490</td>\n",
       "      <td>0.573618</td>\n",
       "      <td>0.522100</td>\n",
       "      <td>0.636416</td>\n",
       "      <td>0.754513</td>\n",
       "      <td>0.462219</td>\n",
       "      <td>19.9316</td>\n",
       "      <td>272.231</td>\n",
       "      <td>17.058</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    loss  learning_rate  epoch   step  train_loss  train_f1  train_recall  \\\n",
       "6    NaN            NaN    1.0   2713         NaN       NaN           NaN   \n",
       "13   NaN            NaN    2.0   5426         NaN       NaN           NaN   \n",
       "21   NaN            NaN    3.0   8139         NaN       NaN           NaN   \n",
       "28   NaN            NaN    4.0  10852         NaN       NaN           NaN   \n",
       "36   NaN            NaN    5.0  13565         NaN       NaN           NaN   \n",
       "\n",
       "    train_precision  train_roc_auc  train_accuracy  ...  eval_loss   eval_f1  \\\n",
       "6               NaN            NaN             NaN  ...   0.090069  0.536499   \n",
       "13              NaN            NaN             NaN  ...   0.085520  0.573798   \n",
       "21              NaN            NaN             NaN  ...   0.084707  0.578203   \n",
       "28              NaN            NaN             NaN  ...   0.088195  0.573957   \n",
       "36              NaN            NaN             NaN  ...   0.091490  0.573618   \n",
       "\n",
       "    eval_recall  eval_precision  eval_roc_auc  eval_accuracy  eval_runtime  \\\n",
       "6      0.428527        0.717209      0.710560       0.408035       21.4140   \n",
       "13     0.488088        0.696021      0.739372       0.449318       20.3191   \n",
       "21     0.506426        0.673686      0.747837       0.459639       21.2268   \n",
       "28     0.513323        0.650835      0.750626       0.459823       20.5626   \n",
       "36     0.522100        0.636416      0.754513       0.462219       19.9316   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  total_flos  \n",
       "6                   253.386                 15.877         NaN  \n",
       "13                  267.039                 16.733         NaN  \n",
       "21                  255.621                 16.018         NaN  \n",
       "28                  263.877                 16.535         NaN  \n",
       "36                  272.231                 17.058         NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store validation metrics in dataframe\n",
    "val_history = log_history[log_history['eval_f1'].notna()]\n",
    "val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "978bf0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_x</th>\n",
       "      <th>learning_rate_x</th>\n",
       "      <th>epoch_x</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss_x</th>\n",
       "      <th>train_f1_x</th>\n",
       "      <th>train_recall_x</th>\n",
       "      <th>train_precision_x</th>\n",
       "      <th>train_roc_auc_x</th>\n",
       "      <th>train_accuracy_x</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss_y</th>\n",
       "      <th>eval_f1_y</th>\n",
       "      <th>eval_recall_y</th>\n",
       "      <th>eval_precision_y</th>\n",
       "      <th>eval_roc_auc_y</th>\n",
       "      <th>eval_accuracy_y</th>\n",
       "      <th>eval_runtime_y</th>\n",
       "      <th>eval_samples_per_second_y</th>\n",
       "      <th>eval_steps_per_second_y</th>\n",
       "      <th>total_flos_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2713</td>\n",
       "      <td>0.086644</td>\n",
       "      <td>0.551453</td>\n",
       "      <td>0.442100</td>\n",
       "      <td>0.732680</td>\n",
       "      <td>0.717511</td>\n",
       "      <td>0.420291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090069</td>\n",
       "      <td>0.536499</td>\n",
       "      <td>0.428527</td>\n",
       "      <td>0.717209</td>\n",
       "      <td>0.710560</td>\n",
       "      <td>0.408035</td>\n",
       "      <td>21.4140</td>\n",
       "      <td>253.386</td>\n",
       "      <td>15.877</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5426</td>\n",
       "      <td>0.073704</td>\n",
       "      <td>0.632315</td>\n",
       "      <td>0.539875</td>\n",
       "      <td>0.762950</td>\n",
       "      <td>0.766257</td>\n",
       "      <td>0.502419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085520</td>\n",
       "      <td>0.573798</td>\n",
       "      <td>0.488088</td>\n",
       "      <td>0.696021</td>\n",
       "      <td>0.739372</td>\n",
       "      <td>0.449318</td>\n",
       "      <td>20.3191</td>\n",
       "      <td>267.039</td>\n",
       "      <td>16.733</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.062126</td>\n",
       "      <td>0.703585</td>\n",
       "      <td>0.624988</td>\n",
       "      <td>0.804793</td>\n",
       "      <td>0.809167</td>\n",
       "      <td>0.581483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084707</td>\n",
       "      <td>0.578203</td>\n",
       "      <td>0.506426</td>\n",
       "      <td>0.673686</td>\n",
       "      <td>0.747837</td>\n",
       "      <td>0.459639</td>\n",
       "      <td>21.2268</td>\n",
       "      <td>255.621</td>\n",
       "      <td>16.018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10852</td>\n",
       "      <td>0.051391</td>\n",
       "      <td>0.769872</td>\n",
       "      <td>0.701431</td>\n",
       "      <td>0.853113</td>\n",
       "      <td>0.848065</td>\n",
       "      <td>0.655847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088195</td>\n",
       "      <td>0.573957</td>\n",
       "      <td>0.513323</td>\n",
       "      <td>0.650835</td>\n",
       "      <td>0.750626</td>\n",
       "      <td>0.459823</td>\n",
       "      <td>20.5626</td>\n",
       "      <td>263.877</td>\n",
       "      <td>16.535</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.045781</td>\n",
       "      <td>0.801975</td>\n",
       "      <td>0.742333</td>\n",
       "      <td>0.872037</td>\n",
       "      <td>0.868776</td>\n",
       "      <td>0.697107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091490</td>\n",
       "      <td>0.573618</td>\n",
       "      <td>0.522100</td>\n",
       "      <td>0.636416</td>\n",
       "      <td>0.754513</td>\n",
       "      <td>0.462219</td>\n",
       "      <td>19.9316</td>\n",
       "      <td>272.231</td>\n",
       "      <td>17.058</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loss_x  learning_rate_x  epoch_x   step  train_loss_x  train_f1_x  \\\n",
       "0     NaN              NaN      1.0   2713      0.086644    0.551453   \n",
       "1     NaN              NaN      2.0   5426      0.073704    0.632315   \n",
       "2     NaN              NaN      3.0   8139      0.062126    0.703585   \n",
       "3     NaN              NaN      4.0  10852      0.051391    0.769872   \n",
       "4     NaN              NaN      5.0  13565      0.045781    0.801975   \n",
       "\n",
       "   train_recall_x  train_precision_x  train_roc_auc_x  train_accuracy_x  ...  \\\n",
       "0        0.442100           0.732680         0.717511          0.420291  ...   \n",
       "1        0.539875           0.762950         0.766257          0.502419  ...   \n",
       "2        0.624988           0.804793         0.809167          0.581483  ...   \n",
       "3        0.701431           0.853113         0.848065          0.655847  ...   \n",
       "4        0.742333           0.872037         0.868776          0.697107  ...   \n",
       "\n",
       "   eval_loss_y  eval_f1_y  eval_recall_y  eval_precision_y  eval_roc_auc_y  \\\n",
       "0     0.090069   0.536499       0.428527          0.717209        0.710560   \n",
       "1     0.085520   0.573798       0.488088          0.696021        0.739372   \n",
       "2     0.084707   0.578203       0.506426          0.673686        0.747837   \n",
       "3     0.088195   0.573957       0.513323          0.650835        0.750626   \n",
       "4     0.091490   0.573618       0.522100          0.636416        0.754513   \n",
       "\n",
       "   eval_accuracy_y  eval_runtime_y  eval_samples_per_second_y  \\\n",
       "0         0.408035         21.4140                    253.386   \n",
       "1         0.449318         20.3191                    267.039   \n",
       "2         0.459639         21.2268                    255.621   \n",
       "3         0.459823         20.5626                    263.877   \n",
       "4         0.462219         19.9316                    272.231   \n",
       "\n",
       "   eval_steps_per_second_y  total_flos_y  \n",
       "0                   15.877           NaN  \n",
       "1                   16.733           NaN  \n",
       "2                   16.018           NaN  \n",
       "3                   16.535           NaN  \n",
       "4                   17.058           NaN  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = pd.merge(train_history, val_history, on='step', how='outer')\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ebe0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting function to check for accuracy with graphs \n",
    "def plot_model_performance(history):\n",
    "\n",
    "    #getting train and validation accuracy\n",
    "    acc = history['train_accuracy_x']\n",
    "    val_acc = history['eval_accuracy_y']\n",
    "\n",
    "    #getting train and validation loss\n",
    "    loss = history['train_loss_x']\n",
    "    val_loss = history['eval_loss_y']\n",
    "\n",
    "    epochs_range = range(5)\n",
    "\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c97c350d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_model_performance(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc7fb89",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5456e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to evaluate the model on the test set\n",
    "def evaluate_model(test_dataset):\n",
    "    test_res = trainer.evaluate(eval_dataset=test_dataset)\n",
    "    print(test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7bb617c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0839780643582344, 'eval_f1': 0.5776892430278885, 'eval_recall': 0.5040290725233054, 'eval_precision': 0.6765641569459173, 'eval_roc_auc': 0.7467785566699561, 'eval_accuracy': 0.4630550948958909, 'eval_runtime': 16.5652, 'eval_samples_per_second': 327.615, 'eval_steps_per_second': 20.525, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c172305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate each emotion label metrics on test set\n",
    "def calc_label_metrics(label, y_targets, y_preds, threshold):\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"accuracy\": metrics.accuracy_score(y_targets, y_preds),\n",
    "        \"precision\": metrics.precision_score(y_targets, y_preds, zero_division=0),\n",
    "        \"recall\": metrics.recall_score(y_targets, y_preds, zero_division=0),\n",
    "        \"f1\": metrics.f1_score(y_targets, y_preds, zero_division=0),\n",
    "        \"mcc\": metrics.matthews_corrcoef(y_targets, y_preds),\n",
    "        \"support\": y_targets.sum(),\n",
    "        \"threshold\": threshold,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78c37071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate overall metric on test set\n",
    "def calc_test_metrics(trainer, test_dataset, target_cols):\n",
    "    y_test = trainer.predict(test_dataset)\n",
    "    threshold = 0.5\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(y_test.predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= 0.5)] = 1\n",
    "    \n",
    "    # finally, compute metrics\n",
    "    y_true = df_test[target_cols].values\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'recall' : recall_micro,\n",
    "               'precision': precision_micro,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    metrics_df = pd.DataFrame.from_dict(metrics, orient='index', columns=['Value'])\n",
    "\n",
    "    display(metrics_df)\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for label_index, label in enumerate(target_cols):\n",
    "        y_targets, y_preds = y_true[:, label_index], y_pred[:, label_index]\n",
    "        results.append(calc_label_metrics(label, y_targets, y_preds, threshold))\n",
    "\n",
    "    per_label_results = pd.DataFrame(results, index=target_cols)\n",
    "    display(per_label_results.drop(columns=[\"label\"]).round(3))\n",
    "    \n",
    "    return y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "794fec4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.577689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.504029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.676564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc</th>\n",
       "      <td>0.746779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.463055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "f1         0.577689\n",
       "recall     0.504029\n",
       "precision  0.676564\n",
       "roc_auc    0.746779\n",
       "accuracy   0.463055"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>mcc</th>\n",
       "      <th>support</th>\n",
       "      <th>threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.941</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.654</td>\n",
       "      <td>504</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.982</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.817</td>\n",
       "      <td>264</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.970</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.522</td>\n",
       "      <td>198</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.938</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.263</td>\n",
       "      <td>320</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.939</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.386</td>\n",
       "      <td>351</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.976</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.421</td>\n",
       "      <td>135</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.973</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.374</td>\n",
       "      <td>153</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.945</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.453</td>\n",
       "      <td>284</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.987</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.449</td>\n",
       "      <td>83</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.973</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.270</td>\n",
       "      <td>151</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.948</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.371</td>\n",
       "      <td>267</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.981</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.454</td>\n",
       "      <td>123</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.995</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.512</td>\n",
       "      <td>37</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.983</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.404</td>\n",
       "      <td>103</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.991</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.691</td>\n",
       "      <td>78</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.989</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.909</td>\n",
       "      <td>352</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.999</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.977</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.572</td>\n",
       "      <td>161</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.982</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.796</td>\n",
       "      <td>238</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.996</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.403</td>\n",
       "      <td>23</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.973</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.501</td>\n",
       "      <td>186</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.998</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.510</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.975</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.254</td>\n",
       "      <td>145</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.998</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.992</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.631</td>\n",
       "      <td>56</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.977</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.564</td>\n",
       "      <td>156</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.978</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.510</td>\n",
       "      <td>141</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.784</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.486</td>\n",
       "      <td>1787</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                accuracy  precision  recall     f1    mcc  support  threshold\n",
       "admiration         0.941      0.680   0.692  0.686  0.654      504        0.5\n",
       "amusement          0.982      0.780   0.875  0.825  0.817      264        0.5\n",
       "anger              0.970      0.628   0.460  0.531  0.522      198        0.5\n",
       "annoyance          0.938      0.448   0.188  0.264  0.263      320        0.5\n",
       "approval           0.939      0.553   0.311  0.398  0.386      351        0.5\n",
       "caring             0.976      0.527   0.356  0.425  0.421      135        0.5\n",
       "confusion          0.973      0.554   0.268  0.361  0.374      153        0.5\n",
       "curiosity          0.945      0.474   0.489  0.482  0.453      284        0.5\n",
       "desire             0.987      0.742   0.277  0.404  0.449       83        0.5\n",
       "disappointment     0.973      0.537   0.146  0.229  0.270      151        0.5\n",
       "disapproval        0.948      0.464   0.341  0.393  0.371      267        0.5\n",
       "disgust            0.981      0.672   0.317  0.431  0.454      123        0.5\n",
       "embarrassment      0.995      0.700   0.378  0.491  0.512       37        0.5\n",
       "excitement         0.983      0.667   0.252  0.366  0.404      103        0.5\n",
       "fear               0.991      0.663   0.731  0.695  0.691       78        0.5\n",
       "gratitude          0.989      0.957   0.875  0.914  0.909      352        0.5\n",
       "grief              0.999      0.000   0.000  0.000  0.000        6        0.5\n",
       "joy                0.977      0.637   0.534  0.581  0.572      161        0.5\n",
       "love               0.982      0.754   0.861  0.804  0.796      238        0.5\n",
       "nervousness        0.996      0.538   0.304  0.389  0.403       23        0.5\n",
       "optimism           0.973      0.728   0.360  0.482  0.501      186        0.5\n",
       "pride              0.998      0.833   0.312  0.455  0.510       16        0.5\n",
       "realization        0.975      0.700   0.097  0.170  0.254      145        0.5\n",
       "relief             0.998      0.000   0.000  0.000  0.000       11        0.5\n",
       "remorse            0.992      0.594   0.679  0.633  0.631       56        0.5\n",
       "sadness            0.977      0.601   0.551  0.575  0.564      156        0.5\n",
       "surprise           0.978      0.598   0.454  0.516  0.510      141        0.5\n",
       "neutral            0.784      0.731   0.543  0.623  0.486     1787        0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "targets, outputs = calc_test_metrics(trainer, test_dataset, target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f1631d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results DataFrame:\n",
      "         Actual                Predicted\n",
      "0     [sadness]          [love, sadness]\n",
      "1  [admiration]             [admiration]\n",
      "2  [excitement]                       []\n",
      "3   [gratitude]              [gratitude]\n",
      "4     [neutral]                [neutral]\n",
      "5   [gratitude]              [gratitude]\n",
      "6   [gratitude]              [gratitude]\n",
      "7   [gratitude]  [admiration, gratitude]\n",
      "8     [remorse]                [sadness]\n",
      "9     [sadness]                [sadness]\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to store actual labels and predicted labels\n",
    "final_df = pd.DataFrame({\n",
    "    'Actual': [list(np.where(targets[i])[0]) for i in range(len(targets))],\n",
    "    'Predicted': [list(np.where(outputs[i])[0]) for i in range(len(outputs))]\n",
    "})\n",
    "\n",
    "# Map label indices to label names in the 'Actual' column\n",
    "final_df['Actual'] = final_df['Actual'].apply(lambda indices: [target_cols[idx] for idx in indices])\n",
    "\n",
    "# Map label indices to label names in the 'Predicted' column\n",
    "final_df['Predicted'] = final_df['Predicted'].apply(lambda indices: [target_cols[idx] for idx in indices])\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(\"Results DataFrame:\")\n",
    "print(final_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "301ef201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the label DataFrame with the original DataFramev\n",
    "val_df_terms = df_test['clean_text']\n",
    "result_df = pd.concat([val_df_terms, final_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a335fd5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am really sorry about your situation frown s...</td>\n",
       "      <td>[sadness]</td>\n",
       "      <td>[love, sadness]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it is wonderful because it is awful at not with</td>\n",
       "      <td>[admiration]</td>\n",
       "      <td>[admiration]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kings fan here good luck to you guys will be a...</td>\n",
       "      <td>[excitement]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i did not know that thank you for teaching me ...</td>\n",
       "      <td>[gratitude]</td>\n",
       "      <td>[gratitude]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>they got bored from haunting earth for thousan...</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5422</th>\n",
       "      <td>thanks i was diagnosed with bp 1 after the hos...</td>\n",
       "      <td>[gratitude]</td>\n",
       "      <td>[gratitude]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5423</th>\n",
       "      <td>well that makes sense</td>\n",
       "      <td>[approval]</td>\n",
       "      <td>[approval]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5424</th>\n",
       "      <td>daddy issues name</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5425</th>\n",
       "      <td>so glad i discovered that subreddit a couple m...</td>\n",
       "      <td>[admiration]</td>\n",
       "      <td>[admiration, joy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5426</th>\n",
       "      <td>had to watch elmo in grouchland one time too m...</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5427 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_text        Actual  \\\n",
       "0     i am really sorry about your situation frown s...     [sadness]   \n",
       "1       it is wonderful because it is awful at not with  [admiration]   \n",
       "2     kings fan here good luck to you guys will be a...  [excitement]   \n",
       "3     i did not know that thank you for teaching me ...   [gratitude]   \n",
       "4     they got bored from haunting earth for thousan...     [neutral]   \n",
       "...                                                 ...           ...   \n",
       "5422  thanks i was diagnosed with bp 1 after the hos...   [gratitude]   \n",
       "5423                              well that makes sense    [approval]   \n",
       "5424                                  daddy issues name     [neutral]   \n",
       "5425  so glad i discovered that subreddit a couple m...  [admiration]   \n",
       "5426  had to watch elmo in grouchland one time too m...     [neutral]   \n",
       "\n",
       "              Predicted  \n",
       "0       [love, sadness]  \n",
       "1          [admiration]  \n",
       "2                    []  \n",
       "3           [gratitude]  \n",
       "4             [neutral]  \n",
       "...                 ...  \n",
       "5422        [gratitude]  \n",
       "5423         [approval]  \n",
       "5424          [neutral]  \n",
       "5425  [admiration, joy]  \n",
       "5426          [neutral]  \n",
       "\n",
       "[5427 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53492f7a",
   "metadata": {},
   "source": [
    "## 4. Save the output, tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5113b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('output_roberta_m1.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8b778c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer\n",
    "tokenizer.save_pretrained('./roberta_M1_transformer/')\n",
    "\n",
    "# Save model\n",
    "trainer.save_model('./roberta_M1_transformer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ba9b95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73edad27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
