{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a4b07d",
   "metadata": {},
   "source": [
    "# Roberta Base "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00c3d39",
   "metadata": {},
   "source": [
    "Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019c1d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn ,cuda\n",
    "from torch.utils.data import DataLoader,Dataset,RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "import nltk.corpus\n",
    "from sklearn import metrics\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, AutoTokenizer, BertModel, BertConfig, AutoModel, AdamW\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer,TrainerCallback\n",
    "import glob\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from transformers import EvalPrediction   \n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8380697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb30b595",
   "metadata": {},
   "source": [
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9fc1af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_path = 'data/clean/'\n",
    "file_pattern = folder_path + '*.csv'\n",
    "csv_files = glob.glob(file_pattern)\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    if 'train' in csv_file:\n",
    "        df_train = pd.read_csv(csv_file)\n",
    "    elif 'val' in csv_file:\n",
    "        df_val = pd.read_csv(csv_file)\n",
    "    else:\n",
    "        df_test = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d73c9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my favourite food is anything i did not have t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>now if he does off himself everyone will think...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why the fuck is bayless isoing</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to make her feel threatened</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dirty southern wankers</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  admiration  amusement  \\\n",
       "0  my favourite food is anything i did not have t...           0          0   \n",
       "1  now if he does off himself everyone will think...           0          0   \n",
       "2                     why the fuck is bayless isoing           0          0   \n",
       "3                        to make her feel threatened           0          0   \n",
       "4                             dirty southern wankers           0          0   \n",
       "\n",
       "   anger  annoyance  approval  caring  confusion  curiosity  desire  ...  \\\n",
       "0      0          0         0       0          0          0       0  ...   \n",
       "1      0          0         0       0          0          0       0  ...   \n",
       "2      1          0         0       0          0          0       0  ...   \n",
       "3      0          0         0       0          0          0       0  ...   \n",
       "4      0          1         0       0          0          0       0  ...   \n",
       "\n",
       "   love  nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0     0            0         0      0            0       0        0        0   \n",
       "1     0            0         0      0            0       0        0        0   \n",
       "2     0            0         0      0            0       0        0        0   \n",
       "3     0            0         0      0            0       0        0        0   \n",
       "4     0            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  neutral  \n",
       "0         0        1  \n",
       "1         0        1  \n",
       "2         0        0  \n",
       "3         0        0  \n",
       "4         0        0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaa441b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30bbe2a",
   "metadata": {},
   "source": [
    "## 2. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46926bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections of config\n",
    "\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 312\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87038cff",
   "metadata": {},
   "source": [
    "#### Storing all 28 labels into variable target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4af580be",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [col for col in df_train.columns if col not in ['clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43261378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b7c54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ffe4fe",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb3d783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the training set\n",
    "train_encodings = tokenizer(list(df_train['clean_text']), padding=True, truncation=True, return_tensors='pt')\n",
    "train_dataset = Dataset.from_dict({'input_ids': train_encodings['input_ids'],\n",
    "                                   'attention_mask': train_encodings['attention_mask'],\n",
    "                                   'labels': torch.tensor(df_train[target_cols].values, dtype=torch.float32)})\n",
    "\n",
    "# Tokenize the validation set\n",
    "val_encodings = tokenizer(list(df_val['clean_text']), padding=True, truncation=True, return_tensors='pt')\n",
    "valid_dataset = Dataset.from_dict({'input_ids': val_encodings['input_ids'],\n",
    "                                 'attention_mask': val_encodings['attention_mask'],\n",
    "                                 'labels': torch.tensor(df_val[target_cols].values, dtype=torch.float32)})\n",
    "\n",
    "\n",
    "test_encodings = tokenizer(list(df_test['clean_text']), padding=True, truncation=True, return_tensors='pt')\n",
    "test_dataset = Dataset.from_dict({'input_ids': test_encodings['input_ids'],\n",
    "                                 'attention_mask': test_encodings['attention_mask'],\n",
    "                                 'labels': torch.tensor(df_test[target_cols].values, dtype=torch.float32)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ae4840",
   "metadata": {},
   "source": [
    "### Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0442a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# download model from model hub\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(target_cols))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c48bc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=28, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e91901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom callback to get train and validation info during training\n",
    "class CustomCallback(TrainerCallback):\n",
    "    \n",
    "    def __init__(self, trainer) -> None:\n",
    "        super().__init__()\n",
    "        self._trainer = trainer\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if control.should_evaluate:\n",
    "            control_copy = copy.deepcopy(control)\n",
    "            self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\")\n",
    "            return control_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1980801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='data/output/',\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=VALID_BATCH_SIZE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    logging_dir=\"data/output/logs\",\n",
    "    learning_rate=float(LEARNING_RATE),\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0eae176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom function to calculate the metrics for multi label classification\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'recall' : recall_micro,\n",
    "               'precision': precision_micro,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3287773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.add_callback(CustomCallback(trainer)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e1c6498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aishah/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13565' max='13565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13565/13565 1:56:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.073335</td>\n",
       "      <td>0.655271</td>\n",
       "      <td>0.607335</td>\n",
       "      <td>0.711423</td>\n",
       "      <td>0.798262</td>\n",
       "      <td>0.533289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.540121</td>\n",
       "      <td>0.496395</td>\n",
       "      <td>0.592295</td>\n",
       "      <td>0.740709</td>\n",
       "      <td>0.426834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.074500</td>\n",
       "      <td>0.059036</td>\n",
       "      <td>0.723432</td>\n",
       "      <td>0.645146</td>\n",
       "      <td>0.823343</td>\n",
       "      <td>0.819535</td>\n",
       "      <td>0.599567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.074500</td>\n",
       "      <td>0.089651</td>\n",
       "      <td>0.556921</td>\n",
       "      <td>0.486520</td>\n",
       "      <td>0.651143</td>\n",
       "      <td>0.737547</td>\n",
       "      <td>0.438076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.063800</td>\n",
       "      <td>0.045362</td>\n",
       "      <td>0.804629</td>\n",
       "      <td>0.748459</td>\n",
       "      <td>0.869913</td>\n",
       "      <td>0.871773</td>\n",
       "      <td>0.700977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.063800</td>\n",
       "      <td>0.093721</td>\n",
       "      <td>0.578463</td>\n",
       "      <td>0.526646</td>\n",
       "      <td>0.641589</td>\n",
       "      <td>0.756875</td>\n",
       "      <td>0.471618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.037475</td>\n",
       "      <td>0.844882</td>\n",
       "      <td>0.797601</td>\n",
       "      <td>0.898122</td>\n",
       "      <td>0.896815</td>\n",
       "      <td>0.751889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.562007</td>\n",
       "      <td>0.519592</td>\n",
       "      <td>0.611962</td>\n",
       "      <td>0.752575</td>\n",
       "      <td>0.461113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.032583</td>\n",
       "      <td>0.868298</td>\n",
       "      <td>0.824804</td>\n",
       "      <td>0.916634</td>\n",
       "      <td>0.910756</td>\n",
       "      <td>0.783128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.103829</td>\n",
       "      <td>0.565042</td>\n",
       "      <td>0.526959</td>\n",
       "      <td>0.609058</td>\n",
       "      <td>0.756066</td>\n",
       "      <td>0.464246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13565, training_loss=0.05944350304482302, metrics={'train_runtime': 6980.8771, 'train_samples_per_second': 31.091, 'train_steps_per_second': 1.943, 'total_flos': 2.320457522193408e+16, 'train_loss': 0.05944350304482302, 'epoch': 5.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start the training process\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9dffa6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0722</td>\n",
       "      <td>4.815702e-05</td>\n",
       "      <td>0.18</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0680</td>\n",
       "      <td>4.631404e-05</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0656</td>\n",
       "      <td>4.447107e-05</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0603</td>\n",
       "      <td>4.262809e-05</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0572</td>\n",
       "      <td>4.078511e-05</td>\n",
       "      <td>0.92</td>\n",
       "      <td>2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2713</td>\n",
       "      <td>0.073335</td>\n",
       "      <td>0.655271</td>\n",
       "      <td>0.607335</td>\n",
       "      <td>0.711423</td>\n",
       "      <td>0.798262</td>\n",
       "      <td>0.533289</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.540121</td>\n",
       "      <td>0.496395</td>\n",
       "      <td>0.592295</td>\n",
       "      <td>0.740709</td>\n",
       "      <td>0.426834</td>\n",
       "      <td>20.3996</td>\n",
       "      <td>265.986</td>\n",
       "      <td>16.667</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0678</td>\n",
       "      <td>3.894213e-05</td>\n",
       "      <td>1.11</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0750</td>\n",
       "      <td>3.709915e-05</td>\n",
       "      <td>1.29</td>\n",
       "      <td>3500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0746</td>\n",
       "      <td>3.525617e-05</td>\n",
       "      <td>1.47</td>\n",
       "      <td>4000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0739</td>\n",
       "      <td>3.341320e-05</td>\n",
       "      <td>1.66</td>\n",
       "      <td>4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0745</td>\n",
       "      <td>3.157022e-05</td>\n",
       "      <td>1.84</td>\n",
       "      <td>5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5426</td>\n",
       "      <td>0.059036</td>\n",
       "      <td>0.723432</td>\n",
       "      <td>0.645146</td>\n",
       "      <td>0.823343</td>\n",
       "      <td>0.819535</td>\n",
       "      <td>0.599567</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089651</td>\n",
       "      <td>0.556921</td>\n",
       "      <td>0.486520</td>\n",
       "      <td>0.651143</td>\n",
       "      <td>0.737547</td>\n",
       "      <td>0.438076</td>\n",
       "      <td>20.3778</td>\n",
       "      <td>266.271</td>\n",
       "      <td>16.685</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0714</td>\n",
       "      <td>2.972724e-05</td>\n",
       "      <td>2.03</td>\n",
       "      <td>5500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0634</td>\n",
       "      <td>2.788426e-05</td>\n",
       "      <td>2.21</td>\n",
       "      <td>6000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0626</td>\n",
       "      <td>2.604128e-05</td>\n",
       "      <td>2.40</td>\n",
       "      <td>6500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0636</td>\n",
       "      <td>2.419830e-05</td>\n",
       "      <td>2.58</td>\n",
       "      <td>7000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0618</td>\n",
       "      <td>2.235533e-05</td>\n",
       "      <td>2.76</td>\n",
       "      <td>7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0638</td>\n",
       "      <td>2.051235e-05</td>\n",
       "      <td>2.95</td>\n",
       "      <td>8000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.045362</td>\n",
       "      <td>0.804629</td>\n",
       "      <td>0.748459</td>\n",
       "      <td>0.869913</td>\n",
       "      <td>0.871773</td>\n",
       "      <td>0.700977</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>8139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093721</td>\n",
       "      <td>0.578463</td>\n",
       "      <td>0.526646</td>\n",
       "      <td>0.641589</td>\n",
       "      <td>0.756875</td>\n",
       "      <td>0.471618</td>\n",
       "      <td>20.4341</td>\n",
       "      <td>265.537</td>\n",
       "      <td>16.639</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0552</td>\n",
       "      <td>1.866937e-05</td>\n",
       "      <td>3.13</td>\n",
       "      <td>8500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0514</td>\n",
       "      <td>1.682639e-05</td>\n",
       "      <td>3.32</td>\n",
       "      <td>9000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0519</td>\n",
       "      <td>1.498341e-05</td>\n",
       "      <td>3.50</td>\n",
       "      <td>9500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0514</td>\n",
       "      <td>1.314043e-05</td>\n",
       "      <td>3.69</td>\n",
       "      <td>10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0523</td>\n",
       "      <td>1.129746e-05</td>\n",
       "      <td>3.87</td>\n",
       "      <td>10500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.00</td>\n",
       "      <td>10852</td>\n",
       "      <td>0.037475</td>\n",
       "      <td>0.844882</td>\n",
       "      <td>0.797601</td>\n",
       "      <td>0.898122</td>\n",
       "      <td>0.896815</td>\n",
       "      <td>0.751889</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.00</td>\n",
       "      <td>10852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.562007</td>\n",
       "      <td>0.519592</td>\n",
       "      <td>0.611962</td>\n",
       "      <td>0.752575</td>\n",
       "      <td>0.461113</td>\n",
       "      <td>20.4838</td>\n",
       "      <td>264.892</td>\n",
       "      <td>16.598</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0501</td>\n",
       "      <td>9.454478e-06</td>\n",
       "      <td>4.05</td>\n",
       "      <td>11000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0442</td>\n",
       "      <td>7.611500e-06</td>\n",
       "      <td>4.24</td>\n",
       "      <td>11500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0441</td>\n",
       "      <td>5.768522e-06</td>\n",
       "      <td>4.42</td>\n",
       "      <td>12000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0435</td>\n",
       "      <td>3.925544e-06</td>\n",
       "      <td>4.61</td>\n",
       "      <td>12500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0444</td>\n",
       "      <td>2.082565e-06</td>\n",
       "      <td>4.79</td>\n",
       "      <td>13000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0425</td>\n",
       "      <td>2.395872e-07</td>\n",
       "      <td>4.98</td>\n",
       "      <td>13500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.032583</td>\n",
       "      <td>0.868298</td>\n",
       "      <td>0.824804</td>\n",
       "      <td>0.916634</td>\n",
       "      <td>0.910756</td>\n",
       "      <td>0.783128</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103829</td>\n",
       "      <td>0.565042</td>\n",
       "      <td>0.526959</td>\n",
       "      <td>0.609058</td>\n",
       "      <td>0.756066</td>\n",
       "      <td>0.464246</td>\n",
       "      <td>20.4323</td>\n",
       "      <td>265.560</td>\n",
       "      <td>16.640</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.059444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.320458e+16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092023</td>\n",
       "      <td>0.578376</td>\n",
       "      <td>0.529942</td>\n",
       "      <td>0.636553</td>\n",
       "      <td>0.758396</td>\n",
       "      <td>0.470426</td>\n",
       "      <td>15.5837</td>\n",
       "      <td>348.249</td>\n",
       "      <td>21.818</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      loss  learning_rate  epoch   step  train_loss  train_f1  train_recall  \\\n",
       "0   0.0722   4.815702e-05   0.18    500         NaN       NaN           NaN   \n",
       "1   0.0680   4.631404e-05   0.37   1000         NaN       NaN           NaN   \n",
       "2   0.0656   4.447107e-05   0.55   1500         NaN       NaN           NaN   \n",
       "3   0.0603   4.262809e-05   0.74   2000         NaN       NaN           NaN   \n",
       "4   0.0572   4.078511e-05   0.92   2500         NaN       NaN           NaN   \n",
       "5      NaN            NaN   1.00   2713    0.073335  0.655271      0.607335   \n",
       "6      NaN            NaN   1.00   2713         NaN       NaN           NaN   \n",
       "7   0.0678   3.894213e-05   1.11   3000         NaN       NaN           NaN   \n",
       "8   0.0750   3.709915e-05   1.29   3500         NaN       NaN           NaN   \n",
       "9   0.0746   3.525617e-05   1.47   4000         NaN       NaN           NaN   \n",
       "10  0.0739   3.341320e-05   1.66   4500         NaN       NaN           NaN   \n",
       "11  0.0745   3.157022e-05   1.84   5000         NaN       NaN           NaN   \n",
       "12     NaN            NaN   2.00   5426    0.059036  0.723432      0.645146   \n",
       "13     NaN            NaN   2.00   5426         NaN       NaN           NaN   \n",
       "14  0.0714   2.972724e-05   2.03   5500         NaN       NaN           NaN   \n",
       "15  0.0634   2.788426e-05   2.21   6000         NaN       NaN           NaN   \n",
       "16  0.0626   2.604128e-05   2.40   6500         NaN       NaN           NaN   \n",
       "17  0.0636   2.419830e-05   2.58   7000         NaN       NaN           NaN   \n",
       "18  0.0618   2.235533e-05   2.76   7500         NaN       NaN           NaN   \n",
       "19  0.0638   2.051235e-05   2.95   8000         NaN       NaN           NaN   \n",
       "20     NaN            NaN   3.00   8139    0.045362  0.804629      0.748459   \n",
       "21     NaN            NaN   3.00   8139         NaN       NaN           NaN   \n",
       "22  0.0552   1.866937e-05   3.13   8500         NaN       NaN           NaN   \n",
       "23  0.0514   1.682639e-05   3.32   9000         NaN       NaN           NaN   \n",
       "24  0.0519   1.498341e-05   3.50   9500         NaN       NaN           NaN   \n",
       "25  0.0514   1.314043e-05   3.69  10000         NaN       NaN           NaN   \n",
       "26  0.0523   1.129746e-05   3.87  10500         NaN       NaN           NaN   \n",
       "27     NaN            NaN   4.00  10852    0.037475  0.844882      0.797601   \n",
       "28     NaN            NaN   4.00  10852         NaN       NaN           NaN   \n",
       "29  0.0501   9.454478e-06   4.05  11000         NaN       NaN           NaN   \n",
       "30  0.0442   7.611500e-06   4.24  11500         NaN       NaN           NaN   \n",
       "31  0.0441   5.768522e-06   4.42  12000         NaN       NaN           NaN   \n",
       "32  0.0435   3.925544e-06   4.61  12500         NaN       NaN           NaN   \n",
       "33  0.0444   2.082565e-06   4.79  13000         NaN       NaN           NaN   \n",
       "34  0.0425   2.395872e-07   4.98  13500         NaN       NaN           NaN   \n",
       "35     NaN            NaN   5.00  13565    0.032583  0.868298      0.824804   \n",
       "36     NaN            NaN   5.00  13565         NaN       NaN           NaN   \n",
       "37     NaN            NaN   5.00  13565    0.059444       NaN           NaN   \n",
       "38     NaN            NaN   5.00  13565         NaN       NaN           NaN   \n",
       "\n",
       "    train_precision  train_roc_auc  train_accuracy  ...  eval_loss   eval_f1  \\\n",
       "0               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "1               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "2               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "3               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "4               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "5          0.711423       0.798262        0.533289  ...        NaN       NaN   \n",
       "6               NaN            NaN             NaN  ...   0.100004  0.540121   \n",
       "7               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "8               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "9               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "10              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "11              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "12         0.823343       0.819535        0.599567  ...        NaN       NaN   \n",
       "13              NaN            NaN             NaN  ...   0.089651  0.556921   \n",
       "14              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "15              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "16              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "17              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "18              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "19              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "20         0.869913       0.871773        0.700977  ...        NaN       NaN   \n",
       "21              NaN            NaN             NaN  ...   0.093721  0.578463   \n",
       "22              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "23              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "24              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "25              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "26              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "27         0.898122       0.896815        0.751889  ...        NaN       NaN   \n",
       "28              NaN            NaN             NaN  ...   0.100012  0.562007   \n",
       "29              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "30              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "31              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "32              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "33              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "34              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "35         0.916634       0.910756        0.783128  ...        NaN       NaN   \n",
       "36              NaN            NaN             NaN  ...   0.103829  0.565042   \n",
       "37              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "38              NaN            NaN             NaN  ...   0.092023  0.578376   \n",
       "\n",
       "    eval_recall  eval_precision  eval_roc_auc  eval_accuracy  eval_runtime  \\\n",
       "0           NaN             NaN           NaN            NaN           NaN   \n",
       "1           NaN             NaN           NaN            NaN           NaN   \n",
       "2           NaN             NaN           NaN            NaN           NaN   \n",
       "3           NaN             NaN           NaN            NaN           NaN   \n",
       "4           NaN             NaN           NaN            NaN           NaN   \n",
       "5           NaN             NaN           NaN            NaN           NaN   \n",
       "6      0.496395        0.592295      0.740709       0.426834       20.3996   \n",
       "7           NaN             NaN           NaN            NaN           NaN   \n",
       "8           NaN             NaN           NaN            NaN           NaN   \n",
       "9           NaN             NaN           NaN            NaN           NaN   \n",
       "10          NaN             NaN           NaN            NaN           NaN   \n",
       "11          NaN             NaN           NaN            NaN           NaN   \n",
       "12          NaN             NaN           NaN            NaN           NaN   \n",
       "13     0.486520        0.651143      0.737547       0.438076       20.3778   \n",
       "14          NaN             NaN           NaN            NaN           NaN   \n",
       "15          NaN             NaN           NaN            NaN           NaN   \n",
       "16          NaN             NaN           NaN            NaN           NaN   \n",
       "17          NaN             NaN           NaN            NaN           NaN   \n",
       "18          NaN             NaN           NaN            NaN           NaN   \n",
       "19          NaN             NaN           NaN            NaN           NaN   \n",
       "20          NaN             NaN           NaN            NaN           NaN   \n",
       "21     0.526646        0.641589      0.756875       0.471618       20.4341   \n",
       "22          NaN             NaN           NaN            NaN           NaN   \n",
       "23          NaN             NaN           NaN            NaN           NaN   \n",
       "24          NaN             NaN           NaN            NaN           NaN   \n",
       "25          NaN             NaN           NaN            NaN           NaN   \n",
       "26          NaN             NaN           NaN            NaN           NaN   \n",
       "27          NaN             NaN           NaN            NaN           NaN   \n",
       "28     0.519592        0.611962      0.752575       0.461113       20.4838   \n",
       "29          NaN             NaN           NaN            NaN           NaN   \n",
       "30          NaN             NaN           NaN            NaN           NaN   \n",
       "31          NaN             NaN           NaN            NaN           NaN   \n",
       "32          NaN             NaN           NaN            NaN           NaN   \n",
       "33          NaN             NaN           NaN            NaN           NaN   \n",
       "34          NaN             NaN           NaN            NaN           NaN   \n",
       "35          NaN             NaN           NaN            NaN           NaN   \n",
       "36     0.526959        0.609058      0.756066       0.464246       20.4323   \n",
       "37          NaN             NaN           NaN            NaN           NaN   \n",
       "38     0.529942        0.636553      0.758396       0.470426       15.5837   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second    total_flos  \n",
       "0                       NaN                    NaN           NaN  \n",
       "1                       NaN                    NaN           NaN  \n",
       "2                       NaN                    NaN           NaN  \n",
       "3                       NaN                    NaN           NaN  \n",
       "4                       NaN                    NaN           NaN  \n",
       "5                       NaN                    NaN           NaN  \n",
       "6                   265.986                 16.667           NaN  \n",
       "7                       NaN                    NaN           NaN  \n",
       "8                       NaN                    NaN           NaN  \n",
       "9                       NaN                    NaN           NaN  \n",
       "10                      NaN                    NaN           NaN  \n",
       "11                      NaN                    NaN           NaN  \n",
       "12                      NaN                    NaN           NaN  \n",
       "13                  266.271                 16.685           NaN  \n",
       "14                      NaN                    NaN           NaN  \n",
       "15                      NaN                    NaN           NaN  \n",
       "16                      NaN                    NaN           NaN  \n",
       "17                      NaN                    NaN           NaN  \n",
       "18                      NaN                    NaN           NaN  \n",
       "19                      NaN                    NaN           NaN  \n",
       "20                      NaN                    NaN           NaN  \n",
       "21                  265.537                 16.639           NaN  \n",
       "22                      NaN                    NaN           NaN  \n",
       "23                      NaN                    NaN           NaN  \n",
       "24                      NaN                    NaN           NaN  \n",
       "25                      NaN                    NaN           NaN  \n",
       "26                      NaN                    NaN           NaN  \n",
       "27                      NaN                    NaN           NaN  \n",
       "28                  264.892                 16.598           NaN  \n",
       "29                      NaN                    NaN           NaN  \n",
       "30                      NaN                    NaN           NaN  \n",
       "31                      NaN                    NaN           NaN  \n",
       "32                      NaN                    NaN           NaN  \n",
       "33                      NaN                    NaN           NaN  \n",
       "34                      NaN                    NaN           NaN  \n",
       "35                      NaN                    NaN           NaN  \n",
       "36                  265.560                 16.640           NaN  \n",
       "37                      NaN                    NaN  2.320458e+16  \n",
       "38                  348.249                 21.818           NaN  \n",
       "\n",
       "[39 rows x 23 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view train and validation metrics from training\n",
    "log_history = pd.DataFrame(trainer.state.log_history)\n",
    "log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf0d198a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2713</td>\n",
       "      <td>0.073335</td>\n",
       "      <td>0.655271</td>\n",
       "      <td>0.607335</td>\n",
       "      <td>0.711423</td>\n",
       "      <td>0.798262</td>\n",
       "      <td>0.533289</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5426</td>\n",
       "      <td>0.059036</td>\n",
       "      <td>0.723432</td>\n",
       "      <td>0.645146</td>\n",
       "      <td>0.823343</td>\n",
       "      <td>0.819535</td>\n",
       "      <td>0.599567</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.045362</td>\n",
       "      <td>0.804629</td>\n",
       "      <td>0.748459</td>\n",
       "      <td>0.869913</td>\n",
       "      <td>0.871773</td>\n",
       "      <td>0.700977</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10852</td>\n",
       "      <td>0.037475</td>\n",
       "      <td>0.844882</td>\n",
       "      <td>0.797601</td>\n",
       "      <td>0.898122</td>\n",
       "      <td>0.896815</td>\n",
       "      <td>0.751889</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.032583</td>\n",
       "      <td>0.868298</td>\n",
       "      <td>0.824804</td>\n",
       "      <td>0.916634</td>\n",
       "      <td>0.910756</td>\n",
       "      <td>0.783128</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    loss  learning_rate  epoch   step  train_loss  train_f1  train_recall  \\\n",
       "5    NaN            NaN    1.0   2713    0.073335  0.655271      0.607335   \n",
       "12   NaN            NaN    2.0   5426    0.059036  0.723432      0.645146   \n",
       "20   NaN            NaN    3.0   8139    0.045362  0.804629      0.748459   \n",
       "27   NaN            NaN    4.0  10852    0.037475  0.844882      0.797601   \n",
       "35   NaN            NaN    5.0  13565    0.032583  0.868298      0.824804   \n",
       "\n",
       "    train_precision  train_roc_auc  train_accuracy  ...  eval_loss  eval_f1  \\\n",
       "5          0.711423       0.798262        0.533289  ...        NaN      NaN   \n",
       "12         0.823343       0.819535        0.599567  ...        NaN      NaN   \n",
       "20         0.869913       0.871773        0.700977  ...        NaN      NaN   \n",
       "27         0.898122       0.896815        0.751889  ...        NaN      NaN   \n",
       "35         0.916634       0.910756        0.783128  ...        NaN      NaN   \n",
       "\n",
       "    eval_recall  eval_precision  eval_roc_auc  eval_accuracy  eval_runtime  \\\n",
       "5           NaN             NaN           NaN            NaN           NaN   \n",
       "12          NaN             NaN           NaN            NaN           NaN   \n",
       "20          NaN             NaN           NaN            NaN           NaN   \n",
       "27          NaN             NaN           NaN            NaN           NaN   \n",
       "35          NaN             NaN           NaN            NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  total_flos  \n",
       "5                       NaN                    NaN         NaN  \n",
       "12                      NaN                    NaN         NaN  \n",
       "20                      NaN                    NaN         NaN  \n",
       "27                      NaN                    NaN         NaN  \n",
       "35                      NaN                    NaN         NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store train metrics in dataframe\n",
    "train_history = log_history[log_history['train_f1'].notna()]\n",
    "train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "97eea248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.540121</td>\n",
       "      <td>0.496395</td>\n",
       "      <td>0.592295</td>\n",
       "      <td>0.740709</td>\n",
       "      <td>0.426834</td>\n",
       "      <td>20.3996</td>\n",
       "      <td>265.986</td>\n",
       "      <td>16.667</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089651</td>\n",
       "      <td>0.556921</td>\n",
       "      <td>0.486520</td>\n",
       "      <td>0.651143</td>\n",
       "      <td>0.737547</td>\n",
       "      <td>0.438076</td>\n",
       "      <td>20.3778</td>\n",
       "      <td>266.271</td>\n",
       "      <td>16.685</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093721</td>\n",
       "      <td>0.578463</td>\n",
       "      <td>0.526646</td>\n",
       "      <td>0.641589</td>\n",
       "      <td>0.756875</td>\n",
       "      <td>0.471618</td>\n",
       "      <td>20.4341</td>\n",
       "      <td>265.537</td>\n",
       "      <td>16.639</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.562007</td>\n",
       "      <td>0.519592</td>\n",
       "      <td>0.611962</td>\n",
       "      <td>0.752575</td>\n",
       "      <td>0.461113</td>\n",
       "      <td>20.4838</td>\n",
       "      <td>264.892</td>\n",
       "      <td>16.598</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103829</td>\n",
       "      <td>0.565042</td>\n",
       "      <td>0.526959</td>\n",
       "      <td>0.609058</td>\n",
       "      <td>0.756066</td>\n",
       "      <td>0.464246</td>\n",
       "      <td>20.4323</td>\n",
       "      <td>265.560</td>\n",
       "      <td>16.640</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    loss  learning_rate  epoch   step  train_loss  train_f1  train_recall  \\\n",
       "6    NaN            NaN    1.0   2713         NaN       NaN           NaN   \n",
       "13   NaN            NaN    2.0   5426         NaN       NaN           NaN   \n",
       "21   NaN            NaN    3.0   8139         NaN       NaN           NaN   \n",
       "28   NaN            NaN    4.0  10852         NaN       NaN           NaN   \n",
       "36   NaN            NaN    5.0  13565         NaN       NaN           NaN   \n",
       "\n",
       "    train_precision  train_roc_auc  train_accuracy  ...  eval_loss   eval_f1  \\\n",
       "6               NaN            NaN             NaN  ...   0.100004  0.540121   \n",
       "13              NaN            NaN             NaN  ...   0.089651  0.556921   \n",
       "21              NaN            NaN             NaN  ...   0.093721  0.578463   \n",
       "28              NaN            NaN             NaN  ...   0.100012  0.562007   \n",
       "36              NaN            NaN             NaN  ...   0.103829  0.565042   \n",
       "\n",
       "    eval_recall  eval_precision  eval_roc_auc  eval_accuracy  eval_runtime  \\\n",
       "6      0.496395        0.592295      0.740709       0.426834       20.3996   \n",
       "13     0.486520        0.651143      0.737547       0.438076       20.3778   \n",
       "21     0.526646        0.641589      0.756875       0.471618       20.4341   \n",
       "28     0.519592        0.611962      0.752575       0.461113       20.4838   \n",
       "36     0.526959        0.609058      0.756066       0.464246       20.4323   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  total_flos  \n",
       "6                   265.986                 16.667         NaN  \n",
       "13                  266.271                 16.685         NaN  \n",
       "21                  265.537                 16.639         NaN  \n",
       "28                  264.892                 16.598         NaN  \n",
       "36                  265.560                 16.640         NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store validation metrics in dataframe\n",
    "val_history = log_history[log_history['eval_f1'].notna()]\n",
    "val_history = val_history.drop(val_history.index[-1])\n",
    "val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "978bf0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_x</th>\n",
       "      <th>learning_rate_x</th>\n",
       "      <th>epoch_x</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss_x</th>\n",
       "      <th>train_f1_x</th>\n",
       "      <th>train_recall_x</th>\n",
       "      <th>train_precision_x</th>\n",
       "      <th>train_roc_auc_x</th>\n",
       "      <th>train_accuracy_x</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss_y</th>\n",
       "      <th>eval_f1_y</th>\n",
       "      <th>eval_recall_y</th>\n",
       "      <th>eval_precision_y</th>\n",
       "      <th>eval_roc_auc_y</th>\n",
       "      <th>eval_accuracy_y</th>\n",
       "      <th>eval_runtime_y</th>\n",
       "      <th>eval_samples_per_second_y</th>\n",
       "      <th>eval_steps_per_second_y</th>\n",
       "      <th>total_flos_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2713</td>\n",
       "      <td>0.073335</td>\n",
       "      <td>0.655271</td>\n",
       "      <td>0.607335</td>\n",
       "      <td>0.711423</td>\n",
       "      <td>0.798262</td>\n",
       "      <td>0.533289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.540121</td>\n",
       "      <td>0.496395</td>\n",
       "      <td>0.592295</td>\n",
       "      <td>0.740709</td>\n",
       "      <td>0.426834</td>\n",
       "      <td>20.3996</td>\n",
       "      <td>265.986</td>\n",
       "      <td>16.667</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5426</td>\n",
       "      <td>0.059036</td>\n",
       "      <td>0.723432</td>\n",
       "      <td>0.645146</td>\n",
       "      <td>0.823343</td>\n",
       "      <td>0.819535</td>\n",
       "      <td>0.599567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089651</td>\n",
       "      <td>0.556921</td>\n",
       "      <td>0.486520</td>\n",
       "      <td>0.651143</td>\n",
       "      <td>0.737547</td>\n",
       "      <td>0.438076</td>\n",
       "      <td>20.3778</td>\n",
       "      <td>266.271</td>\n",
       "      <td>16.685</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.045362</td>\n",
       "      <td>0.804629</td>\n",
       "      <td>0.748459</td>\n",
       "      <td>0.869913</td>\n",
       "      <td>0.871773</td>\n",
       "      <td>0.700977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093721</td>\n",
       "      <td>0.578463</td>\n",
       "      <td>0.526646</td>\n",
       "      <td>0.641589</td>\n",
       "      <td>0.756875</td>\n",
       "      <td>0.471618</td>\n",
       "      <td>20.4341</td>\n",
       "      <td>265.537</td>\n",
       "      <td>16.639</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10852</td>\n",
       "      <td>0.037475</td>\n",
       "      <td>0.844882</td>\n",
       "      <td>0.797601</td>\n",
       "      <td>0.898122</td>\n",
       "      <td>0.896815</td>\n",
       "      <td>0.751889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.562007</td>\n",
       "      <td>0.519592</td>\n",
       "      <td>0.611962</td>\n",
       "      <td>0.752575</td>\n",
       "      <td>0.461113</td>\n",
       "      <td>20.4838</td>\n",
       "      <td>264.892</td>\n",
       "      <td>16.598</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.032583</td>\n",
       "      <td>0.868298</td>\n",
       "      <td>0.824804</td>\n",
       "      <td>0.916634</td>\n",
       "      <td>0.910756</td>\n",
       "      <td>0.783128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103829</td>\n",
       "      <td>0.565042</td>\n",
       "      <td>0.526959</td>\n",
       "      <td>0.609058</td>\n",
       "      <td>0.756066</td>\n",
       "      <td>0.464246</td>\n",
       "      <td>20.4323</td>\n",
       "      <td>265.560</td>\n",
       "      <td>16.640</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loss_x  learning_rate_x  epoch_x   step  train_loss_x  train_f1_x  \\\n",
       "0     NaN              NaN      1.0   2713      0.073335    0.655271   \n",
       "1     NaN              NaN      2.0   5426      0.059036    0.723432   \n",
       "2     NaN              NaN      3.0   8139      0.045362    0.804629   \n",
       "3     NaN              NaN      4.0  10852      0.037475    0.844882   \n",
       "4     NaN              NaN      5.0  13565      0.032583    0.868298   \n",
       "\n",
       "   train_recall_x  train_precision_x  train_roc_auc_x  train_accuracy_x  ...  \\\n",
       "0        0.607335           0.711423         0.798262          0.533289  ...   \n",
       "1        0.645146           0.823343         0.819535          0.599567  ...   \n",
       "2        0.748459           0.869913         0.871773          0.700977  ...   \n",
       "3        0.797601           0.898122         0.896815          0.751889  ...   \n",
       "4        0.824804           0.916634         0.910756          0.783128  ...   \n",
       "\n",
       "   eval_loss_y  eval_f1_y  eval_recall_y  eval_precision_y  eval_roc_auc_y  \\\n",
       "0     0.100004   0.540121       0.496395          0.592295        0.740709   \n",
       "1     0.089651   0.556921       0.486520          0.651143        0.737547   \n",
       "2     0.093721   0.578463       0.526646          0.641589        0.756875   \n",
       "3     0.100012   0.562007       0.519592          0.611962        0.752575   \n",
       "4     0.103829   0.565042       0.526959          0.609058        0.756066   \n",
       "\n",
       "   eval_accuracy_y  eval_runtime_y  eval_samples_per_second_y  \\\n",
       "0         0.426834         20.3996                    265.986   \n",
       "1         0.438076         20.3778                    266.271   \n",
       "2         0.471618         20.4341                    265.537   \n",
       "3         0.461113         20.4838                    264.892   \n",
       "4         0.464246         20.4323                    265.560   \n",
       "\n",
       "   eval_steps_per_second_y  total_flos_y  \n",
       "0                   16.667           NaN  \n",
       "1                   16.685           NaN  \n",
       "2                   16.639           NaN  \n",
       "3                   16.598           NaN  \n",
       "4                   16.640           NaN  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = pd.merge(train_history, val_history, on='step', how='outer')\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9ebe0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting function to check for accuracy with graphs \n",
    "def plot_model_performance(history):\n",
    "\n",
    "    #getting train and validation accuracy\n",
    "    acc = history['train_accuracy_x']\n",
    "    val_acc = history['eval_accuracy_y']\n",
    "\n",
    "    #getting train and validation loss\n",
    "    loss = history['train_loss_x']\n",
    "    val_loss = history['eval_loss_y']\n",
    "\n",
    "    epochs_range = range(5)\n",
    "\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c97c350d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_model_performance(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc7fb89",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5456e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to evaluate the model on the test set\n",
    "def evaluate_model(test_dataset):\n",
    "    test_res = trainer.evaluate(eval_dataset=test_dataset)\n",
    "    print(test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7bb617c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09202256798744202, 'eval_f1': 0.578375581996896, 'eval_recall': 0.529941538947701, 'eval_precision': 0.6365534256974757, 'eval_roc_auc': 0.7583957524783758, 'eval_accuracy': 0.47042564953012717, 'eval_runtime': 15.5837, 'eval_samples_per_second': 348.249, 'eval_steps_per_second': 21.818, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c172305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate each emotion label metrics on test set\n",
    "def calc_label_metrics(label, y_targets, y_preds, threshold):\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"accuracy\": metrics.accuracy_score(y_targets, y_preds),\n",
    "        \"precision\": metrics.precision_score(y_targets, y_preds, zero_division=0),\n",
    "        \"recall\": metrics.recall_score(y_targets, y_preds, zero_division=0),\n",
    "        \"f1\": metrics.f1_score(y_targets, y_preds, zero_division=0),\n",
    "        \"mcc\": metrics.matthews_corrcoef(y_targets, y_preds),\n",
    "        \"support\": y_targets.sum(),\n",
    "        \"threshold\": threshold,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78c37071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate overall metric on test set\n",
    "def calc_test_metrics(trainer, test_dataset, target_cols):\n",
    "    y_test = trainer.predict(test_dataset)\n",
    "    threshold = 0.5\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(y_test.predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= 0.5)] = 1\n",
    "    \n",
    "    # finally, compute metrics\n",
    "    y_true = df_test[target_cols].values\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'recall' : recall_micro,\n",
    "               'precision': precision_micro,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    metrics_df = pd.DataFrame.from_dict(metrics, orient='index', columns=['Value'])\n",
    "\n",
    "    display(metrics_df)\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for label_index, label in enumerate(target_cols):\n",
    "        y_targets, y_preds = y_true[:, label_index], y_pred[:, label_index]\n",
    "        results.append(calc_label_metrics(label, y_targets, y_preds, threshold))\n",
    "\n",
    "    per_label_results = pd.DataFrame(results, index=target_cols)\n",
    "    display(per_label_results.drop(columns=[\"label\"]).round(3))\n",
    "    \n",
    "    return y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "794fec4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.578376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.529942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.636553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc</th>\n",
       "      <td>0.758396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.470426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "f1         0.578376\n",
       "recall     0.529942\n",
       "precision  0.636553\n",
       "roc_auc    0.758396\n",
       "accuracy   0.470426"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>mcc</th>\n",
       "      <th>support</th>\n",
       "      <th>threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.939</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.650</td>\n",
       "      <td>504</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.980</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.798</td>\n",
       "      <td>264</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.964</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.498</td>\n",
       "      <td>198</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.934</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.272</td>\n",
       "      <td>320</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.931</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.346</td>\n",
       "      <td>351</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.972</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.399</td>\n",
       "      <td>135</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.973</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.417</td>\n",
       "      <td>153</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.943</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.402</td>\n",
       "      <td>284</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.986</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.398</td>\n",
       "      <td>83</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.972</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.288</td>\n",
       "      <td>151</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.945</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.343</td>\n",
       "      <td>267</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.981</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.458</td>\n",
       "      <td>123</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.994</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.436</td>\n",
       "      <td>37</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.983</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.401</td>\n",
       "      <td>103</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.990</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.688</td>\n",
       "      <td>78</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.990</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.917</td>\n",
       "      <td>352</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.999</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.978</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.597</td>\n",
       "      <td>161</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.981</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.791</td>\n",
       "      <td>238</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.996</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.403</td>\n",
       "      <td>23</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.974</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.532</td>\n",
       "      <td>186</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.998</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.446</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.974</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.295</td>\n",
       "      <td>145</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.998</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.990</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.555</td>\n",
       "      <td>56</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.972</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.512</td>\n",
       "      <td>156</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.978</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.555</td>\n",
       "      <td>141</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.782</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.491</td>\n",
       "      <td>1787</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                accuracy  precision  recall     f1    mcc  support  threshold\n",
       "admiration         0.939      0.664   0.702  0.683  0.650      504        0.5\n",
       "amusement          0.980      0.748   0.875  0.806  0.798      264        0.5\n",
       "anger              0.964      0.507   0.525  0.516  0.498      198        0.5\n",
       "annoyance          0.934      0.400   0.231  0.293  0.272      320        0.5\n",
       "approval           0.931      0.455   0.319  0.375  0.346      351        0.5\n",
       "caring             0.972      0.434   0.393  0.412  0.399      135        0.5\n",
       "confusion          0.973      0.524   0.353  0.422  0.417      153        0.5\n",
       "curiosity          0.943      0.452   0.412  0.431  0.402      284        0.5\n",
       "desire             0.986      0.590   0.277  0.377  0.398       83        0.5\n",
       "disappointment     0.972      0.500   0.179  0.263  0.288      151        0.5\n",
       "disapproval        0.945      0.428   0.322  0.368  0.343      267        0.5\n",
       "disgust            0.981      0.636   0.341  0.444  0.458      123        0.5\n",
       "embarrassment      0.994      0.647   0.297  0.407  0.436       37        0.5\n",
       "excitement         0.983      0.592   0.282  0.382  0.401      103        0.5\n",
       "fear               0.990      0.634   0.756  0.690  0.688       78        0.5\n",
       "gratitude          0.990      0.960   0.886  0.922  0.917      352        0.5\n",
       "grief              0.999      0.000   0.000  0.000  0.000        6        0.5\n",
       "joy                0.978      0.662   0.559  0.606  0.597      161        0.5\n",
       "love               0.981      0.747   0.857  0.798  0.791      238        0.5\n",
       "nervousness        0.996      0.538   0.304  0.389  0.403       23        0.5\n",
       "optimism           0.974      0.681   0.435  0.531  0.532      186        0.5\n",
       "pride              0.998      0.800   0.250  0.381  0.446       16        0.5\n",
       "realization        0.974      0.558   0.166  0.255  0.295      145        0.5\n",
       "relief             0.998      0.000   0.000  0.000  0.000       11        0.5\n",
       "remorse            0.990      0.532   0.589  0.559  0.555       56        0.5\n",
       "sadness            0.972      0.515   0.538  0.527  0.512      156        0.5\n",
       "surprise           0.978      0.588   0.546  0.566  0.555      141        0.5\n",
       "neutral            0.782      0.700   0.594  0.643  0.491     1787        0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "targets, outputs = calc_test_metrics(trainer, test_dataset, target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f1631d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results DataFrame:\n",
      "         Actual                Predicted\n",
      "0     [sadness]                   [love]\n",
      "1  [admiration]    [admiration, disgust]\n",
      "2  [excitement]               [optimism]\n",
      "3   [gratitude]              [gratitude]\n",
      "4     [neutral]                       []\n",
      "5   [gratitude]              [gratitude]\n",
      "6   [gratitude]              [gratitude]\n",
      "7   [gratitude]  [admiration, gratitude]\n",
      "8     [remorse]       [remorse, sadness]\n",
      "9     [sadness]                [sadness]\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to store actual labels and predicted labels\n",
    "final_df = pd.DataFrame({\n",
    "    'Actual': [list(np.where(targets[i])[0]) for i in range(len(targets))],\n",
    "    'Predicted': [list(np.where(outputs[i])[0]) for i in range(len(outputs))]\n",
    "})\n",
    "\n",
    "# Map label indices to label names in the 'Actual' column\n",
    "final_df['Actual'] = final_df['Actual'].apply(lambda indices: [target_cols[idx] for idx in indices])\n",
    "\n",
    "# Map label indices to label names in the 'Predicted' column\n",
    "final_df['Predicted'] = final_df['Predicted'].apply(lambda indices: [target_cols[idx] for idx in indices])\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(\"Results DataFrame:\")\n",
    "print(final_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "301ef201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the label DataFrame with the original DataFramev\n",
    "val_df_terms = df_test['clean_text']\n",
    "result_df = pd.concat([val_df_terms, final_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a335fd5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am really sorry about your situation frown s...</td>\n",
       "      <td>[sadness]</td>\n",
       "      <td>[love]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it is wonderful because it is awful at not with</td>\n",
       "      <td>[admiration]</td>\n",
       "      <td>[admiration, disgust]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kings fan here good luck to you guys will be a...</td>\n",
       "      <td>[excitement]</td>\n",
       "      <td>[optimism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i did not know that thank you for teaching me ...</td>\n",
       "      <td>[gratitude]</td>\n",
       "      <td>[gratitude]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>they got bored from haunting earth for thousan...</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5422</th>\n",
       "      <td>thanks i was diagnosed with bp 1 after the hos...</td>\n",
       "      <td>[gratitude]</td>\n",
       "      <td>[gratitude]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5423</th>\n",
       "      <td>well that makes sense</td>\n",
       "      <td>[approval]</td>\n",
       "      <td>[approval]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5424</th>\n",
       "      <td>daddy issues name</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5425</th>\n",
       "      <td>so glad i discovered that subreddit a couple m...</td>\n",
       "      <td>[admiration]</td>\n",
       "      <td>[joy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5426</th>\n",
       "      <td>had to watch elmo in grouchland one time too m...</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5427 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_text        Actual  \\\n",
       "0     i am really sorry about your situation frown s...     [sadness]   \n",
       "1       it is wonderful because it is awful at not with  [admiration]   \n",
       "2     kings fan here good luck to you guys will be a...  [excitement]   \n",
       "3     i did not know that thank you for teaching me ...   [gratitude]   \n",
       "4     they got bored from haunting earth for thousan...     [neutral]   \n",
       "...                                                 ...           ...   \n",
       "5422  thanks i was diagnosed with bp 1 after the hos...   [gratitude]   \n",
       "5423                              well that makes sense    [approval]   \n",
       "5424                                  daddy issues name     [neutral]   \n",
       "5425  so glad i discovered that subreddit a couple m...  [admiration]   \n",
       "5426  had to watch elmo in grouchland one time too m...     [neutral]   \n",
       "\n",
       "                  Predicted  \n",
       "0                    [love]  \n",
       "1     [admiration, disgust]  \n",
       "2                [optimism]  \n",
       "3               [gratitude]  \n",
       "4                        []  \n",
       "...                     ...  \n",
       "5422            [gratitude]  \n",
       "5423             [approval]  \n",
       "5424              [neutral]  \n",
       "5425                  [joy]  \n",
       "5426              [neutral]  \n",
       "\n",
       "[5427 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53492f7a",
   "metadata": {},
   "source": [
    "## 4. Save the output, tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5113b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('output_roberta_base.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8b778c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer\n",
    "tokenizer.save_pretrained('./roberta_base_transformer/')\n",
    "\n",
    "# Save model\n",
    "trainer.save_model('./roberta_base_transformer/'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7ba9b95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73edad27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
