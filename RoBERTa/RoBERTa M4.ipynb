{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a4b07d",
   "metadata": {},
   "source": [
    "# Roberta M4 (with class weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00c3d39",
   "metadata": {},
   "source": [
    "Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019c1d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn ,cuda\n",
    "from torch.utils# Roberta M1 (with class weight)\n",
    "\n",
    "Importing the necessary libraries.data import DataLoader,Dataset,RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "import nltk.corpus\n",
    "from sklearn import metrics\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, AutoTokenizer, BertModel, BertConfig, AutoModel, AdamW\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer,TrainerCallback\n",
    "import glob\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from transformers import EvalPrediction   \n",
    "import copy\n",
    "from typing import Optional\n",
    "from torch import FloatTensor\n",
    "from torch.nn import BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8380697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb30b595",
   "metadata": {},
   "source": [
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9fc1af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_path = 'data/clean/'\n",
    "file_pattern = folder_path + '*.csv'\n",
    "csv_files = glob.glob(file_pattern)\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    if 'train' in csv_file:\n",
    "        df_train = pd.read_csv(csv_file)\n",
    "    elif 'val' in csv_file:\n",
    "        df_val = pd.read_csv(csv_file)\n",
    "    else:\n",
    "        df_test = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d73c9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my favourite food is anything i did not have t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>now if he does off himself everyone will think...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why the fuck is bayless isoing</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to make her feel threatened</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dirty southern wankers</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  admiration  amusement  \\\n",
       "0  my favourite food is anything i did not have t...           0          0   \n",
       "1  now if he does off himself everyone will think...           0          0   \n",
       "2                     why the fuck is bayless isoing           0          0   \n",
       "3                        to make her feel threatened           0          0   \n",
       "4                             dirty southern wankers           0          0   \n",
       "\n",
       "   anger  annoyance  approval  caring  confusion  curiosity  desire  ...  \\\n",
       "0      0          0         0       0          0          0       0  ...   \n",
       "1      0          0         0       0          0          0       0  ...   \n",
       "2      1          0         0       0          0          0       0  ...   \n",
       "3      0          0         0       0          0          0       0  ...   \n",
       "4      0          1         0       0          0          0       0  ...   \n",
       "\n",
       "   love  nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0     0            0         0      0            0       0        0        0   \n",
       "1     0            0         0      0            0       0        0        0   \n",
       "2     0            0         0      0            0       0        0        0   \n",
       "3     0            0         0      0            0       0        0        0   \n",
       "4     0            0         0      0            0       0        0        0   \n",
       "\n",
       "   surprise  neutral  \n",
       "0         0        1  \n",
       "1         0        1  \n",
       "2         0        0  \n",
       "3         0        0  \n",
       "4         0        0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaa441b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30bbe2a",
   "metadata": {},
   "source": [
    "## 2. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46926bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections of config\n",
    "\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 200\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 3e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87038cff",
   "metadata": {},
   "source": [
    "#### Storing all 28 labels into variable target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4af580be",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [col for col in df_train.columns if col not in ['clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43261378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ffe4fe",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b7c54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb3d783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the training set\n",
    "train_encodings = tokenizer(list(df_train['clean_text']), padding=True, truncation=True, return_tensors='pt')\n",
    "train_dataset = Dataset.from_dict({'input_ids': train_encodings['input_ids'],\n",
    "                                   'attention_mask': train_encodings['attention_mask'],\n",
    "                                   'labels': torch.tensor(df_train[target_cols].values, dtype=torch.float32)})\n",
    "\n",
    "# Tokenize the validation set\n",
    "val_encodings = tokenizer(list(df_val['clean_text']), padding=True, truncation=True, return_tensors='pt')\n",
    "valid_dataset = Dataset.from_dict({'input_ids': val_encodings['input_ids'],\n",
    "                                 'attention_mask': val_encodings['attention_mask'],\n",
    "                                 'labels': torch.tensor(df_val[target_cols].values, dtype=torch.float32)})\n",
    "\n",
    "\n",
    "test_encodings = tokenizer(list(df_test['clean_text']), padding=True, truncation=True, return_tensors='pt')\n",
    "test_dataset = Dataset.from_dict({'input_ids': test_encodings['input_ids'],\n",
    "                                 'attention_mask': test_encodings['attention_mask'],\n",
    "                                 'labels': torch.tensor(df_test[target_cols].values, dtype=torch.float32)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ae4840",
   "metadata": {},
   "source": [
    "### Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0442a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# download model from model hub\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(target_cols))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e91901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom callback to get train and validation info during training\n",
    "class CustomCallback(TrainerCallback):\n",
    "    \n",
    "    def __init__(self, trainer) -> None:\n",
    "        super().__init__()\n",
    "        self._trainer = trainer\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if control.should_evaluate:\n",
    "            control_copy = copy.deepcopy(control)\n",
    "            self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\")\n",
    "            return control_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb5596",
   "metadata": {},
   "source": [
    "#### Compute class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cdb39da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "num_classes = len(target_cols)\n",
    "class_counts = np.sum(train_dataset['labels'], axis=0)\n",
    "class_weights = 1.0 / class_counts\n",
    "\n",
    "# Normalize weights\n",
    "class_weights /= class_weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82eeccf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00434095, 0.00770108, 0.01144105, 0.00725835, 0.00610007,\n",
       "       0.01649321, 0.01310535, 0.00818262, 0.02796899, 0.0141389 ,\n",
       "       0.00886653, 0.02260797, 0.05936463, 0.02101773, 0.03013129,\n",
       "       0.00673483, 0.23283272, 0.01234719, 0.0085945 , 0.10998846,\n",
       "       0.01133973, 0.16151459, 0.01615146, 0.11717725, 0.03289563,\n",
       "       0.01353066, 0.01691332, 0.00126095])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "611e9804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom trainer to incorporate class weights\n",
    "class MultiLabelTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights: Optional[FloatTensor] = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            class_weights = class_weights.to(self.args.device)\n",
    "            logging.info(f\"Using multi-label classification with class weights\", class_weights)\n",
    "        self.loss_fct = BCEWithLogitsLoss(weight=class_weights)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        try:\n",
    "            loss = self.loss_fct(outputs.logits.view(-1, model.num_labels), labels.view(-1,model.num_labels))\n",
    "        except AttributeError:  # DataParallel\n",
    "            loss = self.loss_fct(outputs.logits.view(-1, model.module.num_labels), labels.view(-1, model.num_labels))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1980801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='data/output/',\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=VALID_BATCH_SIZE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    logging_dir=\"data/output/logs\",\n",
    "    learning_rate=float(LEARNING_RATE),\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0eae176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom function to calculate the metrics for multi label classification\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'recall' : recall_micro,\n",
    "               'precision': precision_micro,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3287773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Trainer instance\n",
    "trainer = MultiLabelTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.add_callback(CustomCallback(trainer)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e1c6498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aishah/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13565' max='13565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13565/13565 2:06:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.093800</td>\n",
       "      <td>0.084499</td>\n",
       "      <td>0.578936</td>\n",
       "      <td>0.478228</td>\n",
       "      <td>0.733373</td>\n",
       "      <td>0.735299</td>\n",
       "      <td>0.453419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.093800</td>\n",
       "      <td>0.088948</td>\n",
       "      <td>0.555228</td>\n",
       "      <td>0.458150</td>\n",
       "      <td>0.704507</td>\n",
       "      <td>0.724864</td>\n",
       "      <td>0.434574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.084100</td>\n",
       "      <td>0.072598</td>\n",
       "      <td>0.639860</td>\n",
       "      <td>0.550287</td>\n",
       "      <td>0.764263</td>\n",
       "      <td>0.771419</td>\n",
       "      <td>0.513062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.084100</td>\n",
       "      <td>0.085282</td>\n",
       "      <td>0.572214</td>\n",
       "      <td>0.489655</td>\n",
       "      <td>0.688257</td>\n",
       "      <td>0.739967</td>\n",
       "      <td>0.451345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.075200</td>\n",
       "      <td>0.062165</td>\n",
       "      <td>0.705801</td>\n",
       "      <td>0.628510</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.810910</td>\n",
       "      <td>0.582727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.075200</td>\n",
       "      <td>0.084976</td>\n",
       "      <td>0.574993</td>\n",
       "      <td>0.505643</td>\n",
       "      <td>0.666391</td>\n",
       "      <td>0.747273</td>\n",
       "      <td>0.458902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.065800</td>\n",
       "      <td>0.052866</td>\n",
       "      <td>0.762300</td>\n",
       "      <td>0.692585</td>\n",
       "      <td>0.847620</td>\n",
       "      <td>0.843560</td>\n",
       "      <td>0.647830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.065800</td>\n",
       "      <td>0.087157</td>\n",
       "      <td>0.577938</td>\n",
       "      <td>0.514890</td>\n",
       "      <td>0.658581</td>\n",
       "      <td>0.751595</td>\n",
       "      <td>0.461482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.048894</td>\n",
       "      <td>0.784582</td>\n",
       "      <td>0.719847</td>\n",
       "      <td>0.862112</td>\n",
       "      <td>0.857397</td>\n",
       "      <td>0.675382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.089046</td>\n",
       "      <td>0.573277</td>\n",
       "      <td>0.513793</td>\n",
       "      <td>0.648339</td>\n",
       "      <td>0.750789</td>\n",
       "      <td>0.462588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13565, training_loss=0.07911767214371891, metrics={'train_runtime': 7606.0445, 'train_samples_per_second': 28.535, 'train_steps_per_second': 1.783, 'total_flos': 2.320457522193408e+16, 'train_loss': 0.07911767214371891, 'epoch': 5.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start the training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9dffa6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1670</td>\n",
       "      <td>2.889421e-05</td>\n",
       "      <td>0.18</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1171</td>\n",
       "      <td>2.778843e-05</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1051</td>\n",
       "      <td>2.668264e-05</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0977</td>\n",
       "      <td>2.557685e-05</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0938</td>\n",
       "      <td>2.447107e-05</td>\n",
       "      <td>0.92</td>\n",
       "      <td>2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2713</td>\n",
       "      <td>0.084499</td>\n",
       "      <td>0.578936</td>\n",
       "      <td>0.478228</td>\n",
       "      <td>0.733373</td>\n",
       "      <td>0.735299</td>\n",
       "      <td>0.453419</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088948</td>\n",
       "      <td>0.555228</td>\n",
       "      <td>0.458150</td>\n",
       "      <td>0.704507</td>\n",
       "      <td>0.724864</td>\n",
       "      <td>0.434574</td>\n",
       "      <td>20.3010</td>\n",
       "      <td>267.277</td>\n",
       "      <td>16.748</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0894</td>\n",
       "      <td>2.336528e-05</td>\n",
       "      <td>1.11</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0869</td>\n",
       "      <td>2.225949e-05</td>\n",
       "      <td>1.29</td>\n",
       "      <td>3500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0853</td>\n",
       "      <td>2.115370e-05</td>\n",
       "      <td>1.47</td>\n",
       "      <td>4000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0830</td>\n",
       "      <td>2.004792e-05</td>\n",
       "      <td>1.66</td>\n",
       "      <td>4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0841</td>\n",
       "      <td>1.894213e-05</td>\n",
       "      <td>1.84</td>\n",
       "      <td>5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5426</td>\n",
       "      <td>0.072598</td>\n",
       "      <td>0.639860</td>\n",
       "      <td>0.550287</td>\n",
       "      <td>0.764263</td>\n",
       "      <td>0.771419</td>\n",
       "      <td>0.513062</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085282</td>\n",
       "      <td>0.572214</td>\n",
       "      <td>0.489655</td>\n",
       "      <td>0.688257</td>\n",
       "      <td>0.739967</td>\n",
       "      <td>0.451345</td>\n",
       "      <td>23.3544</td>\n",
       "      <td>232.334</td>\n",
       "      <td>14.558</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0804</td>\n",
       "      <td>1.783634e-05</td>\n",
       "      <td>2.03</td>\n",
       "      <td>5500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0756</td>\n",
       "      <td>1.673056e-05</td>\n",
       "      <td>2.21</td>\n",
       "      <td>6000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0734</td>\n",
       "      <td>1.562477e-05</td>\n",
       "      <td>2.40</td>\n",
       "      <td>6500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0744</td>\n",
       "      <td>1.451898e-05</td>\n",
       "      <td>2.58</td>\n",
       "      <td>7000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0727</td>\n",
       "      <td>1.341320e-05</td>\n",
       "      <td>2.76</td>\n",
       "      <td>7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0752</td>\n",
       "      <td>1.230741e-05</td>\n",
       "      <td>2.95</td>\n",
       "      <td>8000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.062165</td>\n",
       "      <td>0.705801</td>\n",
       "      <td>0.628510</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.810910</td>\n",
       "      <td>0.582727</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.00</td>\n",
       "      <td>8139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084976</td>\n",
       "      <td>0.574993</td>\n",
       "      <td>0.505643</td>\n",
       "      <td>0.666391</td>\n",
       "      <td>0.747273</td>\n",
       "      <td>0.458902</td>\n",
       "      <td>21.6509</td>\n",
       "      <td>250.613</td>\n",
       "      <td>15.704</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0675</td>\n",
       "      <td>1.120162e-05</td>\n",
       "      <td>3.13</td>\n",
       "      <td>8500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0646</td>\n",
       "      <td>1.009583e-05</td>\n",
       "      <td>3.32</td>\n",
       "      <td>9000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0652</td>\n",
       "      <td>8.990048e-06</td>\n",
       "      <td>3.50</td>\n",
       "      <td>9500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0640</td>\n",
       "      <td>7.884261e-06</td>\n",
       "      <td>3.69</td>\n",
       "      <td>10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0658</td>\n",
       "      <td>6.778474e-06</td>\n",
       "      <td>3.87</td>\n",
       "      <td>10500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.00</td>\n",
       "      <td>10852</td>\n",
       "      <td>0.052866</td>\n",
       "      <td>0.762300</td>\n",
       "      <td>0.692585</td>\n",
       "      <td>0.847620</td>\n",
       "      <td>0.843560</td>\n",
       "      <td>0.647830</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.00</td>\n",
       "      <td>10852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087157</td>\n",
       "      <td>0.577938</td>\n",
       "      <td>0.514890</td>\n",
       "      <td>0.658581</td>\n",
       "      <td>0.751595</td>\n",
       "      <td>0.461482</td>\n",
       "      <td>21.6496</td>\n",
       "      <td>250.628</td>\n",
       "      <td>15.705</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0628</td>\n",
       "      <td>5.672687e-06</td>\n",
       "      <td>4.05</td>\n",
       "      <td>11000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0581</td>\n",
       "      <td>4.566900e-06</td>\n",
       "      <td>4.24</td>\n",
       "      <td>11500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0577</td>\n",
       "      <td>3.461113e-06</td>\n",
       "      <td>4.42</td>\n",
       "      <td>12000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0570</td>\n",
       "      <td>2.355326e-06</td>\n",
       "      <td>4.61</td>\n",
       "      <td>12500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0585</td>\n",
       "      <td>1.249539e-06</td>\n",
       "      <td>4.79</td>\n",
       "      <td>13000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0567</td>\n",
       "      <td>1.437523e-07</td>\n",
       "      <td>4.98</td>\n",
       "      <td>13500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.048894</td>\n",
       "      <td>0.784582</td>\n",
       "      <td>0.719847</td>\n",
       "      <td>0.862112</td>\n",
       "      <td>0.857397</td>\n",
       "      <td>0.675382</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089046</td>\n",
       "      <td>0.573277</td>\n",
       "      <td>0.513793</td>\n",
       "      <td>0.648339</td>\n",
       "      <td>0.750789</td>\n",
       "      <td>0.462588</td>\n",
       "      <td>20.7111</td>\n",
       "      <td>261.985</td>\n",
       "      <td>16.416</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.079118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.320458e+16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      loss  learning_rate  epoch   step  train_loss  train_f1  train_recall  \\\n",
       "0   0.1670   2.889421e-05   0.18    500         NaN       NaN           NaN   \n",
       "1   0.1171   2.778843e-05   0.37   1000         NaN       NaN           NaN   \n",
       "2   0.1051   2.668264e-05   0.55   1500         NaN       NaN           NaN   \n",
       "3   0.0977   2.557685e-05   0.74   2000         NaN       NaN           NaN   \n",
       "4   0.0938   2.447107e-05   0.92   2500         NaN       NaN           NaN   \n",
       "5      NaN            NaN   1.00   2713    0.084499  0.578936      0.478228   \n",
       "6      NaN            NaN   1.00   2713         NaN       NaN           NaN   \n",
       "7   0.0894   2.336528e-05   1.11   3000         NaN       NaN           NaN   \n",
       "8   0.0869   2.225949e-05   1.29   3500         NaN       NaN           NaN   \n",
       "9   0.0853   2.115370e-05   1.47   4000         NaN       NaN           NaN   \n",
       "10  0.0830   2.004792e-05   1.66   4500         NaN       NaN           NaN   \n",
       "11  0.0841   1.894213e-05   1.84   5000         NaN       NaN           NaN   \n",
       "12     NaN            NaN   2.00   5426    0.072598  0.639860      0.550287   \n",
       "13     NaN            NaN   2.00   5426         NaN       NaN           NaN   \n",
       "14  0.0804   1.783634e-05   2.03   5500         NaN       NaN           NaN   \n",
       "15  0.0756   1.673056e-05   2.21   6000         NaN       NaN           NaN   \n",
       "16  0.0734   1.562477e-05   2.40   6500         NaN       NaN           NaN   \n",
       "17  0.0744   1.451898e-05   2.58   7000         NaN       NaN           NaN   \n",
       "18  0.0727   1.341320e-05   2.76   7500         NaN       NaN           NaN   \n",
       "19  0.0752   1.230741e-05   2.95   8000         NaN       NaN           NaN   \n",
       "20     NaN            NaN   3.00   8139    0.062165  0.705801      0.628510   \n",
       "21     NaN            NaN   3.00   8139         NaN       NaN           NaN   \n",
       "22  0.0675   1.120162e-05   3.13   8500         NaN       NaN           NaN   \n",
       "23  0.0646   1.009583e-05   3.32   9000         NaN       NaN           NaN   \n",
       "24  0.0652   8.990048e-06   3.50   9500         NaN       NaN           NaN   \n",
       "25  0.0640   7.884261e-06   3.69  10000         NaN       NaN           NaN   \n",
       "26  0.0658   6.778474e-06   3.87  10500         NaN       NaN           NaN   \n",
       "27     NaN            NaN   4.00  10852    0.052866  0.762300      0.692585   \n",
       "28     NaN            NaN   4.00  10852         NaN       NaN           NaN   \n",
       "29  0.0628   5.672687e-06   4.05  11000         NaN       NaN           NaN   \n",
       "30  0.0581   4.566900e-06   4.24  11500         NaN       NaN           NaN   \n",
       "31  0.0577   3.461113e-06   4.42  12000         NaN       NaN           NaN   \n",
       "32  0.0570   2.355326e-06   4.61  12500         NaN       NaN           NaN   \n",
       "33  0.0585   1.249539e-06   4.79  13000         NaN       NaN           NaN   \n",
       "34  0.0567   1.437523e-07   4.98  13500         NaN       NaN           NaN   \n",
       "35     NaN            NaN   5.00  13565    0.048894  0.784582      0.719847   \n",
       "36     NaN            NaN   5.00  13565         NaN       NaN           NaN   \n",
       "37     NaN            NaN   5.00  13565    0.079118       NaN           NaN   \n",
       "\n",
       "    train_precision  train_roc_auc  train_accuracy  ...  eval_loss   eval_f1  \\\n",
       "0               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "1               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "2               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "3               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "4               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "5          0.733373       0.735299        0.453419  ...        NaN       NaN   \n",
       "6               NaN            NaN             NaN  ...   0.088948  0.555228   \n",
       "7               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "8               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "9               NaN            NaN             NaN  ...        NaN       NaN   \n",
       "10              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "11              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "12         0.764263       0.771419        0.513062  ...        NaN       NaN   \n",
       "13              NaN            NaN             NaN  ...   0.085282  0.572214   \n",
       "14              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "15              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "16              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "17              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "18              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "19              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "20         0.804766       0.810910        0.582727  ...        NaN       NaN   \n",
       "21              NaN            NaN             NaN  ...   0.084976  0.574993   \n",
       "22              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "23              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "24              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "25              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "26              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "27         0.847620       0.843560        0.647830  ...        NaN       NaN   \n",
       "28              NaN            NaN             NaN  ...   0.087157  0.577938   \n",
       "29              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "30              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "31              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "32              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "33              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "34              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "35         0.862112       0.857397        0.675382  ...        NaN       NaN   \n",
       "36              NaN            NaN             NaN  ...   0.089046  0.573277   \n",
       "37              NaN            NaN             NaN  ...        NaN       NaN   \n",
       "\n",
       "    eval_recall  eval_precision  eval_roc_auc  eval_accuracy  eval_runtime  \\\n",
       "0           NaN             NaN           NaN            NaN           NaN   \n",
       "1           NaN             NaN           NaN            NaN           NaN   \n",
       "2           NaN             NaN           NaN            NaN           NaN   \n",
       "3           NaN             NaN           NaN            NaN           NaN   \n",
       "4           NaN             NaN           NaN            NaN           NaN   \n",
       "5           NaN             NaN           NaN            NaN           NaN   \n",
       "6      0.458150        0.704507      0.724864       0.434574       20.3010   \n",
       "7           NaN             NaN           NaN            NaN           NaN   \n",
       "8           NaN             NaN           NaN            NaN           NaN   \n",
       "9           NaN             NaN           NaN            NaN           NaN   \n",
       "10          NaN             NaN           NaN            NaN           NaN   \n",
       "11          NaN             NaN           NaN            NaN           NaN   \n",
       "12          NaN             NaN           NaN            NaN           NaN   \n",
       "13     0.489655        0.688257      0.739967       0.451345       23.3544   \n",
       "14          NaN             NaN           NaN            NaN           NaN   \n",
       "15          NaN             NaN           NaN            NaN           NaN   \n",
       "16          NaN             NaN           NaN            NaN           NaN   \n",
       "17          NaN             NaN           NaN            NaN           NaN   \n",
       "18          NaN             NaN           NaN            NaN           NaN   \n",
       "19          NaN             NaN           NaN            NaN           NaN   \n",
       "20          NaN             NaN           NaN            NaN           NaN   \n",
       "21     0.505643        0.666391      0.747273       0.458902       21.6509   \n",
       "22          NaN             NaN           NaN            NaN           NaN   \n",
       "23          NaN             NaN           NaN            NaN           NaN   \n",
       "24          NaN             NaN           NaN            NaN           NaN   \n",
       "25          NaN             NaN           NaN            NaN           NaN   \n",
       "26          NaN             NaN           NaN            NaN           NaN   \n",
       "27          NaN             NaN           NaN            NaN           NaN   \n",
       "28     0.514890        0.658581      0.751595       0.461482       21.6496   \n",
       "29          NaN             NaN           NaN            NaN           NaN   \n",
       "30          NaN             NaN           NaN            NaN           NaN   \n",
       "31          NaN             NaN           NaN            NaN           NaN   \n",
       "32          NaN             NaN           NaN            NaN           NaN   \n",
       "33          NaN             NaN           NaN            NaN           NaN   \n",
       "34          NaN             NaN           NaN            NaN           NaN   \n",
       "35          NaN             NaN           NaN            NaN           NaN   \n",
       "36     0.513793        0.648339      0.750789       0.462588       20.7111   \n",
       "37          NaN             NaN           NaN            NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second    total_flos  \n",
       "0                       NaN                    NaN           NaN  \n",
       "1                       NaN                    NaN           NaN  \n",
       "2                       NaN                    NaN           NaN  \n",
       "3                       NaN                    NaN           NaN  \n",
       "4                       NaN                    NaN           NaN  \n",
       "5                       NaN                    NaN           NaN  \n",
       "6                   267.277                 16.748           NaN  \n",
       "7                       NaN                    NaN           NaN  \n",
       "8                       NaN                    NaN           NaN  \n",
       "9                       NaN                    NaN           NaN  \n",
       "10                      NaN                    NaN           NaN  \n",
       "11                      NaN                    NaN           NaN  \n",
       "12                      NaN                    NaN           NaN  \n",
       "13                  232.334                 14.558           NaN  \n",
       "14                      NaN                    NaN           NaN  \n",
       "15                      NaN                    NaN           NaN  \n",
       "16                      NaN                    NaN           NaN  \n",
       "17                      NaN                    NaN           NaN  \n",
       "18                      NaN                    NaN           NaN  \n",
       "19                      NaN                    NaN           NaN  \n",
       "20                      NaN                    NaN           NaN  \n",
       "21                  250.613                 15.704           NaN  \n",
       "22                      NaN                    NaN           NaN  \n",
       "23                      NaN                    NaN           NaN  \n",
       "24                      NaN                    NaN           NaN  \n",
       "25                      NaN                    NaN           NaN  \n",
       "26                      NaN                    NaN           NaN  \n",
       "27                      NaN                    NaN           NaN  \n",
       "28                  250.628                 15.705           NaN  \n",
       "29                      NaN                    NaN           NaN  \n",
       "30                      NaN                    NaN           NaN  \n",
       "31                      NaN                    NaN           NaN  \n",
       "32                      NaN                    NaN           NaN  \n",
       "33                      NaN                    NaN           NaN  \n",
       "34                      NaN                    NaN           NaN  \n",
       "35                      NaN                    NaN           NaN  \n",
       "36                  261.985                 16.416           NaN  \n",
       "37                      NaN                    NaN  2.320458e+16  \n",
       "\n",
       "[38 rows x 23 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view train and validation metrics from training\n",
    "log_history = pd.DataFrame(trainer.state.log_history)\n",
    "log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf0d198a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2713</td>\n",
       "      <td>0.084499</td>\n",
       "      <td>0.578936</td>\n",
       "      <td>0.478228</td>\n",
       "      <td>0.733373</td>\n",
       "      <td>0.735299</td>\n",
       "      <td>0.453419</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5426</td>\n",
       "      <td>0.072598</td>\n",
       "      <td>0.639860</td>\n",
       "      <td>0.550287</td>\n",
       "      <td>0.764263</td>\n",
       "      <td>0.771419</td>\n",
       "      <td>0.513062</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.062165</td>\n",
       "      <td>0.705801</td>\n",
       "      <td>0.628510</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.810910</td>\n",
       "      <td>0.582727</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10852</td>\n",
       "      <td>0.052866</td>\n",
       "      <td>0.762300</td>\n",
       "      <td>0.692585</td>\n",
       "      <td>0.847620</td>\n",
       "      <td>0.843560</td>\n",
       "      <td>0.647830</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.048894</td>\n",
       "      <td>0.784582</td>\n",
       "      <td>0.719847</td>\n",
       "      <td>0.862112</td>\n",
       "      <td>0.857397</td>\n",
       "      <td>0.675382</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    loss  learning_rate  epoch   step  train_loss  train_f1  train_recall  \\\n",
       "5    NaN            NaN    1.0   2713    0.084499  0.578936      0.478228   \n",
       "12   NaN            NaN    2.0   5426    0.072598  0.639860      0.550287   \n",
       "20   NaN            NaN    3.0   8139    0.062165  0.705801      0.628510   \n",
       "27   NaN            NaN    4.0  10852    0.052866  0.762300      0.692585   \n",
       "35   NaN            NaN    5.0  13565    0.048894  0.784582      0.719847   \n",
       "\n",
       "    train_precision  train_roc_auc  train_accuracy  ...  eval_loss  eval_f1  \\\n",
       "5          0.733373       0.735299        0.453419  ...        NaN      NaN   \n",
       "12         0.764263       0.771419        0.513062  ...        NaN      NaN   \n",
       "20         0.804766       0.810910        0.582727  ...        NaN      NaN   \n",
       "27         0.847620       0.843560        0.647830  ...        NaN      NaN   \n",
       "35         0.862112       0.857397        0.675382  ...        NaN      NaN   \n",
       "\n",
       "    eval_recall  eval_precision  eval_roc_auc  eval_accuracy  eval_runtime  \\\n",
       "5           NaN             NaN           NaN            NaN           NaN   \n",
       "12          NaN             NaN           NaN            NaN           NaN   \n",
       "20          NaN             NaN           NaN            NaN           NaN   \n",
       "27          NaN             NaN           NaN            NaN           NaN   \n",
       "35          NaN             NaN           NaN            NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  total_flos  \n",
       "5                       NaN                    NaN         NaN  \n",
       "12                      NaN                    NaN         NaN  \n",
       "20                      NaN                    NaN         NaN  \n",
       "27                      NaN                    NaN         NaN  \n",
       "35                      NaN                    NaN         NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store train metrics in dataframe\n",
    "train_history = log_history[log_history['train_f1'].notna()]\n",
    "train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97eea248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088948</td>\n",
       "      <td>0.555228</td>\n",
       "      <td>0.458150</td>\n",
       "      <td>0.704507</td>\n",
       "      <td>0.724864</td>\n",
       "      <td>0.434574</td>\n",
       "      <td>20.3010</td>\n",
       "      <td>267.277</td>\n",
       "      <td>16.748</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085282</td>\n",
       "      <td>0.572214</td>\n",
       "      <td>0.489655</td>\n",
       "      <td>0.688257</td>\n",
       "      <td>0.739967</td>\n",
       "      <td>0.451345</td>\n",
       "      <td>23.3544</td>\n",
       "      <td>232.334</td>\n",
       "      <td>14.558</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084976</td>\n",
       "      <td>0.574993</td>\n",
       "      <td>0.505643</td>\n",
       "      <td>0.666391</td>\n",
       "      <td>0.747273</td>\n",
       "      <td>0.458902</td>\n",
       "      <td>21.6509</td>\n",
       "      <td>250.613</td>\n",
       "      <td>15.704</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087157</td>\n",
       "      <td>0.577938</td>\n",
       "      <td>0.514890</td>\n",
       "      <td>0.658581</td>\n",
       "      <td>0.751595</td>\n",
       "      <td>0.461482</td>\n",
       "      <td>21.6496</td>\n",
       "      <td>250.628</td>\n",
       "      <td>15.705</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089046</td>\n",
       "      <td>0.573277</td>\n",
       "      <td>0.513793</td>\n",
       "      <td>0.648339</td>\n",
       "      <td>0.750789</td>\n",
       "      <td>0.462588</td>\n",
       "      <td>20.7111</td>\n",
       "      <td>261.985</td>\n",
       "      <td>16.416</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    loss  learning_rate  epoch   step  train_loss  train_f1  train_recall  \\\n",
       "6    NaN            NaN    1.0   2713         NaN       NaN           NaN   \n",
       "13   NaN            NaN    2.0   5426         NaN       NaN           NaN   \n",
       "21   NaN            NaN    3.0   8139         NaN       NaN           NaN   \n",
       "28   NaN            NaN    4.0  10852         NaN       NaN           NaN   \n",
       "36   NaN            NaN    5.0  13565         NaN       NaN           NaN   \n",
       "\n",
       "    train_precision  train_roc_auc  train_accuracy  ...  eval_loss   eval_f1  \\\n",
       "6               NaN            NaN             NaN  ...   0.088948  0.555228   \n",
       "13              NaN            NaN             NaN  ...   0.085282  0.572214   \n",
       "21              NaN            NaN             NaN  ...   0.084976  0.574993   \n",
       "28              NaN            NaN             NaN  ...   0.087157  0.577938   \n",
       "36              NaN            NaN             NaN  ...   0.089046  0.573277   \n",
       "\n",
       "    eval_recall  eval_precision  eval_roc_auc  eval_accuracy  eval_runtime  \\\n",
       "6      0.458150        0.704507      0.724864       0.434574       20.3010   \n",
       "13     0.489655        0.688257      0.739967       0.451345       23.3544   \n",
       "21     0.505643        0.666391      0.747273       0.458902       21.6509   \n",
       "28     0.514890        0.658581      0.751595       0.461482       21.6496   \n",
       "36     0.513793        0.648339      0.750789       0.462588       20.7111   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  total_flos  \n",
       "6                   267.277                 16.748         NaN  \n",
       "13                  232.334                 14.558         NaN  \n",
       "21                  250.613                 15.704         NaN  \n",
       "28                  250.628                 15.705         NaN  \n",
       "36                  261.985                 16.416         NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store validation metrics in dataframe\n",
    "val_history = log_history[log_history['eval_f1'].notna()]\n",
    "val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "978bf0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_x</th>\n",
       "      <th>learning_rate_x</th>\n",
       "      <th>epoch_x</th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss_x</th>\n",
       "      <th>train_f1_x</th>\n",
       "      <th>train_recall_x</th>\n",
       "      <th>train_precision_x</th>\n",
       "      <th>train_roc_auc_x</th>\n",
       "      <th>train_accuracy_x</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_loss_y</th>\n",
       "      <th>eval_f1_y</th>\n",
       "      <th>eval_recall_y</th>\n",
       "      <th>eval_precision_y</th>\n",
       "      <th>eval_roc_auc_y</th>\n",
       "      <th>eval_accuracy_y</th>\n",
       "      <th>eval_runtime_y</th>\n",
       "      <th>eval_samples_per_second_y</th>\n",
       "      <th>eval_steps_per_second_y</th>\n",
       "      <th>total_flos_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2713</td>\n",
       "      <td>0.084499</td>\n",
       "      <td>0.578936</td>\n",
       "      <td>0.478228</td>\n",
       "      <td>0.733373</td>\n",
       "      <td>0.735299</td>\n",
       "      <td>0.453419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088948</td>\n",
       "      <td>0.555228</td>\n",
       "      <td>0.458150</td>\n",
       "      <td>0.704507</td>\n",
       "      <td>0.724864</td>\n",
       "      <td>0.434574</td>\n",
       "      <td>20.3010</td>\n",
       "      <td>267.277</td>\n",
       "      <td>16.748</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5426</td>\n",
       "      <td>0.072598</td>\n",
       "      <td>0.639860</td>\n",
       "      <td>0.550287</td>\n",
       "      <td>0.764263</td>\n",
       "      <td>0.771419</td>\n",
       "      <td>0.513062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085282</td>\n",
       "      <td>0.572214</td>\n",
       "      <td>0.489655</td>\n",
       "      <td>0.688257</td>\n",
       "      <td>0.739967</td>\n",
       "      <td>0.451345</td>\n",
       "      <td>23.3544</td>\n",
       "      <td>232.334</td>\n",
       "      <td>14.558</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8139</td>\n",
       "      <td>0.062165</td>\n",
       "      <td>0.705801</td>\n",
       "      <td>0.628510</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.810910</td>\n",
       "      <td>0.582727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084976</td>\n",
       "      <td>0.574993</td>\n",
       "      <td>0.505643</td>\n",
       "      <td>0.666391</td>\n",
       "      <td>0.747273</td>\n",
       "      <td>0.458902</td>\n",
       "      <td>21.6509</td>\n",
       "      <td>250.613</td>\n",
       "      <td>15.704</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10852</td>\n",
       "      <td>0.052866</td>\n",
       "      <td>0.762300</td>\n",
       "      <td>0.692585</td>\n",
       "      <td>0.847620</td>\n",
       "      <td>0.843560</td>\n",
       "      <td>0.647830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087157</td>\n",
       "      <td>0.577938</td>\n",
       "      <td>0.514890</td>\n",
       "      <td>0.658581</td>\n",
       "      <td>0.751595</td>\n",
       "      <td>0.461482</td>\n",
       "      <td>21.6496</td>\n",
       "      <td>250.628</td>\n",
       "      <td>15.705</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13565</td>\n",
       "      <td>0.048894</td>\n",
       "      <td>0.784582</td>\n",
       "      <td>0.719847</td>\n",
       "      <td>0.862112</td>\n",
       "      <td>0.857397</td>\n",
       "      <td>0.675382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089046</td>\n",
       "      <td>0.573277</td>\n",
       "      <td>0.513793</td>\n",
       "      <td>0.648339</td>\n",
       "      <td>0.750789</td>\n",
       "      <td>0.462588</td>\n",
       "      <td>20.7111</td>\n",
       "      <td>261.985</td>\n",
       "      <td>16.416</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loss_x  learning_rate_x  epoch_x   step  train_loss_x  train_f1_x  \\\n",
       "0     NaN              NaN      1.0   2713      0.084499    0.578936   \n",
       "1     NaN              NaN      2.0   5426      0.072598    0.639860   \n",
       "2     NaN              NaN      3.0   8139      0.062165    0.705801   \n",
       "3     NaN              NaN      4.0  10852      0.052866    0.762300   \n",
       "4     NaN              NaN      5.0  13565      0.048894    0.784582   \n",
       "\n",
       "   train_recall_x  train_precision_x  train_roc_auc_x  train_accuracy_x  ...  \\\n",
       "0        0.478228           0.733373         0.735299          0.453419  ...   \n",
       "1        0.550287           0.764263         0.771419          0.513062  ...   \n",
       "2        0.628510           0.804766         0.810910          0.582727  ...   \n",
       "3        0.692585           0.847620         0.843560          0.647830  ...   \n",
       "4        0.719847           0.862112         0.857397          0.675382  ...   \n",
       "\n",
       "   eval_loss_y  eval_f1_y  eval_recall_y  eval_precision_y  eval_roc_auc_y  \\\n",
       "0     0.088948   0.555228       0.458150          0.704507        0.724864   \n",
       "1     0.085282   0.572214       0.489655          0.688257        0.739967   \n",
       "2     0.084976   0.574993       0.505643          0.666391        0.747273   \n",
       "3     0.087157   0.577938       0.514890          0.658581        0.751595   \n",
       "4     0.089046   0.573277       0.513793          0.648339        0.750789   \n",
       "\n",
       "   eval_accuracy_y  eval_runtime_y  eval_samples_per_second_y  \\\n",
       "0         0.434574         20.3010                    267.277   \n",
       "1         0.451345         23.3544                    232.334   \n",
       "2         0.458902         21.6509                    250.613   \n",
       "3         0.461482         21.6496                    250.628   \n",
       "4         0.462588         20.7111                    261.985   \n",
       "\n",
       "   eval_steps_per_second_y  total_flos_y  \n",
       "0                   16.748           NaN  \n",
       "1                   14.558           NaN  \n",
       "2                   15.704           NaN  \n",
       "3                   15.705           NaN  \n",
       "4                   16.416           NaN  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = pd.merge(train_history, val_history, on='step', how='outer')\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ebe0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting function to check for accuracy with graphs \n",
    "def plot_model_performance(history):\n",
    "\n",
    "    #getting train and validation accuracy\n",
    "    acc = history['train_accuracy_x']\n",
    "    val_acc = history['eval_accuracy_y']\n",
    "\n",
    "    #getting train and validation loss\n",
    "    loss = history['train_loss_x']\n",
    "    val_loss = history['eval_loss_y']\n",
    "\n",
    "    epochs_range = range(5)\n",
    "\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c97c350d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_model_performance(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc7fb89",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5456e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to evaluate the model on the test set\n",
    "def evaluate_model(test_dataset):\n",
    "    test_res = trainer.evaluate(eval_dataset=test_dataset)\n",
    "    print(test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7bb617c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08569106459617615, 'eval_f1': 0.5810582481102712, 'eval_recall': 0.5161952915152472, 'eval_precision': 0.6645646867371847, 'eval_roc_auc': 0.7524359209401104, 'eval_accuracy': 0.4718997604569744, 'eval_runtime': 15.7906, 'eval_samples_per_second': 343.686, 'eval_steps_per_second': 21.532, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c172305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate each emotion label metrics on test set\n",
    "def calc_label_metrics(label, y_targets, y_preds, threshold):\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"accuracy\": metrics.accuracy_score(y_targets, y_preds),\n",
    "        \"precision\": metrics.precision_score(y_targets, y_preds, zero_division=0),\n",
    "        \"recall\": metrics.recall_score(y_targets, y_preds, zero_division=0),\n",
    "        \"f1\": metrics.f1_score(y_targets, y_preds, zero_division=0),\n",
    "        \"mcc\": metrics.matthews_corrcoef(y_targets, y_preds),\n",
    "        \"support\": y_targets.sum(),\n",
    "        \"threshold\": threshold,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78c37071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate overall metric on test set\n",
    "def calc_test_metrics(trainer, test_dataset, target_cols):\n",
    "    y_test = trainer.predict(test_dataset)\n",
    "    threshold = 0.5\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(y_test.predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= 0.5)] = 1\n",
    "    \n",
    "    # finally, compute metrics\n",
    "    y_true = df_test[target_cols].values\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'recall' : recall_micro,\n",
    "               'precision': precision_micro,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    metrics_df = pd.DataFrame.from_dict(metrics, orient='index', columns=['Value'])\n",
    "\n",
    "    display(metrics_df)\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for label_index, label in enumerate(target_cols):\n",
    "        y_targets, y_preds = y_true[:, label_index], y_pred[:, label_index]\n",
    "        results.append(calc_label_metrics(label, y_targets, y_preds, threshold))\n",
    "\n",
    "    per_label_results = pd.DataFrame(results, index=target_cols)\n",
    "    display(per_label_results.drop(columns=[\"label\"]).round(3))\n",
    "    \n",
    "    return y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "794fec4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.581058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.516195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.664565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc</th>\n",
       "      <td>0.752436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.471900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "f1         0.581058\n",
       "recall     0.516195\n",
       "precision  0.664565\n",
       "roc_auc    0.752436\n",
       "accuracy   0.471900"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>mcc</th>\n",
       "      <th>support</th>\n",
       "      <th>threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>admiration</th>\n",
       "      <td>0.942</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.662</td>\n",
       "      <td>504</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amusement</th>\n",
       "      <td>0.980</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.799</td>\n",
       "      <td>264</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.968</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.489</td>\n",
       "      <td>198</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoyance</th>\n",
       "      <td>0.937</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.285</td>\n",
       "      <td>320</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.939</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.365</td>\n",
       "      <td>351</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>0.975</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.418</td>\n",
       "      <td>135</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion</th>\n",
       "      <td>0.972</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.402</td>\n",
       "      <td>153</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curiosity</th>\n",
       "      <td>0.948</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.424</td>\n",
       "      <td>284</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desire</th>\n",
       "      <td>0.988</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.522</td>\n",
       "      <td>83</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointment</th>\n",
       "      <td>0.971</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.308</td>\n",
       "      <td>151</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disapproval</th>\n",
       "      <td>0.948</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.352</td>\n",
       "      <td>267</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.980</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.502</td>\n",
       "      <td>123</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embarrassment</th>\n",
       "      <td>0.995</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.550</td>\n",
       "      <td>37</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excitement</th>\n",
       "      <td>0.982</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.415</td>\n",
       "      <td>103</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.991</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.657</td>\n",
       "      <td>78</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>0.990</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.918</td>\n",
       "      <td>352</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grief</th>\n",
       "      <td>0.999</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.977</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.577</td>\n",
       "      <td>161</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.983</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.806</td>\n",
       "      <td>238</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nervousness</th>\n",
       "      <td>0.996</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.314</td>\n",
       "      <td>23</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimism</th>\n",
       "      <td>0.973</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.538</td>\n",
       "      <td>186</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pride</th>\n",
       "      <td>0.997</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.176</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realization</th>\n",
       "      <td>0.972</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.260</td>\n",
       "      <td>145</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>0.998</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remorse</th>\n",
       "      <td>0.992</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.621</td>\n",
       "      <td>56</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.976</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.562</td>\n",
       "      <td>156</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.980</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.579</td>\n",
       "      <td>141</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.783</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.485</td>\n",
       "      <td>1787</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                accuracy  precision  recall     f1    mcc  support  threshold\n",
       "admiration         0.942      0.675   0.714  0.694  0.662      504        0.5\n",
       "amusement          0.980      0.772   0.848  0.809  0.799      264        0.5\n",
       "anger              0.968      0.575   0.444  0.501  0.489      198        0.5\n",
       "annoyance          0.937      0.430   0.231  0.301  0.285      320        0.5\n",
       "approval           0.939      0.550   0.282  0.373  0.365      351        0.5\n",
       "caring             0.975      0.500   0.370  0.426  0.418      135        0.5\n",
       "confusion          0.972      0.500   0.346  0.409  0.402      153        0.5\n",
       "curiosity          0.948      0.502   0.405  0.448  0.424      284        0.5\n",
       "desire             0.988      0.643   0.434  0.518  0.522       83        0.5\n",
       "disappointment     0.971      0.471   0.219  0.299  0.308      151        0.5\n",
       "disapproval        0.948      0.463   0.307  0.369  0.352      267        0.5\n",
       "disgust            0.980      0.585   0.447  0.507  0.502      123        0.5\n",
       "embarrassment      0.995      0.867   0.351  0.500  0.550       37        0.5\n",
       "excitement         0.982      0.559   0.320  0.407  0.415      103        0.5\n",
       "fear               0.991      0.742   0.590  0.657  0.657       78        0.5\n",
       "gratitude          0.990      0.949   0.898  0.923  0.918      352        0.5\n",
       "grief              0.999      0.000   0.000  0.000  0.000        6        0.5\n",
       "joy                0.977      0.633   0.547  0.587  0.577      161        0.5\n",
       "love               0.983      0.783   0.849  0.815  0.806      238        0.5\n",
       "nervousness        0.996      0.571   0.174  0.267  0.314       23        0.5\n",
       "optimism           0.973      0.649   0.468  0.544  0.538      186        0.5\n",
       "pride              0.997      0.500   0.062  0.111  0.176       16        0.5\n",
       "realization        0.972      0.444   0.166  0.241  0.260      145        0.5\n",
       "relief             0.998      0.000   0.000  0.000  0.000       11        0.5\n",
       "remorse            0.992      0.576   0.679  0.623  0.621       56        0.5\n",
       "sadness            0.976      0.578   0.571  0.574  0.562      156        0.5\n",
       "surprise           0.980      0.636   0.546  0.588  0.579      141        0.5\n",
       "neutral            0.783      0.725   0.548  0.624  0.485     1787        0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "targets, outputs = calc_test_metrics(trainer, test_dataset, target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f1631d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results DataFrame:\n",
      "         Actual                Predicted\n",
      "0     [sadness]       [remorse, sadness]\n",
      "1  [admiration]             [admiration]\n",
      "2  [excitement]               [optimism]\n",
      "3   [gratitude]              [gratitude]\n",
      "4     [neutral]                [neutral]\n",
      "5   [gratitude]              [gratitude]\n",
      "6   [gratitude]              [gratitude]\n",
      "7   [gratitude]  [admiration, gratitude]\n",
      "8     [remorse]                [remorse]\n",
      "9     [sadness]                [sadness]\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to store actual labels and predicted labels\n",
    "final_df = pd.DataFrame({\n",
    "    'Actual': [list(np.where(targets[i])[0]) for i in range(len(targets))],\n",
    "    'Predicted': [list(np.where(outputs[i])[0]) for i in range(len(outputs))]\n",
    "})\n",
    "\n",
    "# Map label indices to label names in the 'Actual' column\n",
    "final_df['Actual'] = final_df['Actual'].apply(lambda indices: [target_cols[idx] for idx in indices])\n",
    "\n",
    "# Map label indices to label names in the 'Predicted' column\n",
    "final_df['Predicted'] = final_df['Predicted'].apply(lambda indices: [target_cols[idx] for idx in indices])\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(\"Results DataFrame:\")\n",
    "print(final_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "301ef201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the label DataFrame with the original DataFramev\n",
    "val_df_terms = df_test['clean_text']\n",
    "result_df = pd.concat([val_df_terms, final_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a335fd5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am really sorry about your situation frown s...</td>\n",
       "      <td>[sadness]</td>\n",
       "      <td>[remorse, sadness]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it is wonderful because it is awful at not with</td>\n",
       "      <td>[admiration]</td>\n",
       "      <td>[admiration]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kings fan here good luck to you guys will be a...</td>\n",
       "      <td>[excitement]</td>\n",
       "      <td>[optimism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i did not know that thank you for teaching me ...</td>\n",
       "      <td>[gratitude]</td>\n",
       "      <td>[gratitude]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>they got bored from haunting earth for thousan...</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5422</th>\n",
       "      <td>thanks i was diagnosed with bp 1 after the hos...</td>\n",
       "      <td>[gratitude]</td>\n",
       "      <td>[gratitude]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5423</th>\n",
       "      <td>well that makes sense</td>\n",
       "      <td>[approval]</td>\n",
       "      <td>[approval]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5424</th>\n",
       "      <td>daddy issues name</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5425</th>\n",
       "      <td>so glad i discovered that subreddit a couple m...</td>\n",
       "      <td>[admiration]</td>\n",
       "      <td>[admiration, joy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5426</th>\n",
       "      <td>had to watch elmo in grouchland one time too m...</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5427 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_text        Actual  \\\n",
       "0     i am really sorry about your situation frown s...     [sadness]   \n",
       "1       it is wonderful because it is awful at not with  [admiration]   \n",
       "2     kings fan here good luck to you guys will be a...  [excitement]   \n",
       "3     i did not know that thank you for teaching me ...   [gratitude]   \n",
       "4     they got bored from haunting earth for thousan...     [neutral]   \n",
       "...                                                 ...           ...   \n",
       "5422  thanks i was diagnosed with bp 1 after the hos...   [gratitude]   \n",
       "5423                              well that makes sense    [approval]   \n",
       "5424                                  daddy issues name     [neutral]   \n",
       "5425  so glad i discovered that subreddit a couple m...  [admiration]   \n",
       "5426  had to watch elmo in grouchland one time too m...     [neutral]   \n",
       "\n",
       "               Predicted  \n",
       "0     [remorse, sadness]  \n",
       "1           [admiration]  \n",
       "2             [optimism]  \n",
       "3            [gratitude]  \n",
       "4              [neutral]  \n",
       "...                  ...  \n",
       "5422         [gratitude]  \n",
       "5423          [approval]  \n",
       "5424           [neutral]  \n",
       "5425   [admiration, joy]  \n",
       "5426           [neutral]  \n",
       "\n",
       "[5427 rows x 3 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53492f7a",
   "metadata": {},
   "source": [
    "## 4. Save the output, tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5113b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('output_roberta_m4.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8b778c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer\n",
    "tokenizer.save_pretrained('./roberta_M4_transformer/')\n",
    "\n",
    "# Save model\n",
    "trainer.save_model('./roberta_M4_transformer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ba9b95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73edad27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
